[["index.html", "Spring 2025 Semester Welcome to SDS 322E", " Spring 2025 Semester Dr. Layla Guyot 2025-04-20 Welcome to SDS 322E This is the combination of all my notes for SDS 322E! "],["getstaRted_Key.html", "getstaRted Learning Objectives: Recommended Resources:", " getstaRted Learning Objectives: Understand the basics of RStudio, including the purpose of the four panes. Learn how to execute R code and interpret the output in the Console. Explore built-in functions in R. Introduce the concept of objects, vectors, and data frames in R. ############### Let&#39;s get staRted! ############### # This file is called an R script (a file containing R code) # Notice that this script is called &quot;getstaRted.R&quot; (see tab above) # Think of R files (.R) as text files that contain R code # Hashtags are used before text comments to explain what the code is for ## You can put more than one if you want! # There are 4 panes in the R Studio window # This pane (top left) is the Editor pane, where you can edit the code # To submit code, put your cursor *anywhere on the line* and either: # - hit ctrl+enter for PC or cmd+enter for Mac users # - click &quot;Run&quot; on the top right of this pane print(&quot;Welcome to R!&quot;) ## [1] &quot;Welcome to R!&quot; # Notice how your output popped out on the bottom pane: this is the Console! #------------------------------------------------------------------------------# # R is a calculator #------------------------------------------------------------------------------# 6*2 # 6 times 2 ## [1] 12 6/2 # 6 divided by 2 ## [1] 3 6^2 # 6 squared ## [1] 36 ##### Try it! ##### # Multiply 6 by all previous whole numbers (except 0). # Note: This is called the factorial of 6. 6*5*4*3*2*1 ## [1] 720 #------------------------------------------------------------------------------# # R has built-in functions + documentation #------------------------------------------------------------------------------# # Base R comes with many built-in functions: factorial(6) # better to use this function for calculating factorials! ## [1] 720 ##### Try it! ##### # Find the factorial of 100 using the built-in function. # What should we expect for log(100)? What does R give us? log(100) ## [1] 4.60517 # If you would like to know more about a function: # Put a `?` in front of the function name ?log # Note: the documentation appears in the Output pane! ##### Try it! ##### # In the documentation you can read about the function that computes log base 10. # What is log base 10 of 100? log10(100) ## [1] 2 #------------------------------------------------------------------------------# # R saves and uses objects #------------------------------------------------------------------------------# # The pane in the top right is the Environment. # This is where objects (like datasets) will go! # There&#39;s nothing there yet because we haven&#39;t created any objects # By convention, the notation &quot;&lt;-&quot; is an assignment operator # Note: You can also use &quot;=&quot; but it is not common to do so in the R community # Let&#39;s consider some values and save them as objects variety &lt;- &quot;Fuji&quot; sweetness &lt;- 1 red &lt;- FALSE # IMPORTANT # Object names... # start with a letter, # shouldn’t contain spaces, # should not be predefined in R (e.g., do not label an object &quot;log&quot;) # Now do you see these objects in the Environment? # It saves what is on the right as an object whose name is on the left variety ## [1] &quot;Fuji&quot; # Objects in R can be of different types. # Use class() to check what type of values we saved in the environment class(variety) ## [1] &quot;character&quot; class(sweetness) ## [1] &quot;numeric&quot; class(red) ## [1] &quot;logical&quot; # We can save several values in one object called a vector # Combine elements with c() which stands for concatenate varieties &lt;- c(&quot;Fuji&quot;, &#39;Gala&#39;, &#39;Golden Delicious&#39;, &#39;Granny Smith&#39;) varieties ## [1] &quot;Fuji&quot; &quot;Gala&quot; &quot;Golden Delicious&quot; ## [4] &quot;Granny Smith&quot; # Note that either &#39; &#39; or &quot; &quot; can be used to define strings (with characters) # Vector with numeric values sweetness &lt;- 1:4 # what does &quot;:&quot; do? sweetness ## [1] 1 2 3 4 # Let&#39;s play around with the sweetness object # Add up all the values in the sweetness using the sum() function sum(sweetness) ## [1] 10 # How many values does our &quot;sweetness&quot; vector contain? length(sweetness) ## [1] 4 ##### Try it! ##### # What is the mean value of the sweetness? sum(sweetness)/length(sweetness) ## [1] 2.5 # but easier with mean(), a built-in function! mean(sweetness) ## [1] 2.5 # What if we add some more data in sweetness? c(sweetness, 5) ## [1] 1 2 3 4 5 # What if this data is of a different type? c(sweetness, &quot;very sweet&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## [5] &quot;very sweet&quot; # Check the type of data now class(c(sweetness, &quot;very sweet&quot;)) ## [1] &quot;character&quot; # R coerced the numeric values into characters # What if we want to coerce sweetness as a numeric vector? as.numeric(c(sweetness, &quot;very sweet&quot;)) # what happened? ## Warning: NAs introduced by coercion ## [1] 1 2 3 4 NA # We can save several vectors in one object called a data frame # A data frame is a very important data structure for most relational data sweet_apples &lt;- data.frame( varieties = c(&#39;Fuji&#39;, &#39;Gala&#39;, &#39;Golden Delicious&#39;, &#39;Granny Smith&#39;), sweet_index = c(1, 2, 2, 4), red = c(TRUE,TRUE,FALSE,FALSE) ) sweet_apples # Rows represent different observations, columns represent different variables str(sweet_apples) # Use str() to get more information about an object ## &#39;data.frame&#39;: 4 obs. of 3 variables: ## $ varieties : chr &quot;Fuji&quot; &quot;Gala&quot; &quot;Golden Delicious&quot; &quot;Granny Smith&quot; ## $ sweet_index: num 1 2 2 4 ## $ red : logi TRUE TRUE FALSE FALSE # You can access the different variables with $ sweet_apples$varieties ## [1] &quot;Fuji&quot; &quot;Gala&quot; &quot;Golden Delicious&quot; ## [4] &quot;Granny Smith&quot; ##### Try it! ##### # Calculate the mean sweetness in this data frame mean(sweet_apples$sweet_index) ## [1] 2.25 Recommended Resources: Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: https://rafalab.dfci.harvard.edu/dsbook/getting-started.html, https://rafalab.dfci.harvard.edu/dsbook/r-basics.html Practice more Base R with https://sites.utexas.edu/sos/rstudio-basics/ "],["WS1_RMarkdown_Key.html", "RMarkdown Learning objectives 1. Basic Markdown editing 2. Embedded R code 3. Packages Recommended Resources", " RMarkdown Learning objectives Understand the structure and purpose of R Markdown documents. Practice basic Markdown editing for headers, text formatting, lists, and hyperlinks. Incorporate and execute R code chunks within an R Markdown document. Explore built-in datasets with basic summary statistics and plots. Install and load R packages for more functions. Much of the work in this class will be done via R Markdown documents. R Markdown documents combine text (with some formatting options), R code, and R outputs including figures. They are a great way to produce self-contained, reproducible, well-documented data analyses. In this first .Rmd worksheet, you will learn how to do some basic markdown editing, continue to explore base R, manipulate datasets, and include some R packages. After you have made changes to the document, press “Knit” on the top of this pane and see what you get: hopefully an html file! It will complain in the console if some part of the code is not working. Remember: troubleshooting is part of programming life! 1. Basic Markdown editing Let’s try out basic R Markdown features, as described here: try different headers, make a numbered list and a bulleted list, write some text that is bold and some that is in italics, try the block-quote feature. a. Headers For example These are different headers ######Does not knit as a header Make sure to leave a space after #’s so that they are interpreted as headers. Similarly, leave a blank line between sections. b. Text formatting You can use some basic formatting to highlight some part of the text: bold, italic, or bold and italic Create a blockquote To refer to R objects (variables, datasets, functions, …) within the text, we can use the slanted quotes. Remember the mean() function? We will use it again today. c. Lists and bullet points Create a list: Here Are Four Things Or some bullet points: bullet 1 sub-bullet 1 bullet 2 sub-bullet 2 bullet 3 sub-bullet 3 d. HTML hyperlinks, images, tables, etc. We can include external links and images: Here is a hyperlink to Canvas Below is an image from a URL (for local images, just specify the file path in place of the URL): Note: Modifying text formatting, including links, images, and tables is fairly easy to do with the Visual mode: it works more like a standard text editor. 2. Embedded R code The advantage of a R Markdown document is that it incorporates R code in chunks within the text. a. Code chunks Code chunks will be executed when knitting and, by default, the output will be shown in the knitted file (usually a html output in this course). # Recall our sweet_apples data frame from last lecture sweet_apples &lt;- data.frame( varieties = c(&#39;Fuji&#39;, &#39;Gala&#39;, &#39;Golden Delicious&#39;, &#39;Granny Smith&#39;), sweet_index = c(1, 2, 2, 4), red = c(TRUE,TRUE,FALSE,FALSE) ) sweet_apples We can use the objects created in one chunk in the following chunks: # Find the mean sweetness sum(sweet_apples$red) ## [1] 2 Note: Make sure to skip at least 1 line before and after the code chunk to avoid any formatting issue. Try it! In a group of 4: Part A: Icebreaker questions Each group member answers these two questions: How has your first week of classes been? What is your spirit animal? Part B: Collect and analyze data! Create a data frame containing the following information: the first names of the group members (including yourself), if they’ve been having a good week (TRUE/FALSE), their spirit animal, their age, their height. mydataframe &lt;- data.frame( names = c(&#39;Layla&#39;, &#39;Walter&#39;, &#39;Monita&#39;, &#39;Saniya&#39;, &#39;Daniel&#39;, &#39;Divya&#39;), age = c(35, 99, 99, 99, 99, 99), height = c(61,99,99,99, 99, 99) ) Find the mean age and height of students in your group. mean(mydataframe$age) ## [1] 88.33333 mean(mydataframe$height) ## [1] 92.66667 The youngest member fills in the following spreadsheet: Link to spreadsheet Do you think all groups will get the same mean age? mean height? Why/Why not? Get ready to share with the rest of the class. Not all groups will get the same mean age or mean height because the data vary from group to group. The means for age might be closer together compared to the means for height: there is more variation in height. Note: we can customize code chunks with a variety of options to control their behavior and appearance in the knitted file. In this worksheet, look for include=FALSE, echo=FALSE, or eval=FALSE in the {r} statement for defining a code chunk. b. More base R: built-in datasets R comes with several built-in datasets, which are generally used as demo data for playing with R functions. The dataset used in the following code chunk is cars, which lists speed (mph) and corresponding stopping distance (ft) for some cars from the 1920s. You can run all the code in a chunk at once by using the play button on the top right of the chunk or you can submit code line by line like we did in a regular R script. # Display the first 6 rows of the dataset head(cars) # Focus on one variable cars$dist ## [1] 2 10 4 22 16 10 18 26 34 17 28 14 20 24 28 ## [16] 26 34 34 46 26 36 60 80 20 26 54 32 40 32 40 ## [31] 50 42 56 76 84 36 46 68 32 48 52 56 64 66 54 ## [46] 70 92 93 120 85 Try it! Calculate the mean of each variable in the cars dataset. Write a sentence in bold below the code chunk to interpret these values in context (including units). # Find mean of speed mean(cars$speed) ## [1] 15.4 # Find mean of dist mean(cars$dist) ## [1] 42.98 The mean speed is 15.4 mph and the mean stopping distance is 42.98 ft for these cars from the 1920s. You can create some basic table to summarize each of the variables using the means calculated in a previous code chunk: speed distance mean 15.4 mph 42.98 ft What if we want to include more statistics about our data? A pretty convenient function that can find basic descriptive statistics for all variables in the dataset is summary(): # Take a look at descriptive statistics for all variables summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 It can help us find any inconsistencies in the data (e.g., min or max values that are out of range of what we expect). c. More base R: basic plots You can easily embed plots in your document: # Create a basic histogram hist(cars$speed) If you only want to display the plot in the knitted document, and not the code, you can hide the code with the option echo=FALSE in the code chunk settings (see below in {r, }). For example, you will see a boxplot in the html file but you won’t see the code needed to create the plot: Try it! Use one of the plots above to visualize the distribution of the stopping distance dist. Briefly describe what you see. # Create a basic histogram hist(cars$dist) # Or create a basic boxplot boxplot(cars$dist) In either plot, we can see that most cars had a stopping distance between 20-40 ft and 1 car had a much larger stopping distance. We could also investigate the relationship between the two numeric variables with a scatterplot: # Create a basic scatterplot (by default, variable on x-axis first, then variable on y-axis) plot(cars$speed, cars$dist) We will learn fancier visualizations (especially with labels and titles!) in the next part of this unit but basic plots are useful for simple and quick visualizations. 3. Packages Sometimes base R is not enough! R packages are collections of functions and datasets developed by the R community worldwide. You can install a package by typing the function install.packages(“package_name”) in the console panel OR specifying the option eval=FALSE in the code chunk settings (see below in {r, }). This option will not evaluate the code chunk when knitting. For example, you can install the package tidyverse (it should already be installed on the server): # Install the package install.packages(&quot;tidyverse&quot;) Note: using install.packages() in your Markdown document will prevent you from knitting so don’t include it in your document since you only need to install a package once! Or use the option eval=FALSE in the code chunk settings. Once a package is installed, you need to load the package to call specific functions from this package using library(package_name): # Load the package within the Markdown file library(tidyverse) We will learn great functions from the tidyverse package throughout the semester. Note: your R Markdown file is independent, meaning that all packages used in your document must be called within the document. If you are using functions not built in R, you’ll get an error like: object 'function_name' not found. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: https://rafalab.dfci.harvard.edu/dsbook/r-basics.html, https://rafalab.dfci.harvard.edu/dsbook/reproducible-projects-with-rstudio-and-r-markdown.html R Markdown: The Definitive Guide https://bookdown.org/yihui/rmarkdown/ R Markdown Cheatsheet https://rstudio.github.io/cheatsheets/rmarkdown.pdf "],["WS2_Datasets_Key.html", "Datasets Learning objectives 1. Set up 2. City of Austin Open Data Portal 3. Other data sources Recommended Resources:", " Datasets Learning objectives Organize the R Markdown workflow. Upload necessary packages. Import datasets into R. 1. Set up In the last worksheet, we introduced R Markdown. One important thing to know about R Markdown is that this file is independent of the environment, meaning that all objects used in the file must be created within the file. a. Packages It starts with calling the appropriate packages, usually done at the beginning of the file: # Load packages at the beginning of the file library(tidyverse) b. Importing datasets We mentioned using built-in datasets. But what if we want to explore our own data? We would have to import our data into R. The most common (and most convenient) format for datasets is csv files. The read_csv function from tidyverse allows us to upload a dataset: # Import data from a csv file read_csv(&quot;example.csv&quot;) ## Error: &#39;example.csv&#39; does not exist in current working directory (&#39;/stor/home/lg28373/322E_LG/bookdown&#39;). What could have gone wrong? R could not find a file called example.csv in the working directory. Go to Canvas and download the dataset for today’s materials. Then upload this file in the same folder as this R Markdown document (called the working directory). Then we can import the dataset: # Import data from a csv file read_csv(&quot;survey_age.csv&quot;) But what if we want to work with this dataset? We need to save it to our environment: # Import data from a csv file and save to the environment survey_age &lt;- read_csv(&quot;survey_age.csv&quot;) Try it! Plot the distribution of age with a histogram. Anything you notice in this visualization? # Make a basic histogram hist(survey_age$age) Most students are between 18 and 22 years old and a few are over 22. 2. City of Austin Open Data Portal For your project, you will use data from the City of Austin Open Data Portal. For example, let’s look at the Watershed Reach Index and Problem Scores: - Access the documentation about the dataset to learn about the information it contains. - Preview the dataset to check for the format of the variables. - Export the dataset to import it in RStudio! Note: We can import the dataset directly to RStudio with API endpoint: water_quality &lt;- read_csv(&quot;https://data.austintexas.gov/resource/vk3r-6prc.csv&quot;) Try it! Represent the distribution of index_water_quality and problem_water_quality wiht a histogram, separately. What is the difference between these two variables? # Make a histogram for each variable hist(water_quality$index_water_quality) hist(water_quality$problem_water_quality) These two variables represent different ways to assess water quality with different scales (100 is best or worst, respectively). We can only know that by looking at the documentation! What could be some potential issues for importing data directly from the portal? Be aware that if you get data directly from the web like that it might: No longer be available at some point. Be updated. It might be a better idea to export and save the file locally. 3. Other data sources Only download files from trusted websites! To assess whether a website can be trusted or not, consider the following factors: Check the source of the website: Websites of universities, government agencies, or well-known platforms (e.g., GitHub, Kaggle, and Data.gov…). Look for HTTPS and security Examine the quality of the website: Trustworthy websites often look polished and contain clearly written content, whereas scam sites may look poorly designed or filled with ads. Recommended Resources: Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: https://rafalab.dfci.harvard.edu/dsbook/importing-data.html readr Package Documentation https://readr.tidyverse.org/ Get started on how to use open data with https://data.austintexas.gov/videos "],["WS3_BasicDataManipulation_Key.html", "Basic Data Manipulation Learning objectives 1. Making sense of a dataset 2. Setting conditions 3. Subsetting data 4. Creating new variables Recommended Resources", " Basic Data Manipulation Learning objectives Understand the context and structure of a dataset before manipulating the data. Develop research questions to explore the data. Apply basic conditional statements to filter data or create new variables. Create new variables based on some conditions. This semester we will learn methods to answer questions we may have about our data. We will need to make sure that our data is appropriate to answer our research question OR/AND we will need to adjust our research question to use the data that we have. Start your workflow by uploading the tidyverse package: # Upload package library(tidyverse) 1. Making sense of a dataset Consider the built-in dataset quakes which contains information about earthquakes occurring near Fiji islands since 1964. It is always important to take a quick look at the dataset before doing anything with it: # Take a look at the data frame head(quakes) # Find the numbers of rows, columns nrow(quakes) ## [1] 1000 ncol(quakes) ## [1] 5 # or find both dimensions dim(quakes) ## [1] 1000 5 The context of our data is very important! Get more information about the dataset by running ?quakes in your console. Try it! What does one row represent? What does one column represent? One row represents the occurrence of an earthquake. A column represents a characteristic of each earthquake: where it occurred (latitude and longitude), its depth and magnitude, and how many stations reported it. What if we don’t know much about earthquakes? To understand the nature of the data we are dealing with, we should understand the context. Here is a citation for a Wikipedia page that describes the terminology around earthquakes: https://en.wikipedia.org/wiki/Earthquake Also, this dataset focuses on Earthquakes off Fiji so let’s take a look at a map of the Fiji islands: Try it! In the dataset, we are given the latitude and longitude of the earthquakes. Representing these two variables will help us identify where the earthquakes occurred. Make a plot! # Make a scatterplot plot(quakes$long, quakes$lat, main = &quot;Locating the eathquakes&quot;, xlab = &quot;Longitude&quot;, # Longitude should be on the x-axis ylab = &quot;Latitude&quot;) # Latitude should be on the y-axis Most earthquakes seem to occur around the trenches we see on the picture above. Now that we have a better idea about what data we have available, we should come up with questions we would like to answer with this dataset. Try it! Come up with a research question to guide the exploration of this dataset. Are there any limitations in the data that might affect the types of questions we can answer? One example of a research question: Do earthquakes of greater magnitude tend to occur at deeper depths? Some limitations we should consider: location of the earthquakes (only Fiji islands), data is a “subsample” of a larger dataset (how was the sample made? random?), focus on magnitude above 4, time of the earthquakes (is the dataset still updated and including recent earthquakes?), quality of information (accuracy, consistency), … Let’s demonstrate some concepts for basic data manipulation while manipulating this dataset. 2. Setting conditions a. Conditional statements We can use conditional statements to focus on some parts of the data. Below are some essential comparison operators for setting conditions in R: == means equality != means “not equal” &lt; means “less than” while &lt;= means “less than or equal to” &gt; means “greater than” while &gt;= means “greater than or equal to” x %in% y is looking for the value of x in the object y is.numeric() is testing if a variable is numeric or not is.character() is testing if a variable contains characters or not When testing for conditions, we will get a logical value: TRUE or FALSE (also abbreviated T or F). Let’s test some conditions for a value of a number x: # Create an object to test about x &lt;- 4 # Testing for equality x == 4 ## [1] TRUE x == 5 ## [1] FALSE # Testing for character is.character(x) ## [1] FALSE What if we test some conditions on a vector? # Create an object to test about v &lt;- c(4,5.1,6,4.5,4) # Testing for equality v == 4 ## [1] TRUE FALSE FALSE FALSE TRUE We get a vector of logical values. Now, when we have many elements in our vector, wouldn’t it be nice to be able to count TRUE’s and FALSE’s? A logical value of TRUE actually corresponds to a numeric value of 1, while a logical value of FALSE corresponds to a numeric value of 0. # Let&#39;s count the number of TRUE&#39;s sum(v == 4) ## [1] 2 # And find the proportion of TRUE&#39;s mean(v == 4) ## [1] 0.4 The conditional statement v == 4 returned 2 TRUE’s, or 40% of TRUE’s. Try it! What proportion of earthquakes in the quakes dataset had a magnitude greater than or equal to 6? # Find proportion with mean of TRUE&#39;s mean(quakes$mag &gt;= 6) ## [1] 0.005 About 0.5% of the earthquakes had a magnitude greater than or equal to 6. b. Connectors We can also combine logical statements with connectors: &amp; means that both statements should be TRUE | means that at least one of the statement is TRUE # Testing for equality OR greater than 4 v == 4 | v &gt; 4 ## [1] TRUE TRUE TRUE TRUE TRUE # Testing for less than 5 and greater than 5 v &lt; 5 &amp; v &gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE # Does it make sense to get what we get? Try it! How many earthquakes in the quakes dataset have a depth between 70 and 300 km (both not included)? # Count number of TRUE&#39;s with sum sum(quakes$depth &gt; 70 &amp; quakes$depth &lt; 300) ## [1] 368 368 earthquakes had a depth between 70 and 300 km. We then can use conditional statements to subset our data or create new variables. 3. Subsetting data To subset the observation of our data, we can use the filter function that comes from the tidyverse package. The function follows this structure: filter(dataframe, condition). # Index the variable mag in quakes for magnitude greater than or equal to 6 filter(quakes, mag &gt;= 6) Try it! Create a subset of quakes which contains earthquakes with a depth greater than 300 km. Save it as deep_quakes in your environment. Are all of these deep earthquakes located in the same area? Hint: check the location with a plot. # Create a new object and save it in the environment with &lt;- deep_quakes &lt;- filter(quakes, depth &gt;= 300) # Update the scatterplot plot(deep_quakes$long, deep_quakes$lat, main = &quot;Locating the deep eathquakes&quot;, xlab = &quot;Longitude&quot;, # Longitude should be on the x-axis ylab = &quot;Latitude&quot;) # Latitude should be on the y-axis The earthquakes seem to be scattered around. 4. Creating new variables To create a new column/new variable in our data, we can use the mutate function (that also comes from the tidyverse package), following this structure: mutate(dataframe, new_var = ...). For example, let’s convert depth from kilometers to miles: # Create a new variable mutate(quakes, depth_miles = depth/1.6) We can decide to create a variable depending on the result of a conditional statement (TRUE or FALSE) with the ifelse function, following this structure: ifelse(condition, value if TRUE, value if FALSE). For example, earthquakes with a depth more than 300 km are considered deep. Let’s create a categorical variable that defines the depth of an earthquake as Deep or Not deep: # Create a new variable mutate(quakes, depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) # Note: the variable is not saved in the dataframe quakes If we would like to use this new variable then we need to save a new version of the dataframe: # Save a new dataframe to use the new variable new_quakes &lt;- mutate(quakes, depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) To go beyond just TRUE/FALSE conditions, we can consider the case_when function, following this structure: case_when(condition1 ~ value if TRUE, condition2 ~ value if TRUE, ...). For example, earthquakes with a depth less than 70 km are considered shallow, with a depth more than 300 km are considered deep, and are considered of intermediate depth in between. Let’s create a categorical variable that defines the depth of an earthquake as Shallow, Intermediate, or Deep: # Update the new dataframe with this new variable new_quakes &lt;- mutate(quakes, depth_cat = case_when( depth &lt;= 70 ~ &quot;Shallow&quot;, depth &gt; 70 &amp; depth &lt;= 300 ~ &quot;Intermediate&quot;, depth &gt; 300 ~ &quot;Deep&quot; )) Try it! Create a new variable called mag_cat, splitting the magnitude into 3 categories: Light (magnitude 4 to 4.9), Moderate (magnitude 5 to 5.9), Strong (magnitude 6 to 6.9). Are there any strong earthquakes that occurred at a deep depth? What about other depths? # Create a variable new_quakes &lt;- mutate(new_quakes, mag_cat = case_when( mag &gt;= 4 &amp; mag &lt; 5 ~ &quot;Light&quot;, mag &gt;= 5 &amp; mag &lt; 6 ~ &quot;Moderate&quot;, mag &gt;= 6 &amp; mag &lt; 7 ~ &quot;Strong&quot; )) # Make a boxplot filter(new_quakes, depth_cat == &quot;Deep&quot; &amp; mag_cat == &quot;Strong&quot;) filter(new_quakes, depth_cat == &quot;Intermediate&quot; &amp; mag_cat == &quot;Strong&quot;) filter(new_quakes, depth_cat == &quot;Shallow&quot; &amp; mag_cat == &quot;Strong&quot;) No earthquake occurred at a deep depth. Only 1 occurred at a shallow depth and 4 at an intermediate depth. Next, we will learn some visualizations to explore our data! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Conditional statements, Manipulating data frames Introduction to dplyr "],["WS4_GrammarGraphics_Key.html", "Grammar Graphics Learning objectives 1. The pipe 2. Create a ggplot 3. Layering Recommended Resources", " Grammar Graphics Learning objectives Incorporate the pipe operator (|&gt;) to streamline data manipulation and plotting workflows. Explore the principles of the Grammar of Graphics with the ggplot2 package. Utilize the ggplot() function to create visualizations, defining the dataset, mapping variables to aesthetics, and defining geometric objects. Improve visualizations with labels, scales, color palettes. This semester we will learn how to make different types of visualizations to explore our data. Start your workflow by uploading the tidyverse package which contains the ggplot functions: # Upload package library(tidyverse) We will consider the built-in dataset quakes again. It contains information about earthquakes occurring near Fiji islands since 1964. To refresh our memory, let’s take a look: # Take a look at the dataset head(quakes) 1. The pipe The pipe |&gt; is a very important operator to build on code: # Filter a dataframe filter(quakes, depth &gt; 300) # Equivalent to piping quakes into filter() quakes |&gt; filter(depth &gt; 300) We will use the pipe constantly for different reasons: # Create a new variable in a dataframe mutate(quakes, depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) # Equivalent to piping quakes into mutate() quakes |&gt; mutate(depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) |&gt; # Then we can use that new variable! filter(depth_cat == &quot;Deep&quot;) 2. Create a ggplot a. Define a plot The ggplot() function helps us build a graph. Within this function, we specify the dataframe to explore: # Create a ggplot ggplot(data = quakes) Well, it’s empty because we haven’t specified how to represent the variables in the quakes object… b. Add a geometric object The geom_...() functions helps us define how to represent the variables with geometric objects (points, lines, bars, etc.). For example, geom_point() can represent our data with points: # Create a ggplot with a geometric object ggplot(data = quakes) + geom_point() Well, it does not work because we haven’t specified which variables to represent… Using aes(), aesthetics, we can define the mapping of the variables to each aesthetic of the plot. We can either define the aesthetics for each geometric object or for the whole plot: # Create a ggplot with mapping variables to a geometric object ggplot(data = quakes) + geom_point(aes(x = long, y = lat)) # which here is equivalent to: ggplot(data = quakes, aes(x = long, y = lat)) + geom_point() # Note the importance of placing &quot;+&quot; correctly In the next worksheet, we will explore additional types of geom (geom_histogram, geom_boxplot, geom_bar, …) and discuss which ones are appropriate for different types of variables. c. Map to color, shape, size We can change the appearance of some characteristics of the geometric object. # Coloring points, making them bigger ggplot(data = quakes, aes(x = long, y = lat)) + geom_point(color = &quot;blue&quot;, size = 4) Other common options include fill, shape, and alpha. Try it! In the code below, input different values between 0 and 10 for shape and alpha. What do these options control? # What does shape vs size vs alpha do? ggplot(data = quakes, aes(x = long, y = lat)) + geom_point(shape = 0.5, alpha = 0.5) The option of shape controls the shape of the points while alpha controls the transparency and only varies between 0 and 1. d. Map variables to color, shape, size More importantly, we can change the appearance of some characteristics of the geometric object depending on the values of some variables, inside the aesthetics. For example, let’s control the size of the points by the magnitude of the earthquake: # Map the size to the mag variable ggplot(data = quakes, aes(x = long, y = lat, size = mag)) + geom_point() # Note that the option for size is now within the aes() function Try it! Instead of differentiating the earthquakes based on the size of the magnitude, change the color of the points based on depth. Anything you notice in this graph? # Map the color to the depth variable ggplot(data = quakes, aes(x = long, y = lat, color = depth)) + geom_point() Earthquakes with similar depth tend to be at similar locations. Compare your plot to the following plot that considers depth as a categorical variable instead: # Use a pipe to apply a function to the object quakes quakes |&gt; # Pipe into mutate: no need to repeat the name of the dataframe mutate(depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) |&gt; # Pipe into ggplot: no need to repeat the name of the dataframe ggplot() + # Define geom and aesthetics geom_point(aes(x = long, y = lat, color = depth_cat)) How does the legend of color differ from before? When the depth is a numeric variable, the scale is called continuous with a gradient of blue colors. When the depth is a categorical variable, the scaled is called discrete with distinct colors for each category. 3. Layering We can add many layers to the ggplot and it is a good practice to put the new layers on a new line (be sure to end a line with +). a. Add labels and controlling scales Plots should be easy to interpret and informative labels are a key element in achieving this goal. The labs() function provides customized labels for titles, axes, legends, etc.: # Build a plot... ggplot(data = quakes, aes(x = long, y = lat, color = depth)) + geom_point() + # ...and add labels labs( # Title title = &quot;Distribution of the depth across the trenches&quot;, # Subtitle subtitle = &quot;For eathrquakes that occurred near the Fiji Islands since 1964&quot;, # Caption with source of data caption = &quot;Data obtained from Dr. John Woodhouse, Dept. of Geophysics, Harvard University&quot;, # Label x-axis and y-axis x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, # Legend of color color = &quot;Depth (km)&quot;) b. Control scales We can also change the scales (tick marks) for a better readability with functions starting with scale_: # Build a plot... ggplot(data = quakes, aes(x = long, y = lat, color = depth)) + geom_point() + # ...and adjust scales scale_x_continuous(limits = c(160,190), breaks = seq(160, 190, 2)) + # enforce limits between 160 and 190, show tick marks every 2 degrees scale_y_continuous(limits = c(-40,-10), breaks = seq(-40, -10, 5)) # enforce limits between -40 and -10, show tick marks every 5 degrees c. Facet Faceting reproduce a graph for each level of another variable (or combination of variables). Let’s take a look at the map for three categories of depths: quakes |&gt; # Create a variable called depth_cat mutate(depth_cat = case_when( depth &lt;= 70 ~ &quot;Shallow&quot;, depth &gt; 70 &amp; depth &lt;= 300 ~ &quot;Intermediate&quot;, depth &gt; 300 ~ &quot;Deep&quot;)) |&gt; # Create a plot ggplot() + geom_point(aes(x = long, y = lat, color = depth_cat)) + # Facet by depth category facet_wrap(~depth_cat) d. Color palettes and themes We can customize many aspects of our graphs by hand (colors, scales, background color, grid, …) or we can use some themes or palettes other than the defaults. To define our own palette of colors for numeric values, we can use scale_color_gradient() with setting the color for low values and another color for high values: # Compare values of magnitude (low: not too dangerous, high: more dangerous) ggplot(quakes, aes(x = long, y = lat, color = mag)) + geom_point() + # Change the color palette scale_color_gradient(low = &quot;yellow&quot;, high = &quot;darkred&quot;) Try it! Display the depth of earthquakes depending on their location and choosing an appropriate color palette. # Compare depth with small values represented in light color and high values in dark ggplot(quakes, aes(x = long, y = lat, color = depth)) + geom_point() # Compare depth with small values represented in light color and high values in dark ggplot(quakes, aes(x = long, y = lat, color = depth)) + geom_point() + # Change the color palette scale_color_gradient(low = &quot;lightblue&quot;, high = &quot;darkblue&quot;) The first visualization is not intuitive: small values of depth should be represented in light color and high values in dark to match our intuition. Here is a list of some of the discrete palettes that are color-blind friendly: RColorBrewer::display.brewer.all(colorblindFriendly = TRUE) We can use scale_color_brewer() when we are mapping a categorical variable to the aesthetic color =: quakes |&gt; # Create a variable called depth_cat mutate(depth_cat = case_when( depth &lt;= 70 ~ &quot;Shallow&quot;, depth &gt; 70 &amp; depth &lt;= 300 ~ &quot;Intermediate&quot;, depth &gt; 300 ~ &quot;Deep&quot; )) |&gt; # Compare categories of depth ggplot(aes(x = long, y = lat, color = depth_cat)) + geom_point() + # Change the color palette scale_color_brewer(palette = &quot;Set2&quot;) Explore and choose color palettes for categorical data using this website: https://colorbrewer2.org/ e. Themes There are so many adjustments we can make to our ggplot (change background color, color of axis, color of font, …) and sometimes it is easier to use some predefined themes: # Still same plot ggplot(quakes, aes(x = long, y = lat, color = depth)) + geom_point() + # Use the default theme_minimal() theme_minimal() Visit this website to find a list of all themes available: https://ggplot2.tidyverse.org/reference/ggtheme.html Next, we will learn different geometric objects to represent different types of data! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Introduction to data visualization, ggplot2 Data visualization with ggplot2 :: Cheat Sheet "],["WS5_Describing1Variable_Key.html", "Describing1Variable Learning objectives 1. Describing 1 numeric variable 2. Describing 1 categorical variable Your turn! Recommended Resources", " Describing1Variable Learning objectives Describe the distribution of a single numeric variable using measures of center, spread, and shape. Describe the distribution of a single categorical variable using measures of frequencies and proportions/percentages. Analyze and interpret the key differences in how various visualizations represent data. In this worksheet, we will use ggplot and some geom functions to explore univariate distributions: describing 1 variable at a time. Start your workflow by uploading the tidyverse package which contains the ggplot functions: # Upload package library(tidyverse) We will consider the built-in dataset quakes again. It contains information about earthquakes occurring near Fiji islands since 1964. To refresh our memory, let’s take a look: # Take a look at the dataset head(quakes) Let’s use some univariate graphs and summary statistics to explore the numeric variables in this dataset. 1. Describing 1 numeric variable When describing numeric variables, we pay attention to what a typical value is (center) and how the values vary from each other (spread), where values are most common and where values are rare (shape). a. Histogram Since we can have a wide range of different values, especially for continuous variables, it does not necessarily make sense to visualize what exact value is the most common but rather focus on what range of values is the most common. A histogram represents how frequent some ranges of values (called bins) are. Let’s use geom_histogram(): # Define the ggplot and the dataframe ggplot(data = quakes) + # Use geom_histogram and define mapping aesthetics (we will need more!) geom_histogram(aes(x = depth)) See the message on top of the graph? By default, the number of bins is 30 in ggplot. Can you tell which range of values is the most common? No, we can’t exactly see that because the bins do not align with the tick marks. The bins in a histogram define the ranges of values that are represented by bars. We can adjust the bins by setting the binwidth: ggplot(data = quakes) + geom_histogram(aes(x = depth), # Set bin width (we still need more!) binwidth = 100) Now can you tell which range of values is the most common? No, it is still confusing what the limits of the bins are. We can adjust how the bins are cut by specifying the center (which should always be half of the bin width): ggplot(data = quakes) + geom_histogram(aes(x = depth), # Set bin width and center (that&#39;s better!) binwidth = 100, center = 50) Which range of values appears to be the most common? Earthquakes with a depth between 0 and 100 km are the most common. Noticed how the shape of the histogram changes depending on how we define the bins? We usually recommend to have at least 10 different bins to be able to “see” the variation in our data. Try it! Create a better representation of the distribution of depth! Adjust the previous histogram to represent earthquakes with 1) bins in increments of 50 km, starting at 0, 2) color on the outline with color=\"blue\" and inside of bars with fill=\"orange\", 3) tick marks that matches each bin using scale_x_continuous(), and 4) appropriate labels for the x- and y-axes with labs(). Based on this histogram, at what depth do the earthquakes seem to be most common? ggplot(data = quakes) + geom_histogram(aes(x = depth), # 1) Set bin width and center binwidth = 50, center = 25, # 2) Color bars color = &quot;blue&quot;, fill = &quot;orange&quot;) + # 3) Adjust the tick marks of the x-axis scale_x_continuous(limits = c(0,700), breaks = seq(0,700,50)) + # 4) Labels labs(title = &quot;Distribution of the depth of the earthquakes&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Depth (in km)&quot;, y = &quot;Number of earthquakes&quot;) Earthquakes seem to be most common at a depth between 50 and 100 km (and second most common earthquakes can be found at a depth between 550 and 600 km). b. Boxplot Another common graph for numeric variables is a boxplot which represents the 5-number summary of a variable: minimum, first quartile, median, third quartile, maximum (delimiting the lower point of the whisker, the lower bar of the box, the solid bar in the box, the upper bar of the box, and the upper point of the whisker, respectively). ggplot(data = quakes) + # Use geom_boxplot and define mapping aesthetics geom_boxplot(aes(x = depth)) # Or we sometimes represent the distribution vertically for a boxplot ggplot(data = quakes) + # Switch to y position in the aesthetics geom_boxplot(aes(y = depth)) Note that one of the axes has no meaning when representing a single boxplot and the box takes the entire width. We can remove the information that is not relevant and adjust the x-axis as follows: ggplot(data = quakes) + geom_boxplot(aes(y = depth)) + # Remove labels and tick marks that have no meaning scale_x_continuous(labels = NULL, breaks = NULL, limits = c(-1,1)) Boxplots sometimes summarize the data too much (just into five numbers) and we might miss important characteristics of the data. What information are we missing compared to what we could see in the histogram for depth? We don’t see the two peaks we see in the histogram for example. Try it! Represent the distribution of the magnitude using a boxplot, adjusting the tick marks, adding appropriate labels. Anything you notice in this visualization? # Boxplot of magnitude ggplot(data = quakes) + geom_boxplot(aes(x = mag)) + # Adjust the tick marks of the axes scale_x_continuous(breaks = seq(0,7,0.2)) + scale_y_continuous(labels = NULL, breaks = NULL, limits = c(-1,1)) + # Labels labs(title = &quot;Distribution of the magnitudes of earthquakes&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Magnitude (Richter scale)&quot;, y = &quot;Number of earthquakes&quot;) **Half of the earthquakes have a magnitude less than 4.6 (median). Some earthquakes have a much stronger magnitude and appear as outliers (technically R define outliers ad values beyond Quartile + or - 1.5*IQR).** c. Density and Violin plots Sometimes, we use density and violin plots to display the “smooth” distribution of a numeric variable: ggplot(data = quakes) + # Use geom_density and define mapping aesthetics geom_density(aes(x = depth)) ggplot(data = quakes) + # Use geom_violin and define mapping aesthetics (note: it needs an x- and y-aesthetics) geom_violin(aes(x = depth, y = &quot;&quot;)) d. Reporting summary statistics In statistics, center is a measure that represents a typical value for a numeric variable (we typically report the mean or median). The mean is the arithmetic average of the numeric values: it is the sum of all data values divided by the number of observations. The median splits the data in two halves: into the lowest 50% values and the highest 50% values. # Find the mean mean(quakes$depth) ## [1] 311.371 # Find the median median(quakes$depth) ## [1] 247 Are these two values the same? different? Why/Why not? These values are different: the mean is affected by extreme (large) values so it is greater than the median. Let’s take this opportunity to introduce a new tidyverse function that will help us create summary statistics: summarize(dataframe, stats functions). quakes |&gt; # Find the mean and median summarize( mean_depth = mean(depth), median_depth = median(depth)) Another important measure to report is the spread of a numeric variable which represents how values differ from each other. In statistics, we usually use standard deviation or Interquartile Range (IQR). The standard deviation is the average distance between each data point and the mean of the dataset. The IQR splits the middle 50% of the data. (Note: the first quartile (Q1) separates the data from the lowest 25% values and the third quartile (Q3) separate the data from the highest 25% values; then IQR = Q3 - Q1). # Find the standard deviation sd(quakes$depth) ## [1] 215.5355 # Find the IQR IQR(quakes$depth) ## [1] 444 Or report all values into one output: quakes |&gt; # Summarize with different measures summarize( mean_depth = mean(depth), sd_depth = sd(depth), median_depth = median(depth), IQR_depth = IQR(depth)) Also, remember the summary() function that provides several stats at once: summary(quakes$depth) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 40.0 99.0 247.0 311.4 543.0 680.0 Note: All of these values (except the Mean) define the features of a boxplot. Finally, the shape of the distribution of a numeric variable will influence which statistics we prefer to report. We generally talk about two kinds of shapes: (approximately) symmetric (common values are in the middle with few extreme values on each side)or skewed (common values are on one side with few extreme values on the other side). We usually report mean/sd for distributions that are (approximately) symmetric and median/IQR for distributions that are more skewed. But always interpret the distributions in context! Try it! Represent the distribution of the number of stations using a histogram, adjusting the bins and tick marks, adding appropriate labels. How would you describe the shape of this distribution? Which measure of center seems to better represent a typical value for the number of stations? Write a description of this variable grounded in context. # Boxplot ggplot(data = quakes) + geom_histogram(aes(x = stations), binwidth = 10, center = 5, color = &quot;black&quot;) + # Adjust the tick marks of the x-axis scale_x_continuous(breaks = seq(0,150,10)) + # Labels labs(title = &quot;Distribution of the number of stations reporting earthquakes&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Number of stations reporting an earthquake&quot;, y = &quot;Number of earthquakes&quot;) # Summary statistics summary(quakes$stations) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.00 18.00 27.00 33.42 42.00 132.00 Description of shape: Skewed because most earthquakes have about 10-30 stations reporting them and few earthquakes have a much larger number of stations reporting them. Most appropriate value of center: The median seems to better represent the number of stations typically reporting an earthquake (27) compared to the mean (about 33.4). Description grounded in context: Half of the earthquakes had less than 27 stations reporting them. The number of stations varied from 10 to 132 with 25% of earthquakes having more than 42 stations reporting them. 2. Describing 1 categorical variable When describing categorical variables, we pay attention to which category are the most/least common. Let’s categorize depth as indicating Deep vs Not deep earthquakes: # Create a new variable quakes |&gt; mutate(depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) a. Bar plot We can represent a single categorical variable with a bar graph using geom_bar() where the height of the bar of each category represents how frequently a category appears in the dataset. quakes |&gt; mutate(depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) |&gt; # Make a bar plot ggplot() + # Use geom_bar and define mapping aesthetics geom_bar(aes(x = depth_cat)) Less earthquakes were considered as deep compared to not deep. Try it! Create the variable depth_cat that split depth into 3 categories: a depth less than 70 km is considered “Shallow”, a depth more than 300 km is considered “Deep”, and “Intermediate” in between. Also improve this plot with labels. What do you notice in this graph? quakes |&gt; # Create a variable mutate(depth_cat = case_when( depth &lt;= 70 ~ &quot;Shallow&quot;, depth &gt; 70 &amp; depth &lt;= 300 ~ &quot;Intermediate&quot;, depth &gt; 300 ~ &quot;Deep&quot;)) |&gt; # Make a bar plot ggplot() + geom_bar(aes(x = depth_cat)) + # Add labels labs(title = &quot;Distribution of the earthquakes by depth categories&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Depth category&quot;, y = &quot;Number of earthquakes&quot;) Most earthquakes were considered as deep while less earthquakes occurred in shallow depths. b. Reporting summary statistics When reporting statistics about a categorical variable, we often include counts (frequencies) or proportions (relative frequencies). We can use the function table to find counts but we first need to save the variable into a new dataframe: # Save the new variable new_quakes &lt;- quakes |&gt; mutate(depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not deep&quot;)) Then apply the table() function to this new variable: # Find counts/frequencies table(new_quakes$depth_cat) ## ## Deep Not deep ## 452 548 There are 452 earthquakes that were categorized as Deep and 548 earthquakes that were categorized as Not deep. Then we can use the function prop.table on the counts to find proportions: # Find proportions/relative frequencies prop.table(table(new_quakes$depth_cat)) ## ## Deep Not deep ## 0.452 0.548 Exactly 45.2% of the earthquakes were categorized as Deep . Another way to find the counts and proportions for each category is using another tidyverse function called group_by: combined with the summarize function that allows us to find summaries by groups (n() is used to count observations in the groups): new_quakes |&gt; # Split the data in groups group_by(depth_cat) |&gt; # Summarize per group summarize(count = n(), proportion = n() / nrow(new_quakes)) All summary statistics in one table! Try it! Create a new variable called depth_median to categorize values less than/more than the median depth. Represent the distribution of that new variable with the appropriate graph. Why does it make sense to see what we see? quakes |&gt; # Create a new variable with mutate mutate(depth_median = ifelse(depth &lt; median(depth), &quot;less than the median&quot;, &quot;more than the median&quot;)) |&gt; # Create a bar graph ggplot() + geom_bar(aes(x = depth_median)) + # Add labels labs(title = &quot;Distribution of the earthquakes by depth categories&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Comparing depth to median&quot;, y = &quot;Number of earthquakes&quot;) The median splits the data in half so it makes sense to see that there is the same amount of earthquakes in both groups. Your turn! Analyze some characteristics about students enrolled in my sections this semester in a group of 2: Download one of a datasets (or all of them) containing individual abouvariables about students enrolled in this class. Import this dataset into RStudio. Make a plot to represent the distribution of the variable and report appropriate summary statistics. # Write and submit code here! Write a description of this variable grounded in context. Write sentences here. Copy/paste your plot, statistics, and description in this slideshow. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Visualizing data distributions, Summary Statistics "],["WS6_DescribingRelationships_Key.html", "Describing Relationships Learning objectives 1. Comparing two numeric variables 2. Comparing a numeric variable and a categorical variable 3. Comparing two categorical variables 4. Relationships with more than 2 variables Recommended Resources", " Describing Relationships Learning objectives Describe the relationship between two numeric variables, using the Pearson’s correlation coefficient. Describe the relationship between a numeric and a categorical variable, reporting measures of center and spread for each category. 3. Describe the relationship between two categorical variables, using frequencies and relative proportions/percentages. Discuss how to represent more than two variables on the same visualization. In this worksheet, we will use ggplot and some more geom functions to explore relationships: describing at least 2 variables. Start your workflow by uploading the tidyverse package which contains the ggplot functions: # Upload package library(tidyverse) We will consider the built-in dataset quakes again. It contains information about earthquakes occurring near Fiji islands since 1964. To refresh our memory, let’s take a look: # Take a look at the dataset head(quakes) The type of plots we will use to explore relationships depends on the types of the variables involved. 1. Comparing two numeric variables When comparing two numeric variables, we may wonder if high values on one variable are associated with high/low values for another variable. a. Using a scatterplot The relationship between two numeric variables is typically displayed using a scatterplot with geom_point(). For example, we can investigate if the depth of an earthquake affects the magnitude of that earthquake. In this context, we are considering the depth of an earthquake as an explanatory variable for the magnitude, the response variable. By convention, the response variable is represented on the y-axis and the explanatory on the x-axis. # Define the ggplot and the dataframe ggplot(data = quakes) + # Use geom_point and define mapping aesthetics: x = explanatory, y = response geom_point(aes(x = depth, y = mag)) No matter how deep the earthquake is, the magnitude seems to vary greatly. Try it! Does the magnitude of an earthquake affects how many stations reported that earthquake? Identify what the explanatory and response variables are in this context and make an appropriate plot. What do you notice in this plot? # Define the ggplot and the dataframe ggplot(data = quakes) + # Use geom_point and define mapping aesthetics geom_point(aes(x = mag, y = stations)) In this context, we are considering the magnitude of an earthquake as an explanatory variable for the number of stations that reported the earthquake, the response variable. It looks like the higher the magnitude is, the higher number of stations reported that earthquake. b. Reporting correlation Correlation describes the strength of a (linear) relationship between two variables. With the function cor, we refer by default to the Pearson correlation coefficient which takes values between -1 (strong negative correlation) and 1 (strong positive correlation) with 0 indicating that there is no correlation. # Find the correlation between depth and magnitude cor(quakes$depth, quakes$mag) ## [1] -0.2306377 The correlation between the depth and magnitude of an earthquake is not strong (as shown in the scatterplot above). What about these perfect relationships: # Find the correlation between one variable and itself cor(quakes$depth, quakes$depth) ## [1] 1 # quickly check how the relationship looks like plot(quakes$depth, quakes$depth) # Find the correlation between one variable and its opposite cor(quakes$depth, -quakes$depth) ## [1] -1 # quickly check how the relationship looks like plot(quakes$depth, -quakes$depth) Try it! Can you guess the correlation between the magnitude and the number of stations that reported an earthquake. Calculate it! # Find the correlation between magnitude and stations cor(quakes$mag, quakes$stations) ## [1] 0.8511824 There is a strong correlation as we observed a pretty consistent pattern in the plot above. 2. Comparing a numeric variable and a categorical variable When comparing a numeric variable across categories, we may wonder if the distribution of the numeric variable (center, spread, shape) is about the same across all categories or not. Let’s consider depth as a categorical variable: # Create a new categorical variable in a new object new_quakes &lt;- quakes |&gt; # Create a variable mutate(depth_cat = case_when( depth &lt;= 70 ~ &quot;Shallow&quot;, depth &gt; 70 &amp; depth &lt;= 300 ~ &quot;Intermediate&quot;, depth &gt; 300 ~ &quot;Deep&quot;)) We already compared the magnitude of an earthquake to the depth, but we can take a different perspective to compare magnitude on categories of depth. a. Using grouped boxplots The most convenient way (using R) to compare the distribution of a numeric variable across categories is to use grouped boxplots. Let’s take a different approach to investigate if the depth affects the magnitude of an earthquake by considering depth at 3 different levels. In this context, the explanatory variable would be categorical (deep, intermediate, shallow) and the response variable would be numeric: # Define the ggplot and the dataframe ggplot(data = new_quakes) + # Use geom_boxplot and define mapping aesthetics: x = explanatory, y = response geom_boxplot(aes(x = depth_cat, y = mag)) The magnitude seems to be slightly lower on average for the deep earthquakes and slightly higher compared to earthquakes that were in shallow depth. There is a lot of overlap between these three boxplots though. We discussed in the previous worksheet that a boxplot only represent 5 numbers about a distribution and we miss information about the frequency of each value. We can add the data on a boxplot with geom_jitter: ggplot(data = new_quakes) + geom_boxplot(aes(x = depth_cat, y = mag)) + # Add the data geom_jitter(aes(x = depth_cat, y = mag)) How does “seeing the data” affect or doesn not affect our comparison? Seeing the data with geom_jitter adds more context with the distribution of individual points, helping us identify the number of earthquakes for each group, any gaps or extreme values that boxplots alone might hide. Try it! Use a plot to compare the number of stations that reported the earthquakes depending on the magnitude level: split magnitude into light (4-4.9), moderate (5-5.9), strong (6-6.9). Improve this plot with 1) representing the magnitude levels with fill colors, 2) including the data points with geom_jitter with 20% transparency, 3) adding labels to the axes with relevant units. How does the number of stations compare across the 3 categories of magnitude? # Define the ggplot and the dataframe quakes |&gt; # Create a variable mutate(mag_cat = case_when( mag &gt;= 4 &amp; mag &lt; 5 ~ &quot;Light&quot;, mag &gt;= 5 &amp; mag &lt; 6 ~ &quot;Moderate&quot;, mag &gt;= 6 &amp; mag &lt; 7 ~ &quot;Strong&quot;)) |&gt; ggplot() + # Use geom_boxplot and define mapping aesthetics # x = explanatory, y = response geom_boxplot(aes(x = mag_cat, y = stations, fill = mag_cat)) + scale_fill_manual(values = c(&quot;Light&quot; = &quot;yellow&quot;, &quot;Moderate&quot; = &quot;orange&quot;, &quot;Strong&quot; = &quot;red&quot;)) + geom_jitter(aes(x = mag_cat, y = stations), alpha = 0.2) + labs(x = &quot;Magnitude Level&quot;, y = &quot;Number of stations reporting the earthquake&quot;) There seems to be some important differences between the 3 magnitude levels with a lot of stations reporting the strong earthquakes, however, there were not many strong earthquakes overall. b. Using grouped histograms We need to be careful about grouped histograms. Let’s see why: # Define the ggplot and the dataframe ggplot(data = new_quakes) + # Use geom_histogram and define mapping aesthetics geom_histogram(aes(x = mag, fill = depth_cat), binwidth = 0.1, center = 0.05) Ugly: don’t fill histograms per categories like that! What if we create one histogram for each level of depth instead? We can easily do that with faceting: # Represent the brain weight on the x-axis ggplot(data = new_quakes) + # Use geom_histogram and define mapping aesthetics, x = response geom_histogram(aes(x = mag), binwidth = 0.1, center = 0.05) + # Facet per category facet_wrap(~depth_cat) Try it! How could we improve the grouped histograms above? ggplot(data = new_quakes) + # Use geom_histogram and define mapping aesthetics, x = response, fill = categories geom_histogram(aes(x = mag, fill = depth_cat, # Show proportion y = after_stat(count / sum(count))), # Different binwidth and center binwidth = 0.2, center = 0.1) + # Facet per category facet_wrap(~depth_cat, ncol = 1) + # show all graphs in 1 column (easier to compare histograms vertically) labs(title = &quot;Comparing the distribution of magnitude for each category of depth&quot;, x = &quot;Magnitude (Richter scale)&quot;, y = &quot;Proportion of earthquakes&quot;) Few things to improve the plot: 1) adjust the binwidth and center to have at least 10 bins but not too many, 2) add colors per categories, 3) show proportions instead of counts (however, we lose the information about how many earthquakes were in each category, 4) align the histrograms vertically, 5) add labels. c. Reporting center and spread for each category We briefly introduced the tidyverse function called group_by: combined with the summarize function that allows us to find summaries by groups: new_quakes |&gt; # Split the data in groups group_by(depth_cat) |&gt; # Summarize per group summarize(mean_mag = mean(mag), sd_mag = sd(mag)) All in one table! d. Using a special case of bar graph We can represent the mean value of a numeric variable for each category using a bar with a stat option: # Define the ggplot, the dataframe, and mapping aesthetics ggplot(data = new_quakes, aes(x = depth_cat, y = mag)) + # By default a bar represents a count but we can change what the height of a bar represents # For example, represent a statistic using the mean function geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) But as we mentioned before, we should not only focus on a typical value for a numeric variable but also consider how spread out the values are. We can add error bars to our plot with geom_errorbar() (representing +/- 1 standard deviation from the mean): ggplot(data = new_quakes, aes(x = depth_cat, y = mag)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) + # Add error bars geom_errorbar(stat = &quot;summary&quot;, fun.data = &quot;mean_se&quot;) The mean magnitude seems to be lower for the deep earthquakes and higher for the shallow depth. There does not seem to be a huge difference in the variation of magnitude across the categories of depth. Try it! Represent the mean number of stations reporting an earthquake for each level of magnitude (light, moderate, strong). Which level of magnitude has the highest number of stations on average? Which level of magnitude has the highest variation in the number of stations? quakes |&gt; # Create a variable mutate(mag_cat = case_when( mag &gt;= 4 &amp; mag &lt; 5 ~ &quot;Light&quot;, mag &gt;= 5 &amp; mag &lt; 6 ~ &quot;Moderate&quot;, mag &gt;= 6 &amp; mag &lt; 7 ~ &quot;Strong&quot;)) |&gt; ggplot(aes(x = mag_cat, y = stations, fill = mag_cat)) + # The height of a bar represents the mean geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) + # Add error bars geom_errorbar(stat = &quot;summary&quot;, fun.data = &quot;mean_se&quot;) + # Adjust the label of the y-axis accordingly labs(y = &quot;Mean number of stations&quot;) The strong earthquakes had the highest number of stations reporting these earthquakes but also had the highest variation (certainly because there were not many cases of strong earthquakes: information we are missing in this graph). 3. Comparing two categorical variables When comparing two categorical variables, we may wonder what are the most and least common categories of one variable for other categories of the other variable. Let’s consider both depth and mag as categorical variables: # Create a new categorical variable in a new object new_quakes &lt;- quakes |&gt; # Create new variables mutate(depth_cat = ifelse(depth &gt; 300, &quot;Deep&quot;, &quot;Not Deep&quot;), mag_cat = case_when( mag &gt;= 4 &amp; mag &lt; 5 ~ &quot;Light&quot;, mag &gt;= 5 &amp; mag &lt; 6 ~ &quot;Moderate&quot;, mag &gt;= 6 &amp; mag &lt; 7 ~ &quot;Strong&quot;)) a. Using a stacked plot (not usually recommended) We can create stacked bar plots that compares the distribution of two categorical variables by stacking their counts: # Define the ggplot and the dataframe ggplot(data = new_quakes) + # Use `fill =` to differentiate between categories of magnitude geom_bar(aes(x = depth_cat, fill = mag_cat)) It looks like there are mostly light magnitude across both levels of depth. When the different groups do not have the same size though, it can be difficult to compare across categories. Let’s try the side-by-side bar plot with position = \"dodge\": b. Using a segmented bar plot (usually preferred) We can display the relative distribution at each level with a segmented bar plot with the option position = \"fill\": ggplot(data = new_quakes) + # Use `fill =` to differentiate between categories of magnitude geom_bar(aes(x = depth_cat, fill = mag_cat), position = &quot;fill&quot;) Now we can see that the deep earthquakes has the highest proportion of light magnitude. Try it! Swap the variables for x = and fill =. Does the graph look any different? Note that the default label for the y-axis is count. But what does it represent? Adjust the label for the y-axis in the segmented bar plot. ggplot(data = new_quakes) + # Use `fill =` to differentiate between categories of depth geom_bar(aes(x = mag_cat, fill = depth_cat), position = &quot;fill&quot;) + labs(y = &quot;Proportion of level of depth at each magnitude&quot;) This graph looks different. In this graph, we can see what all earthquakes of the strong magnitude occurred at a lower depth. However, remember that there weren’t many strong earthquakes in this dataset, so it is important to report frequencies along relative frequencies. c. Reporting frequencies and relative frequencies When reporting statistics about two categorical variables, we often include counts (frequencies) or proportions (relative frequencies). We can use the function table to find counts: # Find frequencies table(new_quakes$depth_cat, new_quakes$mag_cat) ## ## Light Moderate Strong ## Deep 381 71 0 ## Not Deep 421 122 5 For example, out of the strong earthquakes, 0 were at a deep depth and 5 were not at a deep depth. # Find proportions (from frequency table) prop.table(table(new_quakes$depth_cat, new_quakes$mag_cat), 1) ## ## Light Moderate Strong ## Deep 0.842920354 0.157079646 0.000000000 ## Not Deep 0.768248175 0.222627737 0.009124088 prop.table(table(new_quakes$depth_cat, new_quakes$mag_cat), 2) ## ## Light Moderate Strong ## Deep 0.4750623 0.3678756 0.0000000 ## Not Deep 0.5249377 0.6321244 1.0000000 What’s the difference between these two tables? Can you associate each table with one of the two segmented bar plots above? The first table shows proportions of the magnitude of earthquakes for each depth level: the proportions are calculated per row (first segmented bar plot). The second table shows proportions of the depth of earthquakes for each magnitude level: the proportions are calculated per column (second segmented bar plot). 4. Relationships with more than 2 variables Visualizing relationships between more than two variables can be challenging, but it’s possible by incorporating additional visual elements. Some common strategies include: Color: Differentiate categories or represent continuous variables through color gradients. Facet: Recreate the same plot for different categories of one variable. Size: Vary the size of points to indicate another numeric variable. Shape: Use different shapes to represent categories, though this works best with a limited number of categories. Alpha (Transparency): Adjust transparency to handle overlapping data points. Try it! How does the magnitude and depth of an earthquakes affects the number of stations reporting an earthquake? Create an appropriate plot including these 3 variables. # Example 1: 2 numeric (points) + 1 categorical (color) ggplot(data = new_quakes) + # Color by category of depth geom_point(aes(x = mag, y = stations, color = depth_cat), alpha = 0.2) + # Labels labs(title = &quot;Relationship between magnitude, depth, and the number of stations reporting an earthquake&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Magnitude (Richter scale)&quot;, y = &quot;Number of stations reporting&quot;, color = &quot;Depth&quot;) # Example 2: 2 numeric (points) + 1 categorical (color + facet) ggplot(data = new_quakes) + # Color by category of depth geom_point(aes(x = mag, y = stations, color = depth_cat), alpha = 0.2) + # Facet by category of depth facet_wrap(~depth_cat) + # Labels labs(title = &quot;Relationship between magnitude, depth, and the number of stations reporting an earthquake&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Magnitude (Richter scale)&quot;, y = &quot;Number of stations reporting&quot;, color = &quot;Depth&quot;) # Example 3: 2 numeric (points) + 1 categorical (shape) ggplot(data = new_quakes) + # Shape by magnitude geom_point(aes(x = depth, y = stations, shape = mag_cat), alpha = 0.2) + # Labels labs(title = &quot;Relationship between magnitude, depth, and the number of stations reporting an earthquake&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Depth (km)&quot;, y = &quot;Number of stations reporting&quot;, shape = &quot;Magnitude&quot;) # Example 4: 2 numeric (points) + 1 numeric (size) ggplot(data = new_quakes) + # Size by value of magnitude geom_point(aes(x = depth, y = stations, size = mag), alpha = 0.2) + # Labels labs(title = &quot;Relationship between magnitude, depth, and the number of stations reporting an earthquake&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Depth (km)&quot;, y = &quot;Number of stations reporting&quot;, size = &quot;Magnitude (Richter scale)&quot;) # Example 5: 1 numeric and 1 categorical (grouped boxplot) + 1 categorical (facet) ggplot(data = new_quakes) + # Use geom_boxplot(aes(x = depth_cat, y = stations)) + geom_jitter(aes(x = depth_cat, y = stations), alpha = 0.2) + # Facet by category of magnitude facet_wrap(~mag_cat) + # Labels labs(title = &quot;Relationship between magnitude, depth, and the number of stations reporting an earthquake&quot;, subtitle = &quot;For 1,000 eathrquakes that occurred near the Fiji Islands since 1964&quot;, x = &quot;Magnitude Level&quot;, y = &quot;Number of stations reporting the earthquake&quot;) These five examples represent the same data with different perspectives! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Data visualization in practice, Data visualization principles R for Data Science, second edition Data visualisation "],["WS7_ExploratoryDataAnalysis_Key.html", "Exploratory Data Analysis Learning objectives 1. Gather some background information 2. Formulate some research questions 3. Investigate trends and relationships 4. Answer the questions and be critical Your turn! Recommended Resources", " Exploratory Data Analysis Learning objectives Understand the context and structure of a dataset. Formulate research questions related to the dataset. Perform exploratory data analysis (EDA) by summarizing and visualizing key variables. Interpret findings and consider biological and ecological implications. Start your workflow by uploading the tidyverse package which contains the ggplot functions: # Upload package library(tidyverse) We will consider the built-in dataset called msleep which contains information about mammals, their sleeping habits, and some other characteristics. 1. Gather some background information a. Context You can get more information about the variables in this dataset by running ?msleep in the console. What was the source for this dataset? Provide a citation for the dataset that you could include in a caption. The dataset was gathered from a study: V. M. Savage and G. B. West. A quantitative, theoretical framework for understanding mammalian sleep. Proceedings of the National Academy of Sciences, 104 (3):1051-1056, 2007. b. Structure How many rows and columns? What does one row in this dataset represent? # Find dimensions dim(msleep) ## [1] 83 11 The dataset contains 83 rows and 11 columns. One row represents one mammal. What types of variables are available? Add the units to the numeric variables listed in the table below: Variable Description name Common name genus Taxonomic rank vore Diet: carni-, herbi-, insecti-, omni-vore order Another taxonomic rank conservation Conservation status of the mammal sleep_total Total amount of daily sleep in hours sleep_rem Amount of REM sleep in hours sleep_cycle Length of sleep cycle in hours awake Amount of time spent awake in hours brainwt Brain weight in kilograms bodywt Body weight in kilograms It is always a good idea to quickly check the summary statistics for the numeric variables: # Create a general summary for each variable summary(msleep) ## name genus vore ## Length:83 Length:83 Length:83 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## order conservation sleep_total ## Length:83 Length:83 Min. : 1.90 ## Class :character Class :character 1st Qu.: 7.85 ## Mode :character Mode :character Median :10.10 ## Mean :10.43 ## sleep_rem sleep_cycle awake ## Min. :0.100 Min. :0.1167 Min. : 4.10 ## 1st Qu.:0.900 1st Qu.:0.1833 1st Qu.:10.25 ## Median :1.500 Median :0.3333 Median :13.90 ## Mean :1.875 Mean :0.4396 Mean :13.57 ## brainwt bodywt ## Min. :0.00014 Min. : 0.005 ## 1st Qu.:0.00290 1st Qu.: 0.174 ## Median :0.01240 Median : 1.670 ## Mean :0.28158 Mean : 166.136 ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] Anything you notice or wonder about? There are some missing values for some of the variables (for example, sleep_rem). Some variables have large values for max compared to the rest of the values (for example, brainwt or bodywt). 2. Formulate some research questions We formulate research questions to guide the analysis. For example, we may wonder: how does the amount of total sleep compare across the diet for these mammals? Write another research question: Another example could be: How does the body weight of a mammal affect their total amount of sleep? 3. Investigate trends and relationships After we formulate a research question, we investigate! a. First look at univariate distributions Always a good idea to understand each variable separately. Make an appropriate graph to investigate each variable involved in this research question: how does the amount of total sleep compare across the diet for these mammals? # Create a visualization for a numeric variable ggplot(msleep) + geom_histogram(aes(x = sleep_total), binwidth = 2, center = 1, color = &quot;black&quot;) + scale_x_continuous(breaks = seq(0,24,2)) + labs(title = &quot;Distribution of Sleep Duration&quot;, x = &quot;Total Sleep Time (hours)&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Provide summary statistics msleep |&gt; # Summarize with different measures summarize( mean_sleep_total = mean(sleep_total), sd_sleep_total = sd(sleep_total)) These mammals sleep an average of 10.4 hours and typically differ by 4.5 hours from the mean. Some mammals sleep less than 2 hours and some sleep up to 20 hours. # Create a visualization for a categorical variable ggplot(msleep) + geom_bar(aes(x = vore, fill =vore)) + labs(title = &quot;Distribution of Diet Types&quot;, x = &quot;Diet Type&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Provide summary statistics msleep |&gt; # Summarize with different measures group_by(vore) |&gt; # Summarize per group summarize(count = n(), proportion = n() / nrow(msleep)) Most mammals are herbivore (38.6%) and a few are insectivore (6%). The type of diet was missing for 7 mammals. b. Then look at relationships Once we learned about each variable, we can investigate the potential relationships between the variables: # Create a visualization for the relationship between a numeric and a categorical variable ggplot(msleep) + geom_boxplot(aes(x = vore, y = sleep_total, fill = vore)) + geom_jitter(aes(x = vore, y = sleep_total), alpha = 0.2) + labs(title = &quot;Sleep Duration by Diet Type&quot;, x = &quot;Diet Type&quot;, y = &quot;Total Sleep Time (hours)&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Provide summary statistics msleep |&gt; # Summarize with different measures group_by(vore) |&gt; # Summarize per group summarize( mean_sleep_total = mean(sleep_total), sd_sleep_total = sd(sleep_total)) Insectivore mammals tend to sleep more (almost 15 hours on average). Omnivore mammals are the most consistent in their total amount of sleep (with the smallest standard deviation). 4. Answer the questions and be critical Discuss any findings from your exploratory data analysis, mention anything either surprising or aligning with your expectations. While it is interesting to note that insectivore mammals tend to sleep more, it is also important to mention that there were not many mammals to learn from in this category and there is a lot of overlap between the different types of diet. What would you be curious about next? What else influence hours of sleep? Your turn! In a group of 2, conduct a quick EDA to answer one of the following questions with data from msleep. When done, copy/paste any graphs, statistics, and conclusion in this slideshow. How does the body weight of a mammal affect their total amount of sleep? # Already looked at the distribution of sleep_total # Look at the distribution of bodywt ggplot(msleep) + geom_histogram(aes(x = bodywt), binwidth = 200, center = 100, color = &quot;black&quot;) + scale_x_continuous(breaks = seq(0,7000,400)) + labs(title = &quot;Distribution of Body Weight&quot;, x = &quot;Body Weight (kilograms)&quot;) # Represent 2 numeric variables ggplot(data = msleep) + geom_point(aes(x = bodywt, y = sleep_total)) + labs(title = &quot;Relationship between Body Weight and Sleep Duration&quot;, x = &quot;Body Weight (kilograms)&quot;, y = &quot;Total Sleep Time (hours)&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Better version of the visualization ggplot(data = msleep) + # Tip: use log10 for data that is highly right-skewed geom_point(aes(x = log10(bodywt), y = sleep_total)) + labs(title = &quot;Relationship between Body Weight and Sleep Duration&quot;, x = &quot;log(body weight) in log(kg)&quot;, y = &quot;Total Sleep Time (hours)&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Provide summary statistics cor(log10(msleep$bodywt), msleep$sleep_total, use = &quot;complete.obs&quot;) ## [1] -0.5686377 There is a general trend where larger mammals tend to sleep less, but the relationship is not so strong. Some very small mammals sleep much longer than larger ones. The log transformation helps to show the pattern more clearly since body weight varies drastically. How does the body weight of a mammal differ based on their diet? # Already looked at the distribution of each variable (bodywt and vore) # Represent 1 numeric variable vs 1 categorical variable ggplot(data = msleep) + geom_boxplot(aes(x = vore, y = bodywt, fill = vore)) + geom_jitter(aes(x = vore, y = bodywt), alpha = 0.2) + labs(title = &quot;Relationship between Body Weight and Diet Types&quot;, x = &quot;Diet Type&quot;, y = &quot;Body Weight (kilograms)&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Better version of the visualization ggplot(data = msleep) + geom_boxplot(aes(x = vore, y = log10(bodywt), fill = vore)) + geom_jitter(aes(x = vore, y = log10(bodywt)), alpha = 0.2) + labs(title = &quot;Relationship between Body Weight and Sleep Duration&quot;, x = &quot;Diet Type&quot;, y = &quot;log(body weight) in log(kg)&quot;, caption = &quot;Data from V. M. Savage and G. B. West (2007)&quot;) # Provide summary statistics msleep |&gt; # Summarize with different measures group_by(vore) |&gt; # Summarize per group summarize( median_bodywt = median(bodywt), IQR_bodywt = IQR(bodywt)) Carnivore mammals tend to have larger body weights on average compared to other diets. However, the largest mammals were herbivore. Indeed, herbivores have the widest range of body sizes. The log transformation helps to show the pattern more clearly since body weight varies drastically. Choose your own question! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Exploratory Data Analysis Data to Viz leads you to the most appropriate graph for your data. "],["WS8_GrammarDataWrangling_Key.html", "Grammar Data Wrangling Learning objectives 1. Operations on rows/observations 2. Operations on columns 3. Create summaries Recommended Resources", " Grammar Data Wrangling Learning objectives Understand the importance of data wrangling to prepare datasets for analysis. Manipulate a dataset to explore its structure. Start summarizing data. In this worksheet, we will discuss the six core dplyr functions to manipulate our data. Start your workflow by uploading the tidyverse package which contains the dplyr functions: # Upload package library(tidyverse) We will consider a built-in dataset, txhousing, which contains information about the housing market in Texas. Run ?txhousing in your console for more details and take a look at the dataset: # Take a look at the first few rows of the dataset head(txhousing) What does one row in this dataset represent? One row represents the information about the real estate market for a specific city, year, and month. Looking at the documentation with ?txhousing, we are told that the information about the housing market in Texas was provided by the TAMU real estate center. The variables are defined as follows: Variables Description city Name of multiple listing service (MLS) area year, month Year, Month for the housing market data sales Number of sales volume Total value of sales median Median sale price listings Total active listings inventory Amount of time (in months) it would take to sell all current listings at current pace of sales date Date for the housing market data (year + month / 12) Take a look at general summary statistics with summary(): # Try to make sense of the values of the variables in the dataset summary(txhousing) ## city year month ## Length:8602 Min. :2000 Min. : 1.000 ## Class :character 1st Qu.:2003 1st Qu.: 3.000 ## Mode :character Median :2007 Median : 6.000 ## Mean :2007 Mean : 6.406 ## 3rd Qu.:2011 3rd Qu.: 9.000 ## sales volume median ## Min. : 6.0 Min. :8.350e+05 Min. : 50000 ## 1st Qu.: 86.0 1st Qu.:1.084e+07 1st Qu.:100000 ## Median : 169.0 Median :2.299e+07 Median :123800 ## Mean : 549.6 Mean :1.069e+08 Mean :128131 ## 3rd Qu.: 467.0 3rd Qu.:7.512e+07 3rd Qu.:150000 ## listings inventory date ## Min. : 0 Min. : 0.000 Min. :2000 ## 1st Qu.: 682 1st Qu.: 4.900 1st Qu.:2004 ## Median : 1283 Median : 6.200 Median :2008 ## Mean : 3217 Mean : 7.175 Mean :2008 ## 3rd Qu.: 2954 3rd Qu.: 8.150 3rd Qu.:2012 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] Anything you notice or wonder about? The data include years 2000-2015. There are some missing values for the variables containing real estate information. Let’s manipulate this dataset, with the 6 core dplyr functions, including some we already know! 1. Operations on rows/observations a. Filter Use filter() to choose rows/observations verifying some conditions: # Filter with one criteria txhousing |&gt; filter(city == &quot;Austin&quot;) Add more than one condition: # Filter with multiple criteria txhousing |&gt; filter(city == &quot;Austin&quot;, sales &lt;= 1000) # or using connectors txhousing |&gt; filter(city == &quot;Austin&quot; &amp; sales &lt;= 1000) We can check the number of rows that satisfy the conditions by pipping into nrow(): txhousing |&gt; # Filter with multiple criteria filter(city == &quot;Austin&quot;, sales &lt;= 1000) |&gt; # Count the rows nrow() ## [1] 2 Only 2 rows satisfied these conditions! b. Arrange Use arrange() to sort rows/observations for some variables Default is ascending (from least to greatest or alphabetically for categories) but to sort in the other direction use desc(). txhousing |&gt; # Sort by number of sales, least-to-greatest arrange(sales) txhousing |&gt; # Sort by number of sales, greatest-to-least (descending order) arrange(desc(sales)) c. Minimum/Maximum values Only keep the maximum/minimum values for some variables with slice_max()/slice_min(). # Select rows with maximum values for a variable txhousing |&gt; # Use `slice_max(number of rows, variables)` slice_max(n = 4, sales) # Select rows with minimum values for a variable txhousing |&gt; # Use `slice_min(number of rows, variables)` slice_min(n = 4, sales) Why do you think we get more than 4 rows? Because the values of sales repeat. To have the 4 lowest values of sales, we need to include any repeats. Try it! When were the lowest 5 numbers of sales for Austin? Display them in chronological order. Anything you notice about the dates? txhousing |&gt; # focus on Austin filter(city == &quot;Austin&quot;) |&gt; # minimum values slice_min(n = 5, sales) |&gt; # in chronological order arrange(date) The lowest number of sales occurred in different years but always in January. 2. Operations on columns a. Select Use select() to keep or rename a subset of columns/variables. txhousing |&gt; # Select to keep only some variables select(city, year, month, sales) txhousing |&gt; # Select to keep columns using indexes of the columns select(1:4) txhousing |&gt; # Drop variables using &quot;-&quot; select(-city, -year, -month, -date) # Select and rename... txhousing |&gt; # Use `select()` to rename some variables new_name = old_name select(Location = city, Calendar_Year = year, Month = month, Number_of_sales = sales) txhousing |&gt; # or just use rename() with the same structure new_name = old_name rename(Location = city, Calendar_Year = year, Month = month, Number_of_sales = sales) b. Mutate Use mutate() to create new columns/variables: txhousing |&gt; # Find the mean sale price per row mutate(mean_price = volume/sales) Try it! Calculate the difference between the average price as calculated above and the median sale price. Are these two measures the same? Why/Why not? txhousing |&gt; # Only keep variables of interest select(city,year,month,sales,volume,median) |&gt; # Create the average variable then find the difference mutate(mean_price = volume/sales, diff = mean_price - median) |&gt; filter(diff == 0) There are only 3 instances for which the median and mean are the same. They are usually not the same because the distribution of housing prices tend to be skewed actually. 3. Create summaries a. Summarize Use summarize() (or summarise() in British!) to calculate summary statistics on columns/variables. Some useful summary functions: mean(), sd(), median(), IQR(), min(), max(), n(), n_distinct(), cor(), … # Find the mean number of sales txhousing |&gt; summarize(mean_sales = mean(sales, na.rm = T)) # ignore NA values # Add more summaries: txhousing |&gt; summarize( # the mean mean_sales = mean(sales, na.rm = T), # the median median_sales = median(sales, na.rm = T), # the number of rows n_rows = n(), # the number of distinct cities in the dataset n_cities = n_distinct(city), # the correlation between sales and median price correlation = cor(sales, median, use = &quot;complete.obs&quot;)) Try it! Find the total number of sales for Austin in 2009. txhousing |&gt; # Focus on Austin in 2009 filter(city == &quot;Austin&quot;, year == 2009) |&gt; # the total number of sales summarize(sum_sales = sum(sales, na.rm = TRUE)) In 2009, there were 20,747 sales in the Austin area. What if we wanted to generate a similar report for each year across all cities in txhousing? Let’s use a function that allows us to create summaries per subgroup. b. Group by This is one very important function! It enables us to create subgroups and apply a function to all these subgroups For example, find summaries per city and per year: # Find summaries by subgroups txhousing |&gt; # Each year is a subgroup group_by(year) |&gt; # Create summaries for each subgroup summarize(total_sales = sum(sales, na.rm = TRUE), # total number of sales nb_rows = n()) # count how many rows in each subset Note that there are less rows in 2015. How could it influence the total number of sales during that year? The number of sales is not representative of the whole year of 2015, probably less than usual. Let’s try to be a little more specific and find the total number of sales per year and per month: # Find summaries by subgroups txhousing |&gt; # Each year/month is a subgroup group_by(year, month) |&gt; # Create summaries for each subgroup summarize(total_sales = sum(sales, na.rm = TRUE), # total number of sales nb_rows = n()) # count how many rows: what does this number correspond to? Try it! Find the total number of sales per month across all cities in txhousing, but ignoring values from 2015 since there are not complete. Then create a ggplot to show how the number of sales may vary per month. # Let&#39;s add a ggplot after using some of the dplyr functions above! txhousing |&gt; filter(year != 2015) |&gt; group_by(month) |&gt; summarize(total_sales = sum(sales, na.rm = TRUE)) |&gt; # Add a ggplot ggplot(aes(x = month, y = total_sales)) + # with points and a line geom_point() + geom_line() + # Change the scale to show each month scale_x_continuous(breaks = seq(1,12,1)) + scale_y_continuous(limits = c(0,500000)) + # Add some labels labs(y = &quot;Total monthly sales&quot;, title = &quot;Monthly sales over the years&quot;) Looks like most sales typically happen in the summer! One more example with a plot! Put comments to describe what each line of code does and add a title and labels: txhousing |&gt; # for each city group_by(city) |&gt; # calculate the total number of sales summarize(total_sales = sum(sales, na.rm = TRUE)) |&gt; # only keep the 10 cities with the highest numbers of sales slice_max(n = 5, total_sales) |&gt; # make a plot with the cities reordered depending on the number of total sales ggplot(aes(y = fct_reorder(city, total_sales), x = total_sales)) + # use bars representing that one number of total_sales geom_bar(stat = &quot;identity&quot;) + # adjust title and labels labs(x = &quot;Total number of sales&quot;, y = &quot;Location&quot;, title = &quot;Top 10 locations with highest number of total sales&quot;, subtitle = &quot;For sales between January 2000 and July 2015&quot;, caption = &quot;Data from the TAMU real estate center, https://www.recenter.tamu.edu/&quot;) Note that group_by is not just use to create summaries but that’s its most common use. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Data transformation More practie with dplyr and cheat sheets: A Grammar of Data Manipulation "],["WS9_Joining_Key.html", "Joining Learning objectives 1. Joining datasets 2. Considering other options when joining Recommended Resources", " Joining Learning objectives Identify key variables for joining datasets. Understand the purpose of joining datasets and apply different types of joins. Handle mismatched keys, auto-suffixing, and using several key variables. Start your workflow by uploading the tidyverse package which contains the joining functions: # Upload package library(tidyverse) Consider the following built-in datasets containing information about some band members of the Beatles and Rolling Stones: # Preview datasets band_members band_instruments band_instruments2 1. Joining datasets To join datasets, we first need to identify a key variable (a variable, or sometimes a set of variables, that defines a unique row in a dataset). What is the key variable to join the band_members and band_instruments datasets above? Each dataset contains the name of the band member as a variable. Check these animations: tidyexplain a. Inner join Join datasets using inner_join() to get the information they have in common: # Join 2 datasets with `inner_join()` inner_join(band_members, band_instruments, by = &quot;name&quot;) Why we only get 2 rows? There were only 2 band members common to both datasets. b. Left join Join datasets using left_join() to keep information from the “left” dataset and add information from the “right” dataset: # Join 2 datasets with `left_join()` left_join(band_members, band_instruments, by = &quot;name&quot;) There was one row in the left dataset that did not appear in the right dataset. How did R handle that? It reported a missing value, NA. Try it! Swap the left and right datasets from above. How do the resulting joined dataset compare? # Swap left and right left_join(band_instruments, band_members, by = &quot;name&quot;) We get different band members and another missing value because we keep the observations in the left dataset and add the corresponding information from the right dataset, if available. c. Right join This function does the opposite of left_join() so it is not widely used. # Join 2 datasets with `right_join()` right_join(band_members, band_instruments, by = &quot;name&quot;) Which left_join() function above gave a similar result? The one from the Try it! section: swapping the two datasets. d. Full join Join datasets using full_join() to keep information from both datasets: # Join 2 datasets with `full_join()` full_join(band_members, band_instruments, by = &quot;name&quot;) Note how R added missing values for the band and instruments that were in only one of these two datasets. e. Anti join We can use anti_join() to get information from the left dataset for which there is no information in the right dataset: # Find missing observations with `anti_join()` anti_join(band_members, band_instruments, by = &quot;name&quot;) Mick did not have an instrument in band_instruments. Try it! Find if any musician did not have a band reported in band_members. # Find missing observations with `anti_join()` anti_join(band_instruments, band_members, by = &quot;name&quot;) The band of Keith was not identified. f. Semi join What happens if we use semi_join()? # Using `semi_join()` semi_join(band_members, band_instruments, by = &quot;name&quot;) The semi_join() is like an inner_join() but only keeping the variables of the “left” dataset (not very commonly used). 2. Considering other options when joining There are some options and common issues to consider when joining different datasets. a. No matching key What happens if we are joining 2 datasets that have different names for the key variable? # Join the two datasets with different key variables left_join(band_members, band_instruments2, by = &quot;name&quot;) There is an error because R did not find any common variable (no key variable). We would need to specify the name of the key in each dataset: # Join the two datasets with different key variables left_join(band_members, band_instruments2, # and specify which variables match across datasets with `c()` by = c(&quot;name&quot; = &quot;artist&quot;)) Note: The order in which we specify the match for the key variable matters: specify the match from the left dataset to the right dataset. Also, note that only the first name of the key variable is kept in the joined dataset. b. Auto-suffixing Here is another dataset reporting instruments for each band member: # Consider this new dataset band_instruments3 &lt;- data.frame( name = c(&quot;John&quot;,&quot;Paul&quot;,&quot;Keith&quot;), plays = c(&quot;a Steinway upright piano his biggest solo hit Imagine &quot;, &quot;54 different instruments&quot;, &quot;10 out of his 3,000 guitars regularly&quot;)) band_instruments3 What happens if we are joining 2 datasets with the same variable name that is not a key variable? # Join the two variables of instruments played left_join(band_instruments, band_instruments3, by = &quot;name&quot;) Any columns that have the same name in both datasets but are not used as key variables to join will be given suffixes .x and .y to specify which original dataset they came from (left and right, respectively). You can modify the default suffix: # Join the two variables of instruments played left_join(band_instruments, band_instruments3, by = &quot;name&quot;, # To give names to the suffix, use `suffix =` suffix = c(&quot;.instrument&quot;,&quot;.fun_fact&quot;)) Note: If the same variable appears in both dataset with the same meaning, it might be a key variable! See section d. below. c. Duplicates Here is another dataset reporting a member of a new band: # Consider this new dataset band_members2 &lt;- data.frame( name = c(&quot;Mick&quot;,&quot;John&quot;,&quot;Paul&quot;,&quot;John&quot;), band = c(&quot;Stones&quot;, &quot;Beatles&quot;, &quot;Beatles&quot;, &quot;Bluesbreakers&quot;)) band_members2 Try it! Join all the information from band_members2 to the instruments they play. Is the information contained in the resulting dataset correct? # Joining 2 datasets with a duplicate key left_join(band_members2, band_instruments, by = &quot;name&quot;) The “John” that we matched between the two datasets are playing in different bands. It turns out that John Mayall did play guitar but was mainly a singer/keyboard/harp player. Check out the last guitar player touring with him, Carolyn Wonderland, who lives here in ATX! Note that it is sometimes useful to add repeating information for some rows that share the same key. We just need to be careful that it makes sense! d. Several key variables Sometimes one variable is not enough to identify a unique match. Consider this new dataset: # Add a variable to instruments band_instruments4 &lt;- band_instruments |&gt; mutate(band = c(&quot;Beatles&quot;, &quot;Beatles&quot;, &quot;Stones&quot;)) # Take a look band_instruments4 # Join it with band_members2 What key variable(s) should be taken into account to identify a unique row? We should both consider name and band. # Join 2 datasets with 2 key variables left_join(band_members2, band_instruments4, # List the key variables with `c()` by = c(&quot;name&quot;, &quot;band&quot;)) Through this worksheet, we created many datasets to play with, here’s one last one! # Information about the bands band_information &lt;- data.frame( main_band = c(&quot;Beatles&quot;,&quot;Stones&quot;), plays = c(&quot;1960-1970&quot;, &quot;1962-present&quot;)) band_information Try it! Join the information from band_members, band_instuments, band_information into one dataset. Does it matter which joining function you use? # Inner join band_members |&gt; # Join instruments inner_join(band_instruments, by = &quot;name&quot;) |&gt; # Join band information inner_join(band_information, by = c(&quot;band&quot; = &quot;main_band&quot;), suffix = c(&quot;.instrument&quot;,&quot;.years_active&quot;)) # Left join band_members |&gt; # Join instruments left_join(band_instruments, by = &quot;name&quot;) |&gt; # Join band information left_join(band_information, by = c(&quot;band&quot; = &quot;main_band&quot;), suffix = c(&quot;.instrument&quot;,&quot;.years_active&quot;)) # Full join band_members |&gt; # Join instruments full_join(band_instruments, by = &quot;name&quot;) |&gt; # Join band information full_join(band_information, by = c(&quot;band&quot; = &quot;main_band&quot;), suffix = c(&quot;.instrument&quot;,&quot;.years_active&quot;)) It does matter as we don’t get the same number of rows: some of the information was not available in all tables. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Relational data Animations of joining functions: tidyexplain Learn about the bands! The Beatles, The Rolling Stones, The Bluesbreakers. "],["WS10_Tidying_Key.html", "Tidying Learning objectives 1. Pivoting 2. Separating or uniting variables Your turn! Recommended Resources", " Tidying Learning objectives Identify if a given dataset is considered tidy or not. Transform datasets between wide and long formats using pivoting functions. Split or combine variables as needed for tidying data. Start your workflow by uploading the tidyverse package which contains the tidying functions: # Upload package library(tidyverse) The following tables all display the number of tuberculosis (TB) cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000. They contain values associated with four variables (country, year, cases, and population), but each table organizes the values in a different way: # Open the different tabular representations of the tuberculosis data table1 table2 table3 table4a table4b table5 Which of these tables are tidy? Only table1 is truly tidy. It’s the only table where each column is a variable. Note that table3 and table5 are closed to be tidy but the rate variable is not usable as is. 1. Pivoting Check these animations: tidyexplain a. Wide to long Let’s focus on table4a: # Look at table4a - Tuberculosis cases table4a The function pivot_longer() makes a dataset “longer” by increasing the number of rows and decreasing the number of columns. # Use pivot_longer() to have an observation for each country/year newtable4a &lt;- pivot_longer(table4a, # Columns in table4a to put as rows cols = c(`1999`, `2000`), # use `` for unconventional variable names # Save the columns 1999 and 2000 as values of a variable `year` names_to = &quot;year&quot;, # Save the cell values as the values of a variable `cases` values_to = &quot;cases&quot;) newtable4a Here is how we pivoted table4 into a longer, tidy form: Try it! Do the same for table4b. Think about what the numbers represent in that table to name the variable appropriately and name the resulting dataset as newtable4b. Then join newtable4a and newtable4b. Is the joined dataset tidy? If so, calculate the rate of TB cases (cases divided by population). Which country/year had the smallest rate of TB cases? # Tidy table4b - Population newtable4b &lt;- pivot_longer(table4b, cols = c(`1999`, `2000`), names_to = &quot;year&quot;, values_to = &quot;population&quot;) # Join tidy tables inner_join(newtable4a, newtable4b, by = c(&quot;country&quot;, &quot;year&quot;)) |&gt; mutate(rate = cases/population) |&gt; slice_min(n = 1, rate) Yes, the joined dataset is tidy: each row represent an observation for a country/year and we were able to calculate the rate then find the minimum rate (Afghanistan, 1999). What if we had joined table4a and table4b before tidying? # Take a look at both tables again table4a table4b Then join by the key variable: # Join untidy tables inner_join(table4a, table4b, by = &quot;country&quot;) # Improve how we join these untidy tables joined_untidy &lt;- inner_join(table4a, table4b, by = &quot;country&quot;, # Adding explicit suffixes suffix = c(&quot;_cases&quot;, &quot;_population&quot;)) joined_untidy Now, let’s try to tidy the joined dataset: # Using pivot_longer() on all columns pivot_longer(joined_untidy, # Refer to all variables with `:` cols = c(&#39;1999_cases&#39;:&#39;2000_population&#39;)) Is that dataset tidy? No, that dataset is not tidy: some rows represent cases and some rows represent population so each variable is not into its own column. We need to split values for the variable name like 1999.cases into two columns (one for year, one for cases/population). The function separate() can find the separator automatically (or we can specify the separator with sep =): # Using pivot_longer() on all columns pivot_longer(joined_untidy, cols = c(&#39;1999_cases&#39;:&#39;2000_population&#39;)) |&gt; # Distinguish between year and cases/population variables separate(name, sep = &quot;_&quot;, into = c(&quot;year&quot;, &quot;type&quot;)) The column value does not refer to a unique variable and each row does not represent one observation of country/year (for example, Afghanistan in 1999 is represented by 2 rows) We need to make the dataset wider. b. Long to wide The last dataset we created above is actually called table2: # Take a look at table2 table2 The function pivot_wider() makes datasets wider by increasing the number of columns and decreasing the number of rows. # Use pivot_wider() to have a variable for the number of cases and one for population pivot_wider(table2, # the values of the variable `type` will become variables names_from = type, # the cell values values of `count` will match the corresponding variable values_from = count) Here is how we pivoted table2 into a wider, tidy form: Try it! Do something similar for table1 so we only have one row per country. Is this resulting data tidy? Could we calculate the rate of TB cases? # Use pivot_wider() to have a variable for the number of cases and one for population pivot_wider(table1, names_from = year, # differentiate by type values_from = c(cases, population)) This resulting dataset is not tidy because the variables of cases and population are split into two columns. We could calculate the rate of TB cases for each year but we would have to calculate the rates separately: cases_1999/population_1999 and cases_2000/population_2000. These calculations would not be practical if we had data about more years. 2. Separating or uniting variables Some other functions that can help make our data tidy are separate() (see above for an example) and unite(). a. Separate As mentioned above, we can split a variable into two or more variables with separate(). R can find the separator automatically or you could specify the separator with the argument sep = \" \". # Take a look at table3 table3 Try it! Separate rate into two variables: cases and population. What do you notice about the type of the resulting variables? Why do you think that happened? Note: Add the argument convert = TRUE in separate() to convert the variables in the appropriate format. # R detects &quot;/&quot; automatically but we can specify the separator with `sep = &quot;/&quot;` separate(table3, rate, sep = &quot;/&quot;, into = c(&quot;cases&quot;, &quot;population&quot;)) # Ask R to create variables with the appropriate type separate(table3, rate, sep = &quot;/&quot;, into = c(&quot;cases&quot;, &quot;population&quot;), convert = TRUE) The variables cases and population were defined as character variables first because the original variable rate contained the symbol / which is considered as a character. b. Unite On the opposite, we can combine two variables into one with unite(). # Take a look at table5 table5 Let’s gather century and year: # Use unite() to rejoin the variables century and year created above unite(table5, new, century, year) # R places &quot;_&quot; automatically or you can specify a separator with `sep = &quot;&quot;` unite(table5, new, century, year, sep = &quot;&quot;) Your turn! Practice the tidyr functions we have learned on the billboard dataset. This built-in dataset contains songs rankings for Billboard top 100 in the year of 2000 for each week after it entered the Billboard (wk1-wk76). # Take a look at the dataset head(billboard) Why is that data not tidy? Which pivot_...() function should we use to make billboard tidy? Try it! The rank in each week after the song entered the top 100 was recorded in 76 columns which is not tidy. # Use pivot_longer billboard |&gt; pivot_longer(wk1:wk76, names_to = &quot;week&quot;, values_to = &quot;ranking&quot;) Which function should you use to split the date into three variables: year, month, day? Try it! # Use separate billboard |&gt; separate(date.entered, sep = &quot;-&quot;, into = c(&quot;year&quot;,&quot;month&quot;,&quot;day&quot;)) Which function should you use to combine the name of the artist with the name of the track, separated by :? Try it! # Use `unite()` billboard |&gt; unite(artist_track, sep = &quot;: &quot;, artist, track) In which format should billboard be if we wanted to check at what rank songs enter the Billboard top 100? Try it! What is the average ranking on the first week? What is the best ranking on the first week? # Find the max of wk1 billboard |&gt; summarize(best_rank_wk1 = min(wk1), avg_rank_wk1 = mean(wk1)) In which format should billboard be if we wanted to count how many songs per artist entered the Billboard top 100 in 2000? Try it! Which artists had more than three songs on the billboard that year? # Find the number of songs per artist billboard |&gt; group_by(artist) |&gt; summarize(n = n()) |&gt; filter(n &gt; 3) In which format should billboard be if we wanted to count how many weeks each song stayed on the Billboard top 100? Try it! Which 5 songs by which artist stayed the longest? # Best format: use pivot_longer billboard |&gt; unite(artist_track, sep = &quot;: &quot;, artist, track) |&gt; pivot_longer(wk1:wk76, names_to = &quot;week&quot;, values_to = &quot;ranking&quot;) |&gt; # Many ways to do this # Create a variable to check if a song was on the billboard mutate(is_on_billboard = ifelse(ranking &gt; 0, TRUE, FALSE)) |&gt; # Subset for each track group_by(artist_track) |&gt; # Count how many weeks the track was on billboard summarize(total_weeks = sum(is_on_billboard, na.rm = TRUE)) |&gt; # Top 5 songs slice_max(n = 5, total_weeks) In which format should billboard be if we wanted to check the most likely week for a song to reach #1 on the Billboard for the first time?? Try it! What is the average week a song reached #1? # Use the tidy version! billboard |&gt; pivot_longer(cols = wk1:wk76, names_to = &quot;week&quot;, values_to = &quot;ranking&quot;) |&gt; # Separate the week variable separate(week, sep = &quot;k&quot;, into = c(&quot;w&quot;, &quot;week_on_billboard&quot;), convert = TRUE) |&gt; # Only keep weeks on top rankings filter(ranking == 1) |&gt; # Group by song group_by(track) |&gt; # First week on top of the billboard summarize(first_week_at_1 = min(week_on_billboard, na.rm = TRUE)) |&gt; # Average number of week to get on top of the billboard summarize(mean_first_week = mean(first_week_at_1, na.rm = TRUE)) Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Tidy data Animations of joining functions: tidyexplain "],["WS11_StringsRegEx_Key.html", "Strings Reg Ex Learning objectives 1. Strings 2. Regular expressions (Regex) Recommended Resources", " Strings Reg Ex Learning objectives Manipulate and analyze strings. Detect and replace patterns in strings, using regular expressions (regex). Integrate string operations in data wrangling workflows. Start your workflow by uploading the tidyverse package which contains the functions to manipulate strings: # Upload package library(tidyverse) We will refer to some string objects and also manipulate strings within a dataframe, like txhousing: # Take a look at the distinct cities in txhousing txhousing |&gt; distinct(city) Let’s manipulate strings with functions from the stringr package: the name of these functions start with str_. 1. Strings Strings are defined with either single quotes ' or double quotes \": a. Calculating length The str_length() function can help us find the length of a string: # String length str_length(&quot;abc&quot;) ## [1] 3 # How is that different? str_length(&quot;a b c&quot;) ## [1] 5 We can apply this function to many strings contained in a vector! # String length of a vector str_length(txhousing$city) ## [1] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ## [32] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ## [ reached getOption(&quot;max.print&quot;) -- omitted 8552 entries ] We can also apply this function to create a new variable in a dataframe: # Keep track of the length of a city in a variable txhousing |&gt; # Only see distinct cities distinct(city) |&gt; # Find the length and create a new variable mutate(city_length = str_length(city)) |&gt; # Sort from shorter to longer name arrange(city_length) b. Combining strings We can use str_c() to combine two or more strings: # Combine strings str_c(&quot;Happy&quot;, &quot;Monday&quot;, &quot;!&quot;) ## [1] &quot;HappyMonday!&quot; # By default, no space but we can add the argument sep = str_c(&quot;Happy&quot;, &quot;Monday&quot;, &quot;!&quot;, sep = &quot; &quot;) ## [1] &quot;Happy Monday !&quot; Try it! Add “, TX” to all cities in txhousing. # Add state information txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Create new variable with a suffix mutate(city_tx = str_c(city, &quot;, TX&quot;)) What if we want to combine all the values of one vector/variable together? # Use the argument collapse = str_c(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), collapse = &quot;&quot;) ## [1] &quot;abc&quot; # Or separate by a comma and a space str_c(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), collapse = &quot;, &quot;) ## [1] &quot;a, b, c&quot; We can get all distinct cities in one object! # Add state information txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Pull the city as a vector pull() |&gt; # Collapse all cities together, separated by a comma and a space str_c(collapse = &quot;, &quot;) ## [1] &quot;Abilene, Amarillo, Arlington, Austin, Bay Area, Beaumont, Brazoria County, Brownsville, Bryan-College Station, Collin County, Corpus Christi, Dallas, Denton County, El Paso, Fort Bend, Fort Worth, Galveston, Garland, Harlingen, Houston, Irving, Kerrville, Killeen-Fort Hood, Laredo, Longview-Marshall, Lubbock, Lufkin, McAllen, Midland, Montgomery County, Nacogdoches, NE Tarrant County, Odessa, Paris, Port Arthur, San Angelo, San Antonio, San Marcos, Sherman-Denison, South Padre Island, Temple-Belton, Texarkana, Tyler, Victoria, Waco, Wichita Falls&quot; c. Changing cases We can change the strings from lower to uppercase and vice-versa (also use sentence case): # To lower case str_to_lower(&quot;Happy Monday!&quot;) ## [1] &quot;happy monday!&quot; # To upper case str_to_upper(&quot;Happy Monday!&quot;) ## [1] &quot;HAPPY MONDAY!&quot; # To sentence case str_to_sentence(&quot;Happy Monday!&quot;) ## [1] &quot;Happy monday!&quot; Especially useful if there is some inconsistencies in the categories of a variable! d. Subsetting strings We can focus on a subset of a string with str_sub() (only works with indexing positions though): # Select a position in the string str_sub(&quot;Happy Monday!&quot;, start = 1, end = 5) ## [1] &quot;Happy&quot; # Or count backwards with - str_sub(&quot;Happy Monday!&quot;, start = -7, end = -2) ## [1] &quot;Monday&quot; We can also split a string by finding a separator: # Split given a pattern str_split(&quot;Happy Monday!&quot;, pattern = &quot; &quot;) ## [[1]] ## [1] &quot;Happy&quot; &quot;Monday!&quot; Note that the resulting object is called a list and is difficult to manipulate within dataframes. e. Finding (exact) matches in strings Let’s start finding patterns in strings! We can find if a pattern occurs in our data with str_detect(): # Detect the matches str_detect(&quot;Monday&quot;, pattern = &quot;day&quot;) ## [1] TRUE Try it! Any cities in txhousing contain “county” in their name? If you don’t find any, try to change the names to lower cases first. txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Transform to lower case mutate(city = str_to_lower(city)) |&gt; # Filter cities based on a pattern filter(str_detect(city, &quot;county&quot;)) There are 5 “cities” with the pattern “county”. What if we want to replace a pattern with str_replace(): # Replace the matches str_replace(&quot;Monday&quot;, pattern = &quot;Mon&quot;, replacement = &quot;Tues&quot;) ## [1] &quot;Tuesday&quot; Try it! Replace Fort with Ft in the names of the cities. txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Update variable mutate(city = str_replace(city, &quot;Fort&quot;, &quot;Ft&quot;)) |&gt; # Check that it worked filter(str_detect(city, &quot;Ft&quot;)) 2. Regular expressions (Regex) Regular expressions are used to describe patterns in strings. They’re a little weird at first but they can be very useful, especially when we are looking for patterns with some flexibility. a. Wildcards Use . to match any character (except a new line): # Detect the matches in a dataframe txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Filter cities with any character and &quot;ounty&quot; filter(str_detect(city, &quot;.ounty&quot;)) b. Anchors Let’s find a match at the beginning of a string with ^ or at the end of a string with $ : # Detect the matches in a dataframe txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Filter cities starting with A filter(str_detect(city, &quot;^A&quot;)) # Detect the matches in a dataframe txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Filter cities ending with n filter(str_detect(city, &quot;n$&quot;)) Try it! What is the number of cities that start with the initial of your first name? txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Filter cities based on a pattern filter(str_detect(city, &quot;^L&quot;)) There are 4 cities that start with L. c. Flexible patterns To look for certain patterns, we will use []. Here are a few useful patterns: [0-9] matches any digit [ ] matches any single space [abc] matches a, b, or c [a-zA-Z] matches any letter, lower case or upper case [a-zA-Z0-9] matches any alphanumeric character Let’s check any city names that start with A or B: txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Filter cities starting with A or B filter(str_detect(city, &quot;^[AB]&quot;)) Try it! Find how many city names end with a vowel. txhousing |&gt; # Only distinct cities distinct(city) |&gt; # Filter cities based on a pattern filter(str_detect(city, &quot;[aeiouAEIOU]$&quot;)) There are 14 cities that end with a vowel. d. Special characters In regular expressions, some characters have special meanings (e.g., . matches any character, ^ indicates the start of a string, etc.). Sometimes, we may want to search for these special characters themselves rather than their functionality. To do this, we can “escape” them using a backslash (\\). # Actually referring to a quote for a string &#39;\\&#39;&#39; ## [1] &quot;&#39;&quot; The trick is that \\ is a special character itself so we sometimes have to use a few of those \\\\: # Compare these two pieces of code: str_replace_all(&quot;Happy Monday.&quot;, pattern = &quot;.&quot;, replacement = &quot;!&quot;) ## [1] &quot;!!!!!!!!!!!!!&quot; str_replace_all(&quot;Happy Monday.&quot;, pattern = &quot;\\\\.&quot;, replacement = &quot;!&quot;) ## [1] &quot;Happy Monday!&quot; Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Strings More practie with stringr and cheat sheets: Simple, Consistent Wrappers for Common Strings Operations More practie with regex: regex101: build, test, and debug regex "],["WS12_DatesTimes_Key.html", "Dates Times Learning objectives 1. Parsing dates and times 2. Manipulating dates and times Recommended Resources", " Dates Times Learning objectives Recognize different ways dates and times are stored in datasets. Parse and convert dates and times to handle different formats. Manipulate dates and times to extract specific components or to calculate differences. Summarize and visualize date/time Data. Start your workflow by uploading the tidyverse package which contains the functions to manipulate dates and times: # Upload package library(tidyverse) We will take a look at different dates in txhousing and in flights of New York City. # Install packages install.packages(&quot;nycflights13&quot;) # Load package library(nycflights13) Here are the datasets we will manipulate today: # Take a look at txhousing and flights head(txhousing) head(flights) In what format are dates/times reported in each dataset? There are various formats in each dataset: either separated by year, month, etc., or appearing as a combination of this information into one value (for example, date in txhousing or time_hour in flights`). 1. Parsing dates and times Parsing dates and times means converting a string or numeric representation of a date or time into a proper object that R can understand and work with. When data is read from a file, sometimes dates and times appear in formats that are not automatically recognized as dates/times. a. R formats Here is a typical date/time format in R: # Look at what date/time is now! now() ## [1] &quot;2025-04-20 14:29:57 CDT&quot; By default, R considers: dates as “yyyy-mm-dd” (year-month-day) times as “hh:mm:ss” (hours:minutes:seconds) date/times, dttm as “yyyy-mm-dd hh:mm:ss” Here are some examples of different ways to report dates as a string and to convert it as a date in an R format: # year, month, day ymd(&quot;2025-02-19&quot;) ## [1] &quot;2025-02-19&quot; # day, month, year dmy(&quot;19.2.2025&quot;) ## [1] &quot;2025-02-19&quot; dmy(&quot;19/2/2025&quot;) ## [1] &quot;2025-02-19&quot; dmy(&quot;19-2-2025&quot;) ## [1] &quot;2025-02-19&quot; dmy(&quot;the 19th of February 2025&quot;) ## [1] &quot;2025-02-19&quot; dmy(&quot;19 de febrero de 2025&quot;) # this one did not work, why? ## Warning: All formats failed to parse. No formats found. ## [1] NA # Note we can change default language/cultural conventions with `locale =` # month, day, year mdy(&quot;2/19/2025&quot;) ## [1] &quot;2025-02-19&quot; mdy(&quot;February 19th, 2025&quot;) ## [1] &quot;2025-02-19&quot; Similarly, we can convert strings into time: # date in year, month, day and time ymd_hms(&quot;2025-02-19 09:00:00 AM&quot;) ## [1] &quot;2025-02-19 09:00:00 UTC&quot; # also check other date functions with _hms or _hm, or simply the function hm() and hms() hms(&quot;09:00:00 AM&quot;) ## [1] &quot;9H 0M 0S&quot; If the date is already in a R format but appears as a string, we can change it as a date with as_date() or as_datetime(): # date in year, month, day and time class(&quot;2025-02-19 09:00:00 AM&quot;) ## [1] &quot;character&quot; # Change it as a date as_date(&quot;2025-02-19 09:00:00 AM&quot;) ## [1] &quot;2025-02-19&quot; class(as_date(&quot;2025-02-19 09:00:00 AM&quot;)) ## [1] &quot;Date&quot; # Change it as a date/time as_datetime(&quot;2025-02-19 09:00:00 AM&quot;) ## [1] &quot;2025-02-19 09:00:00 UTC&quot; class(as_datetime(&quot;2025-02-19 09:00:00 AM&quot;)) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; b. Combining date/time components We can combine the different parts of a date with make_date() or also add time with make_datetime(). # Combine year and month into a date txhousing |&gt; mutate(new_date = make_date(year, month)) By default, the day on the date was set to the first day of the month. Try it! In flights, the time_hour variable does not contain the minutes for the scheduled flights. Can you make a new variable that contains all the appropriate time information? # Combine year and month into a date flights |&gt; mutate(new_date = make_datetime(year, month, day, hour, minute)) |&gt; # Check that it worked select(year, month, day, hour, minute, time_hour, new_date) |&gt; head() c. Extracting part(s) of the date On the contrary, we might want to extract some specific date/time information from a date: # Extract year, month, day and time year(now()) ## [1] 2025 month(now()) ## [1] 4 day(now()) ## [1] 20 # What do those indicate? week(now()) ## [1] 16 wday(now()) ## [1] 1 hour(now()) ## [1] 14 minute(now()) ## [1] 29 second(now()) ## [1] 57.96388 Check the label and abbr options for month() and wkday(): # Convenient options month(now(), label = TRUE, abbr = FALSE) ## [1] April ## 12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; ... &lt; December wday(now(), , label = TRUE, abbr = TRUE) ## [1] Sun ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat Try it! On which day of the week were you born? # Write and submit code here! Write sentences here. 2. Manipulating dates and times a. Finding differences between dates and times We can find date/time differences with difftime(): # How many days between now and the first day of the semester? difftime(now(), mdy_hms(&quot;1-13-2025 08:00:00 am&quot;), units = &quot;days&quot;) ## Time difference of 97.47914 days # What if we want to find the difference with another unit? difftime(now(), mdy_hms(&quot;1-13-2025 08:00:00 am&quot;), units = &quot;weeks&quot;) ## Time difference of 13.92559 weeks difftime(now(), mdy_hms(&quot;1-13-2025 08:00:00 am&quot;), units = &quot;hours&quot;) ## Time difference of 2339.499 hours difftime(now(), mdy_hms(&quot;1-13-2025 08:00:00 am&quot;), units = &quot;mins&quot;) ## Time difference of 140370 mins difftime(now(), mdy_hms(&quot;1-13-2025 08:00:00 am&quot;), units = &quot;secs&quot;) ## Time difference of 8422198 secs Note that the output reports the time difference with a unit. If we would like to find the value of the difference, we can use the function as.numeric(): # Report only a value as.numeric(difftime(now(), mdy_hms(&quot;1-13-2025 08:00:00 am&quot;), units = &quot;days&quot;)) ## [1] 97.47914 Try it! How many days between now and your birthday? # Write and submit code here! Write sentences here. b. Summarizing date/time data Depending on the level of detail we would like to focus on, we can aggregate the data by specific time units. For example, we can compare summaries over years, months, days of the week, or by the hour, minute, second. Try it! In the flights dataset, extract the information of the weekday from the time_hour variable. On which day of the week are there more flights? # Use a plot flights |&gt; mutate(week_day = wday(time_hour, label = TRUE)) |&gt; ggplot() + geom_bar(aes(x = week_day)) # Or find summary statistics flights |&gt; mutate(week_day = wday(time_hour, label = TRUE)) |&gt; group_by(week_day) |&gt; summarize(total_flights = n()) |&gt; arrange(total_flights) There are more flights on Monday (and less flights on Saturday). Try it! We looked at the number of flights per day before. Compare the number of flights at another time unit. Do you notice any differences? # Comparing per month flights |&gt; mutate(month_label = month(time_hour, label = TRUE)) |&gt; ggplot() + geom_bar(aes(x = month_label)) # Comparing per day of the month flights |&gt; ggplot() + geom_histogram(aes(x = day), binwidth = 1, center = 0.5, color = &quot;black&quot;) # Comparing per hour of the day flights |&gt; ggplot() + geom_histogram(aes(x = hour), binwidth = 1, center = 0.5, color = &quot;black&quot;) # Comparing every 10 minutes flights |&gt; ggplot() + geom_histogram(aes(x = minute), binwidth = 1, center = 0.5, color = &quot;black&quot;) We can note that: the number of flights varies per month (seems to be related to how many days there are in a month), the number of flights varies per day (with less occurences of day 31), the number of flights varies per hour (more flights in early morning and in the middle of the afternoon), the number of flights varies per minute (with most flights scheduled on the hour and about every 5 minutes). We can also represent the values of a variable over time: # Comparing sales over time txhousing |&gt; group_by(date) |&gt; summarize(total_sales = sum(sales, na.rm = TRUE)) |&gt; ggplot() + geom_line(aes(x = date, y = total_sales)) And compare if there is the same pattern over a repeating time unit (for example, months repeat every year): # Comparing monthly sales for each year txhousing |&gt; group_by(year,month) |&gt; summarize(total_sales = sum(sales, na.rm = TRUE)) |&gt; ggplot() + geom_line(aes(x = month, y = total_sales)) + facet_wrap(~year) Try it! Compare the maximum distance for a flight per hour of the day. When do longer flights depart from NYC airports? # Comparing per hour flights |&gt; group_by(hour) |&gt; summarize(max_distance = max(distance, na.rm = TRUE)) |&gt; ggplot() + geom_bar(aes(x = hour, y = max_distance), stat = &quot;identity&quot;) The longest flights seem to be scheduled at 9am, 10am, and 1pm. c. A few remarks Here are some common pitfalls to look out for: Different date formats (e.g., MM/DD/YYYY vs. DD/MM/YYYY) can lead to incorrect parsing. Always specify the date format explicitly when converting strings to dates. Take into account that not all years are 365 days (leap years), not all months have the same amount of days, and not all days are 24 hours (daylight saving time). Most lubridate functions are designed to take those facts into account. The time is not the same depending on where the data was collected. Convert dates/times between time zones with_tz() from the lubridate package. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Dates and times More practice with lubridate and cheat sheets: Make Dealing with Dates a Little Easier "],["WS13_Recoding_Key.html", "Recoding Learning objectives 1. Consider numeric variables as categorical variables 2. Consider a log-transformations for numeric variables that are highly skewed 3. Recoding categories Your turn! Recommended Resources", " Recoding Learning objectives Understand when and why to recode variables in a dataset. Apply different methods for recoding variables under some conditions. Create ordered categorical variables. Use different resources to add more information. Start your workflow by uploading the tidyverse package which contains the functions to recode and the nycflights13 to access datasets to practice recoding: # Load packages library(tidyverse) library(nycflights13) Here are the three datasets we will manipulate today: # Take a look at flights and weather head(flights) head(airports) head(weather) Flights get delayed… What do you think could be associated with the delay of a flight? Lots of things! Weather, mechanical issues, strikes… Try it! Look at the distribution of dep_delay. What do you notice? What does the warning message indicates? ggplot(data = flights) + # Distribution of delay geom_boxplot(aes(x = dep_delay)) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). The distribution of delay is highly skewed (most delays are small but there are some outstanding delays). Also note that some delays are negative. The warning message indicates that there are 8255 rows that were not represented here: there are missing. flights |&gt; # Filter missing values filter(is.na(dep_delay)) These flights more likely represent flights that have been cancelled. There are many reasons why we may want to recode some variables. The following list is definitely not exhaustive! 1. Consider numeric variables as categorical variables We can recode a numeric variable as a categorical variable based on conditions with case_when(). flights |&gt; # Recode delay in 3 categories: more than 15-minute delay, on-time, or cancelled mutate(dep_delay_cat = case_when( dep_delay &gt; 15 ~ &quot;More than 15-minute delay&quot;, dep_delay &lt;= 15 ~ &quot;On time&quot;, is.na(dep_delay) ~ &quot;Cancelled&quot;)) |&gt; ggplot() + geom_bar(aes(x = dep_delay_cat)) Overall, flights are more likely to be on time than delayed or cancelled. Let’s also consider what might affect the delays. For example, check how the values of delays vary per month: ggplot(data = flights) + # Delays over the months geom_point(aes(x = month, y = dep_delay)) ## Warning: Removed 8255 rows containing missing values or values outside the ## scale range (`geom_point()`). There is a lot of overlap in this plot. Let’s try a boxplot for each month: ggplot(data = flights) + # Delays over the months geom_boxplot(aes(x = month, y = dep_delay)) ## Warning: Continuous x aesthetic ## ℹ did you forget `aes(group = ...)`? ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). It didn’t work! Check the first warning message: we should consider month as a group: ggplot(data = flights) + # Delays over the months as a group geom_boxplot(aes(x = month, y = dep_delay, group = month)) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). Still not great because we are comparing many months. Try it! Compare the departure delay over the year by grouping months into quarters (Jan-Apr is the first quarter for example). What do you notice in this graph? flights |&gt; # Recode month into 4 quarters mutate(month_cat = case_when( month &lt;= 3 ~ &quot;1st quarter&quot;, 4 &lt;= month &amp; month &lt;= 6 ~ &quot;2nd quarter&quot;, 7 &lt;= month &amp; month &lt;= 9 ~ &quot;3rd quarter&quot;, 10 &lt;= month &amp; month &lt;= 12 ~ &quot;4th quarter&quot;)) |&gt; # Use this new variable in a ggplot ggplot() + geom_boxplot(aes(x = month_cat, y = dep_delay)) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). It looks like the values of extreme delays decrease over the year. 2. Consider a log-transformations for numeric variables that are highly skewed When a variable is highly skewed, it might be difficult to “see” the variation: ggplot(data = flights) + # Distribution of delay geom_boxplot(aes(x = dep_delay)) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). For distributions of numeric variables that are heavily right-skewed like departure delay we can apply a log-transformation: ggplot(data = flights) + # Distribution of log-transformation for delay geom_boxplot(aes(x = log10(dep_delay))) # represent log(dep_delay) ## Warning in FUN(X[[i]], ...): NaNs produced ## Warning: Removed 208344 rows containing non-finite outside the scale range ## (`stat_boxplot()`). What is the unit of this new variable? The unit would be log(minutes), not intuitive. Applying this transformation created many missing values (indicated by the first warning message) because log10() can only be applied to positive values. Here is a quick fix: ggplot(data = flights) + # Distribution of log-transformation for delay, adjusting for positive values geom_boxplot(aes(x = log10(dep_delay - min(dep_delay, na.rm = TRUE) + 1))) # shift values of the variable so all the values of positive before considering log ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). Now what is the unit of this new variable? The unit would be log(minutes-minimum+1), so not intuitive. Note that this log-transformation only works for right-skewed data. It is mostly useful when making comparisons. Try it! Compare the delay across the 3 airports of origins without a transformation, then again with transformation. Are all airports comparable in terms of delayed flights? ggplot(data = flights) + # Compare airports without transformation geom_boxplot(aes(x = origin, y = dep_delay)) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). ggplot(data = flights) + # Compare airports with transformation geom_boxplot(aes(x = origin, y = log10(dep_delay - min(dep_delay, na.rm = TRUE) + 1))) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_boxplot()`). It is easier to compare the median for the log-transformed variable. Even though there is still a lot of overlap across the 3 airports, we can better see that LGA tends to have slightly lower values of delay. 3. Recoding categories We can reorder some of the categories if we would like. For example, in terms of total passenger traffic and size, the order from biggest to smallest is JFK, EWR, LGA: flights |&gt; # Recode airports of origin in a certain order mutate(origin_name = factor(origin, levels = c(&quot;JFK&quot;, &quot;EWR&quot;, &quot;LGA&quot;))) |&gt; # Use this new variable in a ggplot ggplot(aes(x = origin_name, y = dep_delay)) + # Report mean departure delay for each airport of origin geom_bar(stat = &quot;summary&quot;, fun = mean) + labs(y = &quot;Mean departure delay&quot;) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_summary()`). The smallest airport,La Guardia, has the lowest mean departure delay. We can also recode some values of the categories with recode() (and we don’t need to recode all values actually!). flights |&gt; # Recode origin with names mutate(origin_name = dplyr::recode(origin, &quot;EWR&quot; = &quot;Newark Liberty Intl&quot;, &quot;JFK&quot; = &quot;John F Kennedy Intl&quot;, &quot;LGA&quot; = &quot;La Guardia&quot;)) |&gt; # Use this new variable in a ggplot ggplot(aes(x = origin_name, y = dep_delay)) + # Report mean departure delay for each airport of origin geom_bar(stat = &quot;summary&quot;, fun = mean) + labs(y = &quot;Mean departure delay&quot;) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_summary()`). Note that the names are displayed in alphabetical order. Or instead of recoding the names of the airports by “hand”, we could import the names from the airports dataset: flights |&gt; # Join names of airports left_join(airports, by = c(&quot;origin&quot; = &quot;faa&quot;)) |&gt; # Report mean departure delay for each airport of origin ggplot(aes(x = name, y = dep_delay)) + geom_bar(stat = &quot;summary&quot;, fun = mean) + labs(y = &quot;Mean departure delay&quot;) ## Warning: Removed 8255 rows containing non-finite outside the scale range ## (`stat_summary()`). Your turn! In a group of 2, investigate if the weather had an impact on the departure delays! # Take a look at the dataset head(weather) Choose one of the weather variables that you think might affect the delay. Take a look at the distribution of this variable. Describe anything you notice about this variable. # Use wind speed ggplot(data = weather) + geom_boxplot(aes(x = wind_speed)) ## Warning: Removed 4 rows containing non-finite outside the scale range ## (`stat_boxplot()`). summary(weather$wind_speed) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 6.905 10.357 10.518 13.809 1048.361 4 The maximum value of wind speed does not make sense! There are also 4 missing values of wind speed. Join the hourly weather data to each flight. What key variables and what joining function should we use? We should use both the origin and time_hour to join the weather information corresponding to the time of each flight. # Join flights_weather &lt;- flights |&gt; # Join the weather data by origin and hour left_join(weather, by = c(&quot;origin&quot;, &quot;time_hour&quot;)) Explore the relationship between the weather variable you chose above and departure delay. You can use any recoding to improve the comparison. Describe anything you notice about this relationship. flights_weather |&gt; # Compare wind speed and departure delay ggplot(aes(x = wind_speed, y = dep_delay)) + geom_point() ## Warning: Removed 9861 rows containing missing values or values outside the ## scale range (`geom_point()`). There is a lot of overlap but high wind speed is not necessarily associated with high delays. flights_weather |&gt; # Categorize departure delay mutate(dep_delay_cat = case_when( dep_delay &gt; 15 ~ &quot;More than 15-minute delay&quot;, dep_delay &lt;= 15 ~ &quot;On time&quot;, is.na(dep_delay) ~ &quot;Cancelled&quot;)) |&gt; # Compare wind speed and departure delay ggplot(aes(x = log10(wind_speed+1), fill = dep_delay_cat)) + geom_boxplot() ## Warning: Removed 1634 rows containing non-finite outside the scale range ## (`stat_boxplot()`). The log-transformation of wind speed seems to be higher for flights that are delayed or cancelled. flights_weather |&gt; # Categorize wind speed and departure delay mutate(wind_speed_cat = case_when( wind_speed &gt; 30 ~ &quot;Strong&quot;, wind_speed &lt;= 30 &amp; wind_speed &gt; 10 ~ &quot;Moderate&quot;, wind_speed &lt;= 10 ~ &quot;Light&quot;), dep_delay_cat = case_when( dep_delay &gt; 15 ~ &quot;More than 15-minute delay&quot;, dep_delay &lt;= 15 ~ &quot;On time&quot;, is.na(dep_delay) ~ &quot;Cancelled&quot;)) |&gt; # Compare wind speed and departure delay ggplot(aes(x = wind_speed_cat, fill = dep_delay_cat)) + geom_bar(position = &quot;fill&quot;) There is a greater proportion of flights that are delayed or cancelled when there are strong winds. Copy/paste your last plot and description in this slideshow. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Review Data transformation and Factors "],["WS14_MissingDataExtreme_Key.html", "Missing Data Extreme Learning objectives 1. Missing data 2. Extreme values Recommended Resources", " Missing Data Extreme Learning objectives Identify missing data in a dataset and understand its potential impact on analysis. Handle missing data using different approaches (removal, imputation, and alternative calculations). Evaluate whether extreme values are meaningful or erroneous. Start your workflow by uploading the tidyverse package for data wrangling: # Load packages library(tidyverse) We will investigate a dataset that was retrieved from the National Centers for Environmental Information https://www.ncei.noaa.gov/ about various daily weather parameters recorded in Austin, Texas, between January 1st, 2000 and December 31st, 2015. # Upload the weather dataset from my GitHub to your environment daily_atxweather &lt;- read_csv(&quot;https://raw.githubusercontent.com/laylaguyot/datasets/main//atx_weather.csv&quot;) # Take a look! head(daily_atxweather) Here is a description of the variables included in this dataset: Variables Description date date in AWND average daily wind speed (mph) PRCP total precipitation (in) SNOW total snowfall (in) TAVG average of hourly temperature (°F) TMAX max temperature (°F) TMIN min temperature (°F) TSUN total sunshine (minutes) Visit the following page for the documentation of how the data is collected and recorded: https://www.ncei.noaa.gov/data/daily-summaries/doc/GHCND_documentation.pdf. In a perfect world, this type of documentation would exist for all datasets! 1. Missing data It is pretty common to have missing data. We need to check for any underlying reasons and consider those when deciding how to handle missing values. a. R messages In R, the philosophy is that missing values should never silently go missing. Check the number of missing values for each variable: # Summary of the data shows the number of NA values for numeric variables summary(daily_atxweather) ## station date AWND ## Length:5844 Length:5844 Min. : 0.220 ## Class :character Class :character 1st Qu.: 5.140 ## Mode :character Mode :character Median : 7.380 ## Mean : 7.999 ## 3rd Qu.:10.290 ## PRCP SNOW TAVG ## Min. : 0.00000 Min. :0.0000000 Min. : 0.00 ## 1st Qu.: 0.00000 1st Qu.:0.0000000 1st Qu.:57.00 ## Median : 0.00000 Median :0.0000000 Median :71.00 ## Mean : 0.09191 Mean :0.0001712 Mean :67.95 ## 3rd Qu.: 0.00000 3rd Qu.:0.0000000 3rd Qu.:81.00 ## TMAX TMIN TSUN ## Min. : 28.0 Min. :10.00 Min. : 0.0 ## 1st Qu.: 71.0 1st Qu.:43.00 1st Qu.: 210.0 ## Median : 83.0 Median :59.00 Median : 518.0 ## Mean : 80.5 Mean :55.67 Mean : 548.4 ## 3rd Qu.: 93.0 3rd Qu.:70.00 3rd Qu.: 662.0 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] That’s why if there is any missing value in the input, the output will be a missing value. For example, we have come across that issue many times by now, what happens if we calculate the mean of a variable with some missing values? mean(daily_atxweather$TAVG) ## [1] NA We discussed how to compute the mean, ignoring missing values, with the argument na.rm = TRUE. mean(daily_atxweather$TAVG, na.rm = TRUE) ## [1] 67.94998 What happens when there are missing values in a ggplot? # How do missing values appear in a boxplot? ggplot(daily_atxweather) + geom_boxplot(aes(x = TAVG)) ## Warning: Removed 2805 rows containing non-finite outside the scale range ## (`stat_boxplot()`). The warning message indicates that there was an issue with 2 values (the missing values). Try it! Create a new variable, freezing, to keep track of freezing average temperatures (less than 32 degrees Fahrenheit). Then represent this new variable in a bar plot. How does R let you know about missing values? # How do missing values appear in a bar plot daily_atxweather |&gt; mutate(freezing = ifelse(TAVG &lt; 32, &quot;freezing&quot;, &quot;not freezing&quot;)) |&gt; ggplot() + geom_bar(aes(x = freezing)) R shows how many values were missing within the bar plot. b. Making sense of the structure It’s always a good idea to check the number of rows and columns that we expect. Try it! If the weather data is recorded daily from January 1st, 2000 to December 31st, 2015, how many rows do we expect? Does it match the actual number of rows in daily_atxweather? # Check if there is a match: 16 years of 365 days + 4 extra days (leap years) 16*365 + 4 == nrow(daily_atxweather) ## [1] TRUE # or using difftime difftime(mdy(&quot;12-31-2015&quot;), mdy(&quot;01-01-2000&quot;), units = &quot;days&quot;) + 1 ## Time difference of 5844 days There is a match! Is there any reason why some average temperatures are missing? Let’s check for patterns: # Check for missing values per year daily_atxweather |&gt; mutate(year = year(mdy(date))) |&gt; group_by(year) |&gt; summarize(missing = sum(is.na(TAVG))) It looks like there was an issue with reporting average temperatures between 2005 and 2013. c. Handling missing values There is no perfect way of handling missing values. But we should always keep the context in consideration. How would removing missing values from the dataset potentially affect our data analysis? Removing missing data would remove some part of our data. For example, the average of hourly temperatures, TAVG, has many values missing for some years and is only partially available for other years. Calculating a maximum value over a year with partial data available is probably going to report something inaccurate. We could just decide not using a variable if it has many values missing or we could find a way around! Let’s consider a daily average temperature as the average of the minimum temperature and maximum temperature instead: # Create a new variable in daily_atxweather daily_atxweather |&gt; mutate(TAVG_calc = (TMIN + TMAX)/2) |&gt; # Double check it worked! select(date,TAVG,TMIN,TMAX,TAVG_calc) The calculated values seem pretty close to the hourly average! But no missing values for that new variable! Try it! There are 2 values of snowfall that are missing, take a look at these values. Is there a value that would be reasonable to replace these missing values? If so, replace the missing values in a new variable called SNOW_recoded. # Check the missing values of SNOW daily_atxweather |&gt; filter(is.na(SNOW)) # Replace the missing values of SNOW with 0 daily_atxweather |&gt; mutate(SNOW_recoded = ifelse(is.na(SNOW), 0, SNOW)) |&gt; # Double check it worked! filter(is.na(SNOW)) |&gt; select(date,SNOW,SNOW_recoded) It is reasonable to assume the snowfall was 0 in on these days because those occurred in the summer. If it makes sense to remove missing values, we could choose to remove all rows containing any missing values (for any variable): # Using na.omit() daily_atxweather |&gt; na.omit() # Using drop_na() daily_atxweather |&gt; drop_na() Just be careful that it makes sense and that we are not omitting important information! 2. Extreme values a. Checking extreme values It’s always a good idea to check basic summary statistics to see if there is any values that may seem surprising: # Generate basic summary statistics summary(daily_atxweather) ## station date AWND ## Length:5844 Length:5844 Min. : 0.220 ## Class :character Class :character 1st Qu.: 5.140 ## Mode :character Mode :character Median : 7.380 ## Mean : 7.999 ## 3rd Qu.:10.290 ## PRCP SNOW TAVG ## Min. : 0.00000 Min. :0.0000000 Min. : 0.00 ## 1st Qu.: 0.00000 1st Qu.:0.0000000 1st Qu.:57.00 ## Median : 0.00000 Median :0.0000000 Median :71.00 ## Mean : 0.09191 Mean :0.0001712 Mean :67.95 ## 3rd Qu.: 0.00000 3rd Qu.:0.0000000 3rd Qu.:81.00 ## TMAX TMIN TSUN ## Min. : 28.0 Min. :10.00 Min. : 0.0 ## 1st Qu.: 71.0 1st Qu.:43.00 1st Qu.: 210.0 ## Median : 83.0 Median :59.00 Median : 518.0 ## Mean : 80.5 Mean :55.67 Mean : 548.4 ## 3rd Qu.: 93.0 3rd Qu.:70.00 3rd Qu.: 662.0 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] Notice anything surprising? Some extremely high values for precipitation and sunshine compared to the rest of the values. Let’s investigate variables that may have some extreme values. Try it! Take a look at the 10 highest values for PRCP. Does the maximum value seem reasonable? # Check high values of PRCP daily_atxweather |&gt; slice_max(n = 10, PRCP) It might seem surprising but there was actually a flood that day! See https://data.austintexas.gov/stories/s/2015-Halloween-Flood/m4yz-eydu/ Then consider the maximum value for the daily amount of sunshine. Since it was recorded in minutes and knowing that the longest day is about 17 hours long in Austin, what would be the maximum value that is plausible? # Maximum value of sunshine to expect 17*60 ## [1] 1020 # Maximum value of sunshine in weather data max(daily_atxweather$TSUN, na.rm = TRUE) ## [1] 9916 That’s not possible to observe such a high value! # Check the rows with more than 17 hours of sunshine (or more than 1020 minutes) daily_atxweather |&gt; filter(TSUN &gt; 1020) |&gt; arrange(desc(TSUN)) |&gt; select(date, TSUN) Some of the highest observations are even reported in winter time… Do you think it is reasonable to drop the values for TSUN that are less than 1020? Why or why not? If we can’t explain why these values do not make sense, it is probably not a variable that is well defined and we should not rely on this variable in our analysis. b. Representing extreme values Take a look at the distribution of a variable with some extreme values: # Check distribution of PRCP daily_atxweather |&gt; ggplot() + geom_boxplot(aes(x = PRCP)) Let’s transform the distribution of this variable: # Check distribution of PRCP daily_atxweather |&gt; ggplot() + geom_boxplot(aes(x = log(PRCP+1))) Still not great because there are many days without rain. What if we only focus on the days with precipitation: # Check distribution of PRCP when greater than 0 daily_atxweather |&gt; filter(PRCP &gt; 0) |&gt; ggplot() + geom_boxplot(aes(x = PRCP)) #geom_boxplot(aes(x = log(PRCP))) # with transformation c. Handling extreme values Again, handling extreme values should rely on the context of our data. Always check that extreme values make sense. If they don’t, we can choose to remove them from the analysis. Because some extreme values can refer to exceptional conditions we might not be interested in, we could choose to remove them but we need to justify why we remove them. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Gallery of Missing Data Visualisations "],["WS15_WebScraping_Key.html", "Web Scraping Learning objectives 1. Web scraping data with code 2. Web scraping and ethics Recommended Resources", " Web Scraping Learning objectives Extract and process text data from web pages. Understand the basics of web scraping using the rvest package. Identify challenges associated with web scraping and data extraction. Start your workflow by uploading the tidyverse package for data wrangling: # Load packages library(tidyverse) We will also use the rvest package so first install it: # Install packages (only needed once!) install.packages(&quot;rvest&quot;) Then load the package: # Load packages library(rvest) We will use different sources for web data so let’s just call them when we need them! 1. Web scraping data with code With the rvest package, we can: Read HTML source code from a website. Break it into a nice structure. Extract the specific elements we want to analyze. Let’s work with a simple example first, the countries of the world and read the HTML content of this page in R: read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) ## {html_document} ## &lt;html lang=&quot;en&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; ... ## [2] &lt;body&gt;\\n &lt;nav id=&quot;site-nav&quot;&gt;&lt;div class=&quot;container&quot;&gt;\\n ... ## [3] &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/1 ... ## [4] &lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3. ... ## [5] &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/pnotify ... ## [6] &lt;link href=&quot;https://cdnjs.cloudflare.com/ajax/libs/pnotify/ ... ## [7] &lt;script type=&quot;text/javascript&quot;&gt;\\n \\n PNotify.prototyp ... ## [8] &lt;script type=&quot;text/javascript&quot;&gt;\\n $(&quot;video&quot;).hover(funct ... ## [9] &lt;script&gt;\\n (function(i,s,o,g,r,a,m){i[&#39;GoogleAnalyticsOb ... ## [10] &lt;script&gt;\\n !function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fb ... ## [11] &lt;noscript&gt;&lt;img height=&quot;1&quot; width=&quot;1&quot; style=&quot;display:none&quot; sr ... ## [12] &lt;script type=&quot;text/javascript&quot;&gt;\\n /* &lt;![CDATA[ */\\n v ... ## [13] &lt;script type=&quot;text/javascript&quot; src=&quot;//www.googleadservices. ... ## [14] &lt;noscript&gt;\\n &lt;div style=&quot;display:inline;&quot;&gt;\\n &lt;img hei ... ## [15] &lt;script async src=&quot;https://www.googletagmanager.com/gtag/js ... ## [16] &lt;script&gt;\\n window.dataLayer = window.dataLayer || [];\\n ... a. Using element tags We can select some elements of this page. Without a deep knowledge of HMTL, we can use a simple tool to differentiate between all the different elements: Download Chrome’s Selector Gadget extension, which lets you easily identify the HTML “tag” for a pattern of elements that you want to scrape. Open the webpage you want to scrape from and click on the “Developer Tools”, the puzzle piece to the right of the address bar. Click on an element you want to scrape. If anything is highlighted yellow that you don’t want, click it to remove it from your selection. Once you see the element tag in the bar at the bottom, we can now scrape those with html_elements: read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-name&quot;) ## {xml_nodeset (250)} ## [1] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [2] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [3] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [4] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [5] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [6] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [7] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [8] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [9] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [10] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [11] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [12] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [13] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [14] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [15] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [16] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [17] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [18] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [19] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## [20] &lt;h3 class=&quot;country-name&quot;&gt;\\n &lt;i c ... ## ... We can keep text only with html_text: read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-name&quot;) |&gt; html_text(trim = TRUE) ## [1] &quot;Andorra&quot; ## [2] &quot;United Arab Emirates&quot; ## [3] &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; ## [5] &quot;Anguilla&quot; ## [6] &quot;Albania&quot; ## [7] &quot;Armenia&quot; ## [8] &quot;Angola&quot; ## [9] &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; ## [11] &quot;American Samoa&quot; ## [12] &quot;Austria&quot; ## [13] &quot;Australia&quot; ## [14] &quot;Aruba&quot; ## [15] &quot;Åland&quot; ## [16] &quot;Azerbaijan&quot; ## [17] &quot;Bosnia and Herzegovina&quot; ## [18] &quot;Barbados&quot; ## [19] &quot;Bangladesh&quot; ## [20] &quot;Belgium&quot; ## [21] &quot;Burkina Faso&quot; ## [22] &quot;Bulgaria&quot; ## [23] &quot;Bahrain&quot; ## [24] &quot;Burundi&quot; ## [25] &quot;Benin&quot; ## [26] &quot;Saint Barthélemy&quot; ## [27] &quot;Bermuda&quot; ## [28] &quot;Brunei&quot; ## [29] &quot;Bolivia&quot; ## [30] &quot;Bonaire&quot; ## [31] &quot;Brazil&quot; ## [32] &quot;Bahamas&quot; ## [33] &quot;Bhutan&quot; ## [34] &quot;Bouvet Island&quot; ## [35] &quot;Botswana&quot; ## [36] &quot;Belarus&quot; ## [37] &quot;Belize&quot; ## [38] &quot;Canada&quot; ## [39] &quot;Cocos [Keeling] Islands&quot; ## [40] &quot;Democratic Republic of the Congo&quot; ## [41] &quot;Central African Republic&quot; ## [42] &quot;Republic of the Congo&quot; ## [43] &quot;Switzerland&quot; ## [44] &quot;Ivory Coast&quot; ## [45] &quot;Cook Islands&quot; ## [46] &quot;Chile&quot; ## [47] &quot;Cameroon&quot; ## [48] &quot;China&quot; ## [49] &quot;Colombia&quot; ## [50] &quot;Costa Rica&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 200 entries ] Similarly, let’s scrape the capitals: read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-capital&quot;) |&gt; html_text(trim = TRUE) ## [1] &quot;Andorra la Vella&quot; &quot;Abu Dhabi&quot; ## [3] &quot;Kabul&quot; &quot;St. John&#39;s&quot; ## [5] &quot;The Valley&quot; &quot;Tirana&quot; ## [7] &quot;Yerevan&quot; &quot;Luanda&quot; ## [9] &quot;None&quot; &quot;Buenos Aires&quot; ## [11] &quot;Pago Pago&quot; &quot;Vienna&quot; ## [13] &quot;Canberra&quot; &quot;Oranjestad&quot; ## [15] &quot;Mariehamn&quot; &quot;Baku&quot; ## [17] &quot;Sarajevo&quot; &quot;Bridgetown&quot; ## [19] &quot;Dhaka&quot; &quot;Brussels&quot; ## [21] &quot;Ouagadougou&quot; &quot;Sofia&quot; ## [23] &quot;Manama&quot; &quot;Bujumbura&quot; ## [25] &quot;Porto-Novo&quot; &quot;Gustavia&quot; ## [27] &quot;Hamilton&quot; &quot;Bandar Seri Begawan&quot; ## [29] &quot;Sucre&quot; &quot;Kralendijk&quot; ## [31] &quot;Brasília&quot; &quot;Nassau&quot; ## [33] &quot;Thimphu&quot; &quot;None&quot; ## [35] &quot;Gaborone&quot; &quot;Minsk&quot; ## [37] &quot;Belmopan&quot; &quot;Ottawa&quot; ## [39] &quot;West Island&quot; &quot;Kinshasa&quot; ## [41] &quot;Bangui&quot; &quot;Brazzaville&quot; ## [43] &quot;Bern&quot; &quot;Yamoussoukro&quot; ## [45] &quot;Avarua&quot; &quot;Santiago&quot; ## [47] &quot;Yaoundé&quot; &quot;Beijing&quot; ## [49] &quot;Bogotá&quot; &quot;San José&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 200 entries ] Now we can put all the information about the countries and their capitals in the same dataset: # Put it all in a dataframe data.frame( &quot;names&quot; = read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-name&quot;) |&gt; html_text(trim = TRUE), &quot;capitals&quot; = read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-capital&quot;) |&gt; html_text(trim = TRUE)) Try it! Scrape the population and area for each country then calculate the population density. Which 5 countries have the highest population density in the world? # Put it all in a dataframe data.frame( &quot;names&quot; = read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-name&quot;) |&gt; html_text(trim = TRUE), &quot;population&quot; = read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-population&quot;) |&gt; html_text(trim = TRUE), &quot;area&quot; = read_html(&quot;https://www.scrapethissite.com/pages/simple/&quot;) |&gt; html_elements(&quot;.country-area&quot;) |&gt; html_text(trim = TRUE)) |&gt; # Create a new variable mutate(density = round(as.numeric(population)/as.numeric(area), 0)) |&gt; # Highest population density slice_max(density, n = 5) Some of these countries are actually really small (Monaco, Gibraltar, Vatican City). Other websites are not as easy to scrape. Check free parties for SXSW: # Put names and times in a dataframe data.frame( &quot;names&quot; = read_html(&quot;https://do512.com/sxswfree&quot;) |&gt; html_elements(&quot;.ds-listing-event-title-text&quot;) |&gt; html_text(trim = TRUE), &quot;time&quot; = read_html(&quot;https://do512.com/sxswfree&quot;) |&gt; html_elements(&quot;.dtstart&quot;) |&gt; html_text(trim = TRUE)) Can you get the date? I couldn’t… b. Using element types With a little more knowledge of HTML elements, we can extract specific data from web pages as well: &lt;p&gt; for paragraphs &lt;a href=...&gt; for hyperlinks &lt;img&gt; for images &lt;table&gt; for tabular data Let’s look at the webpage that lists all R packages on CRAN: # Webpage with all R packages read_html(&quot;https://cran.r-project.org/web/packages/available_packages_by_name.html&quot;) ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;title&gt;CRAN: Available Packages By Name&lt;/title&gt;\\n&lt;li ... ## [2] &lt;body lang=&quot;en&quot;&gt;\\n&lt;div class=&quot;container&quot;&gt;\\n&lt;h1&gt;Available CRA ... All the names of the packages appear as links: # Extract hyperlinks with &quot;a&quot; read_html(&quot;https://cran.r-project.org/web/packages/available_packages_by_name.html&quot;) |&gt; html_elements(&quot;a&quot;) ## {xml_nodeset (22364)} ## [1] &lt;a href=&quot;#available-packages-A&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;A&lt;/span&gt; ... ## [2] &lt;a href=&quot;#available-packages-B&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;B&lt;/span&gt; ... ## [3] &lt;a href=&quot;#available-packages-C&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;C&lt;/span&gt; ... ## [4] &lt;a href=&quot;#available-packages-D&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;D&lt;/span&gt; ... ## [5] &lt;a href=&quot;#available-packages-E&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;E&lt;/span&gt; ... ## [6] &lt;a href=&quot;#available-packages-F&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;F&lt;/span&gt; ... ## [7] &lt;a href=&quot;#available-packages-G&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;G&lt;/span&gt; ... ## [8] &lt;a href=&quot;#available-packages-H&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;H&lt;/span&gt; ... ## [9] &lt;a href=&quot;#available-packages-I&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;I&lt;/span&gt; ... ## [10] &lt;a href=&quot;#available-packages-J&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;J&lt;/span&gt; ... ## [11] &lt;a href=&quot;#available-packages-K&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;K&lt;/span&gt; ... ## [12] &lt;a href=&quot;#available-packages-L&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;L&lt;/span&gt; ... ## [13] &lt;a href=&quot;#available-packages-M&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;M&lt;/span&gt; ... ## [14] &lt;a href=&quot;#available-packages-N&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;N&lt;/span&gt; ... ## [15] &lt;a href=&quot;#available-packages-O&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;O&lt;/span&gt; ... ## [16] &lt;a href=&quot;#available-packages-P&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;P&lt;/span&gt; ... ## [17] &lt;a href=&quot;#available-packages-Q&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;Q&lt;/span&gt; ... ## [18] &lt;a href=&quot;#available-packages-R&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;R&lt;/span&gt; ... ## [19] &lt;a href=&quot;#available-packages-S&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;S&lt;/span&gt; ... ## [20] &lt;a href=&quot;#available-packages-T&quot;&gt;&lt;span class=&quot;CRAN&quot;&gt;T&lt;/span&gt; ... ## ... These are all hyperlinks! We can grab the hyperlinked text with html_text(): # Save the hyperlinked text read_html(&quot;https://cran.r-project.org/web/packages/available_packages_by_name.html&quot;) |&gt; html_elements(&quot;a&quot;) |&gt; html_text(trim = TRUE) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ## [4] &quot;D&quot; &quot;E&quot; &quot;F&quot; ## [7] &quot;G&quot; &quot;H&quot; &quot;I&quot; ## [10] &quot;J&quot; &quot;K&quot; &quot;L&quot; ## [13] &quot;M&quot; &quot;N&quot; &quot;O&quot; ## [16] &quot;P&quot; &quot;Q&quot; &quot;R&quot; ## [19] &quot;S&quot; &quot;T&quot; &quot;U&quot; ## [22] &quot;V&quot; &quot;W&quot; &quot;X&quot; ## [25] &quot;Y&quot; &quot;Z&quot; &quot;A3&quot; ## [28] &quot;AalenJohansen&quot; &quot;AATtools&quot; &quot;ABACUS&quot; ## [31] &quot;abasequence&quot; &quot;abbreviate&quot; &quot;abc&quot; ## [34] &quot;abc.data&quot; &quot;ABC.RAP&quot; &quot;ABCanalysis&quot; ## [37] &quot;abclass&quot; &quot;ABCoptim&quot; &quot;abcrf&quot; ## [40] &quot;abcrlda&quot; &quot;abctools&quot; &quot;abd&quot; ## [43] &quot;abdiv&quot; &quot;abe&quot; &quot;aberrance&quot; ## [46] &quot;abess&quot; &quot;abglasso&quot; &quot;ABHgenotypeR&quot; ## [49] &quot;abima&quot; &quot;abind&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 22314 entries ] Try it! Save the names of the packages in a dataframe and get rid of the first 26 observations that only refer to the 26 letters of the alphabet to search package names. Find how many packages start with the letters gg. Hint: recall str_detect() and Regex anchors. Then create a visualization to check what is the most common first letter for a package. # Create data frame r_packages &lt;- data.frame( &quot;names&quot; = read_html(&quot;https://cran.r-project.org/web/packages/available_packages_by_name.html&quot;) |&gt; html_elements(&quot;a&quot;) |&gt; html_text(trim = TRUE)) |&gt; # Get rid of the first 26 elements filter(str_length(names) &gt; 1) # Find the number of packages that start with gg r_packages |&gt; filter(str_detect(str_to_lower(names), &quot;^gg&quot;)) |&gt; nrow() ## [1] 233 # Most common first letter for a package r_packages |&gt; mutate(first_letter = str_extract(str_to_lower(names), &quot;^.&quot;)) |&gt; # Create a bar graph for each letter ggplot(aes(x = first_letter)) + geom_bar() + labs(x = &quot;first letter of package names&quot;, y = &quot;number of packages&quot;) Most R packages start with the letter S! Note: There are currently 1 packages. This number changes every semesters (and almost everyday)! Let’s look at the first letter of each package and look at the distribution of each letter with a bar graph: 2. Web scraping and ethics Web scraping can be a powerful tool for collecting data from the internet, but it also raises important ethical considerations. While publicly available data may seem free to use, scraping websites without permission can violate terms of service or privacy rights. Ethical web scraping involves respecting a website’s guidelines for automated access and ensuring that the collected data is used responsibly. When in doubt, seeking permission or using official APIs is a more ethical and sustainable approach to obtaining data. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Web scraping Selector Gadget extension "],["WS16_TextMining_Key.html", "Text Mining Learning objectives 1. Text data 2. Stop words 3. Word Clouds 4. Sentiment analysis Recommended Resources", " Text Mining Learning objectives Tokenize text using tidytext and compute word frequencies. Remove stop words and other irrelevant terms to refine text data. Visualize word frequencies with word clouds. Associate words with sentiments. We will use the tidytext and textdata packages as well as an extension of ggplot with ggwordcloud: # Install packages (only needed once!) install.packages(&quot;tidytext&quot;) install.packages(&quot;textdata&quot;) install.packages(&quot;ggwordcloud&quot;) Then load all packages: # Load packages library(tidyverse) library(tidytext) library(textdata) library(ggwordcloud) We will look at the lyrics for Taylor Swift’s songs. Someone already scrape the lyrics from online sources for us! # Upload data from GitHub taylor_songs &lt;- read_csv(&quot;https://raw.githubusercontent.com/shaynak/taylor-swift-lyrics/main/songs.csv&quot;) # Take a look head(taylor_songs) Try it! How many songs are in this dataset? Over how many albums? # Structure nrow(taylor_songs) ## [1] 257 n_distinct(taylor_songs$Album) ## [1] 24 A total of 257 songs and 24 albums. How could we analyze data about the lyrics? Split sentences into words: most common words? 1. Text data We need to do some clean up to investigate text data. For example: Get rid of the punctuation Put text in lowercase Split sentences into separate words To do all that, we will use a function from tidytext that put text data automatically into words: unnest_tokens(). Note: This function has a lot of cool options. lyrics &lt;- taylor_songs |&gt; # Split each comment into words unnest_tokens(input = Lyrics, output = word) We created a new object, lyrics, that contains all words separately. Try it! Using dplyr functions, find the frequency of each word. Take a look at the 10 most common words. What do you think about these top 10 words? Anything interesting? Not interesting? # Data wrangling! lyrics |&gt; group_by(word) |&gt; summarize(freq = n()) |&gt; slice_max(n = 10, freq) Most of these words are not very informative. 2. Stop words To clean up text data, we might want to omit some words that are not so relevant. Luckily, we can access a list of stop_words from the tidytext package. There are three lexicons available: onix, SMART or snowball: # Lexicons available with number of words contained in them table(stop_words$lexicon) ## ## onix SMART snowball ## 404 571 174 Let’s first consider the snowball lexicon. # Create the list of stop words for the snowball lexicon snowball_stops &lt;- stop_words |&gt; filter(lexicon == &quot;snowball&quot;) # Take a quick look head(snowball_stops) Let’s get rid of these snowball_stops with anti_join() (word in lyrics that are not in snowball_stops): lyrics_clean &lt;- lyrics |&gt; # For each word... group_by(word) |&gt; # Find how many times that word was repeated summarize(freq = n()) |&gt; # Only keep the words that DO NOT appear in snowball_stops anti_join(snowball_stops, by = &quot;word&quot;) # Take a quick look at most common words lyrics_clean |&gt; slice_max(freq, n = 10) Anything you notice in these most common words? The word “chorus” is probably just indicated when the chorus is in the lyrics so we could also get rid of it. Try it! Get rid of more stop_words by using the SMART lexicon instead of the snowball lexicon. Update the list of the 10 most common words. What do you think about these top 10 words? Anything word we should get rid of? lyrics |&gt; # For each word... group_by(word) |&gt; # Find how many times that word was repeated summarize(freq = n()) |&gt; # Only keep the words that DO NOT appear in snowball_stops anti_join(stop_words |&gt; filter(lexicon == &quot;SMART&quot;), by = &quot;word&quot;) |&gt; slice_max(freq, n = 10) We could get rid of “chorus”, “verse”, “taylor”, “swift”, “pre” as these words refer to the structure of the songs. 3. Word Clouds A word cloud represents how frequent some words are. We should summarize how frequent are each word is before putting the information into our word cloud with a new geom_ function from the ggwordcloud package: # Using a ggplot ggplot(lyrics_clean, aes(label = word)) + geom_text_wordcloud() + # a new geom! # Control the size of the words scale_size_area(max_size = 24) + theme_minimal() ## Warning in wordcloud_boxes(data_points = points_valid_first, boxes ## = boxes, : Some words could not fit on page. They have been placed ## at their original positions. # it might take a second Try it! Let’s make that word cloud a little prettier… Use dplyr and ggplot functions to 1) Only keep the 20 most common words, 2) Make the most common words look bigger, 3) Use different colors if the words are more or less common. # Only focus on the 20 most common words lyrics_clean |&gt; slice_max(n = 20, freq) |&gt; ggplot(aes(label = word, size = freq, color = freq)) + geom_text_wordcloud() + scale_size_area(max_size = 24) + theme_minimal() There are still a bunch of words we don’t really care about… Try it! Get rid of words that are not important when interested about the lyrics content. You can also try different lexicons! lyrics_clean |&gt; # Get rid of words about the structure of the dataset filter(!word %in% c(&quot;chorus&quot;, &quot;verse&quot;, &quot;taylor&quot;, &quot;swift&quot;, &quot;pre&quot;)) |&gt; # Only keep the 20 most common words slice_max(n = 20, freq) |&gt; # Represent the words with a word cloud ggplot(aes(label = word, size = freq, color = freq)) + geom_text_wordcloud() + # a new geom! scale_size_area(max_size = 24) + theme_minimal() There are still words we should ignore maybe? like “oh” and “ooh”? 4. Sentiment analysis Sentiment analysis uses a scored lexicon of words, with emotion scores or labels (negative vs. positive) indicating each word’s emotional content. Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, overall, it can provide some insights. We can use the tidytext function get_sentiments() to load a lexicon for sentiments of a large number of words. Here are a few examples: # Get sentiments get_sentiments(&quot;bing&quot;) get_sentiments(&quot;afinn&quot;) get_sentiments(&quot;nrc&quot;) a. Positive/Negative sentiments In the bing lexicon, each word was attributed a positive or negative sentiment. Try it! Match the words from the lyrics and their corresponding sentiments (if available). Are Taylor Swift’s lyrics mostly positive or negative? lyrics |&gt; # Only keep the words with a corresponding sentiment with inner_join() inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) |&gt; # Compare each sentiment group_by(sentiment) |&gt; summarize(freq = n()) |&gt; # Calculate proportion mutate(prop = freq / sum(freq)) ## Warning in inner_join(lyrics, get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 74415 of `x` matches multiple rows in `y`. ## ℹ Row 3805 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. # or use a bar graph lyrics |&gt; # Only keep the words with a corresponding sentiment with inner_join() inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) |&gt; # Make a plot ggplot() + geom_bar(aes(x = sentiment)) ## Warning in inner_join(lyrics, get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 74415 of `x` matches multiple rows in `y`. ## ℹ Row 3805 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. Slightly more positive words! Bonus question: Which five words contributed the most to each sentiment? # Words for each sentiment lyrics |&gt; # Only keep the words with a corresponding sentiment with inner_join() inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) |&gt; # For each word, in each sentiment group_by(sentiment, word) |&gt; summarize(freq = n()) |&gt; # Keep the top 5 slice_max(freq, n = 5) |&gt; # Make a plot (same type as shown in the slides) ggplot() + geom_bar(aes(y = fct_reorder(word, freq), x = freq, fill = sentiment), stat = &quot;identity&quot;) + facet_wrap(~sentiment, scales = &quot;free&quot;) + labs(y = &quot;Most common words&quot;) ## Warning in inner_join(lyrics, get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 74415 of `x` matches multiple rows in `y`. ## ℹ Row 3805 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. Note that the word “swift” probably refers to her last name instead of the adjective. b. Scoring sentiments In the afinn lexicon, each word was attributed a score: negative scores represent negative sentiments whereas positive scores represent positive sentiments with different intensities. Try it! Match the words from the lyrics and their corresponding sentiment value (if available). Find the average sentiment (with the mean) for each song. How does the mean sentiment vary from song to song? Which song has the highest score? lyrics |&gt; # Assign a scored sentiment to a word if available inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) |&gt; # Summarize average sentiment for each song group_by(Title) |&gt; summarize(avg_sentiment = mean(value)) |&gt; # Make a plot ggplot() + geom_histogram(aes(x = avg_sentiment), binwidth = 1, center = 0.5, color = &quot;black&quot;) # Check the highly positive songs lyrics |&gt; # Assign a scored sentiment to a word if available inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) |&gt; # Summarize average sentiment for each song group_by(Title) |&gt; summarize(avg_sentiment = mean(value)) |&gt; slice_max(n = 1, avg_sentiment) There are more songs with a positive sentiment between 0 and 1. The most positive song is “A Perfectly Good Heart”. c. Using a wide range of sentiments In the nrc lexicon, each word was associated with a sentiment and a single word can carry multiple sentiments. See for example the word celebrity: # Get sentiments get_sentiments(&quot;nrc&quot;) |&gt; filter(word == &quot;celebrity&quot;) Try it! Match the words from the lyrics and their corresponding sentiment value (if available). Find the dominant sentiment for each album (beyond just positive/negative). Which dominant sentiment is the most common across all the albums? lyrics |&gt; # Assign a scored sentiment to a word if available inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) |&gt; # Summarize average sentiment for each song group_by(Album, sentiment) |&gt; summarize(freq = n()) |&gt; # Remove positive or negative sentiments filter(!(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;))) |&gt; # Top sentiment per album slice_max(n = 1, freq) |&gt; # Make a plot ggplot() + geom_bar(aes(y = fct_reorder(sentiment, freq), x = freq), stat = &quot;identity&quot;) + labs(y = &quot;Dominant sentiment of the album&quot;) ## Warning in inner_join(lyrics, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 14 of `x` matches multiple rows in `y`. ## ℹ Row 2028 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. More albums have “joy” as the dominant sentiment! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Text mining More options for word clouds: ggwordcloud: a word cloud geom for ggplot2 More options on Sentiment analysis with tidy data "],["WS17_SentimentAnalysis_Key.html", "Sentiment Analysis Learning objectives 1. Positive/Negative sentiments 2. Scoring sentiments 3. Using a wide range of sentiments Recommended Resources", " Sentiment Analysis Learning objectives Apply different sentiment lexicons to analyze text data. Visualize sentiment distributions and dominant emotions in text data. We will use the textdata package for lexicons of sentiments: # Install package (only needed once!) install.packages(&quot;textdata&quot;) Then load all packages needed for this worksheet: # Load packages library(tidyverse) library(tidytext) library(ggwordcloud) library(textdata) We will continue exploring lyrics of Taylor Swift’s songs: # Upload data from GitHub taylor_songs &lt;- read_csv(&quot;https://raw.githubusercontent.com/shaynak/taylor-swift-lyrics/main/songs.csv&quot;) # Take a look head(taylor_songs) Try it! Split the lyrics of each song into words with unnest_tokens. words &lt;- taylor_songs |&gt; # Split each lyric into words unnest_tokens(input = Lyrics, output = word) Sentiment analysis uses a scored lexicon of words, with emotion scores or labels (negative vs. positive) indicating each word’s emotional content. Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, overall, it can provide some insights. We can use the tidytext function get_sentiments() to load a lexicon for sentiments of a large number of words. Here are a few examples: # Get sentiments get_sentiments(&quot;bing&quot;) |&gt; head() get_sentiments(&quot;afinn&quot;) |&gt; head() get_sentiments(&quot;nrc&quot;) |&gt; head() 1. Positive/Negative sentiments In the bing lexicon, each word was attributed a positive or negative sentiment. Try it! Match the words from the lyrics and their corresponding sentiments (if available). Are Taylor Swift’s lyrics mostly positive or negative? words |&gt; # Only keep the words with a corresponding sentiment with inner_join() inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) |&gt; # Compare each sentiment group_by(sentiment) |&gt; summarize(freq = n()) |&gt; # Calculate proportion mutate(prop = freq / sum(freq)) ## Warning in inner_join(words, get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 74415 of `x` matches multiple rows in `y`. ## ℹ Row 3805 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. # or use a bar graph words |&gt; # Only keep the words with a corresponding sentiment with inner_join() inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) |&gt; # Make a plot ggplot() + geom_bar(aes(x = sentiment)) ## Warning in inner_join(words, get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 74415 of `x` matches multiple rows in `y`. ## ℹ Row 3805 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. Slightly more positive words! Bonus question: Which five words contributed the most to each sentiment? # Words for each sentiment words |&gt; # Only keep the words with a corresponding sentiment with inner_join() inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) |&gt; # For each word, in each sentiment group_by(sentiment, word) |&gt; summarize(freq = n()) |&gt; # Keep the top 5 slice_max(freq, n = 5) |&gt; # Make a plot (same type as shown in the slides) ggplot() + geom_bar(aes(y = fct_reorder(word, freq), x = freq, fill = sentiment), stat = &quot;identity&quot;) + facet_wrap(~sentiment, scales = &quot;free&quot;) + labs(y = &quot;Most common words&quot;) ## Warning in inner_join(words, get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 74415 of `x` matches multiple rows in `y`. ## ℹ Row 3805 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. Note that the word “swift” probably refers to her last name instead of the adjective. 2. Scoring sentiments In the afinn lexicon, each word was attributed a score: negative scores represent negative sentiments whereas positive scores represent positive sentiments with different intensities. Try it! Find the average sentiment (with the mean) for each song. How does the mean sentiment vary from song to song? Which songs have the highest score? words |&gt; # Assign a scored sentiment to a word if available inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) |&gt; # Summarize average sentiment for each song group_by(Title) |&gt; summarize(avg_sentiment = mean(value)) |&gt; # Make a plot ggplot() + geom_histogram(aes(x = avg_sentiment), binwidth = 1, center = 0.5, color = &quot;black&quot;) # Check the highly positive songs words |&gt; # Assign a scored sentiment to a word if available inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) |&gt; # Summarize average sentiment for each song group_by(Title) |&gt; summarize(avg_sentiment = mean(value)) |&gt; slice_max(n = 5, avg_sentiment) There are more songs with a positive sentiment between 0 and 1. Some songs are more positive with an average score above 2! 3. Using a wide range of sentiments In the nrc lexicon, each word was associated with a sentiment and a single word can carry multiple sentiments. See for example the word celebrity: # Get sentiments get_sentiments(&quot;nrc&quot;) |&gt; filter(word == &quot;celebrity&quot;) Try it! Find the dominant sentiment for each album (beyond positive/negative). Which dominant sentiment is the most common over all the albums? words |&gt; # Assign a scored sentiment to a word if available inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) |&gt; # Summarize average sentiment for each song group_by(Album, sentiment) |&gt; summarize(freq = n()) |&gt; # Remove positive or negative sentiments filter(!(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;))) |&gt; # Top sentiment per album slice_max(n = 1, freq) |&gt; # Make a plot ggplot() + geom_bar(aes(y = fct_reorder(sentiment, freq), x = freq), stat = &quot;identity&quot;) + labs(y = &quot;Dominant sentiment of the album&quot;) ## Warning in inner_join(words, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and ## `y`. ## ℹ Row 14 of `x` matches multiple rows in `y`. ## ℹ Row 2028 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. More albums have “joy” as the dominant sentiment! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Text mining More options on Sentiment analysis with tidy data "],["WS18_SupervisedLearning_Key.html", "Supervised Learning Learning objectives 1. Defining a model 2. Comparing models Recommended Resources", " Supervised Learning Learning objectives Understand the concept of supervised learning and its application in predicting outcomes. Analyze the trade-offs between accuracy and stability in model selection. Start your workflow by uploading the tidyverse package as always: # Load packages library(tidyverse) It’s been a while since we introduced this dataset, but remember mtcars? # Take a quick look head(mtcars) We will discuss how we could predict the value of fuel efficiency (mpg) of the Alfa Romeo Alfetta GTV 2.0 based on other features of cars. Note: this particular car is not in the mtcars dataset but it was introduced in 1974 like many of the cars in this dataset. 1. Defining a model If the goal is to predict the fuel efficiency of the car, the mpg variable is the outcome. But what could our predictor(s) be? We could consider any variable in the mtcars dataset as any of these characteristics could potentially impact mpg. a. A model with no predictor How could we predict a value of mpg if we have no information about the Alfa Romeo Alfetta GTV 2.0? Try it! Represent the distribution of mpg for all cars in mtcars. Since we have no other information about the Alfa Romeo Alfetta GTV 2.0, what would be a wild guess for its value of mpg? # Take a look at the values of mpg mtcars |&gt; ggplot(aes(x = mpg)) + geom_histogram(binwidth = 2.5, center = 1.25, color = &quot;black&quot;, fill = &quot;blue&quot;) + labs(x = &quot;Miles per gallon&quot;, y = &quot;Number of cars&quot;) # Try the mean value of mpg mean(mtcars$mpg) ## [1] 20.09062 We could estimate the mpg of the Alfa Romeo Alfetta GTV 2.0 to be average, about 20 mpg. b. Some models with one predictor Let’s find the correlation coefficient between mpg and each predictor in the dataset: # Find the correlation matrix cor(mtcars)[,1] # only show the first column ## mpg cyl disp hp drat wt ## 1.0000000 -0.8521620 -0.8475514 -0.7761684 0.6811719 -0.8676594 ## qsec vs am gear carb ## 0.4186840 0.6640389 0.5998324 0.4802848 -0.5509251 Some features of the cars seem to be highly correlated with mpg. Let’s focus on the weight of a car: consider the wt variable as the predictor. We can represent the relationship between the two variables with a scatterplot (recall: by convention, the outcome variable is represented on the y-axis). We can estimate the predicted mpg to just be the mean, regardless of the weight (a very simple model): # Represent mpg vs. weight mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Using the mean: Prediction is constant&quot;) + # Show the overall mean of mpg as a horizontal line geom_hline(aes(yintercept = mean(mpg)), size = 3, color = &quot;steelblue&quot;) With this model, regardless of the weight, we predict the fuel efficiency to be about 20 mpg. We know that the weight of an Alfa Romeo Alfetta GTV 2.0 was about 2500 lbs (see Wikipedia). Consider the following model that goes through each data point: mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + # Consider a very refined model that goes through each data point geom_line(color = &quot;steelblue&quot;, size = 2) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Using all data points: Prediction is car specific&quot;) + scale_y_continuous(breaks = seq(10, 35, 2)) + # Show the weight as a vertical line geom_vline(xintercept = 2.5, color = &quot;red&quot;) Has the prediction for the mpg of the Alfa Romeo Alfetta GTV 2.0 changed based on the value of its weight with this model? This new model seems to predict a fuel efficiency of about 21.5 mpg, a little higher than the overall mean of about 20 mpg. Now, let’s consider another model with an overall linear trend (we will talk about linear regression in the next worksheet): mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + # Consider a linear regression model geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;steelblue&quot;, size = 2) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Using linear regression: Prediction is based on a trend&quot;) + scale_y_continuous(breaks = seq(10, 35, 2)) + # Show the weight as a vertical line geom_vline(xintercept = 2.5, color = &quot;red&quot;) Has the prediction for the mpg of the Alfa Romeo Alfetta GTV 2.0 changed based on this new model? This new model seems to predict a fuel efficiency of about 24 mpg. Which model is more useful to make a prediction? It’s hard to say. But overall, we want: Predictions that are accurate (close to the truth) Predictions that are stable (should not change much if we add new data) It is difficult to be both accurate and stable: there is a trade-off between these two conditions. 2. Comparing models Let’s add a new data point, about the Volvo 200 Series in 1974 that also weighted 2,500 lbs but for which we know the mpg: new_mtcars &lt;- mtcars |&gt; # Only keep variables of interest select(mpg, wt) |&gt; # Add a new data point (add a new row) rbind(data.frame(mpg = 29, wt = 2.5)) How does this new data point affect, or not, our models? Try it! Represent the 3 models (constant, all data points, linear regression) with the new data point. Which model resulted in the biggest change in the prediction for the Alfa Romeo Alfetta GTV 2.0? the smallest change? Note: only guess-estimate the prediction based on the visualizations. # Model 1: Mean new_mtcars |&gt; select(mpg, wt) |&gt; rbind(data.frame(mpg = 29, wt = 2.5)) |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Using the mean&quot;) + geom_hline(aes(yintercept = mean(mpg)), size = 3, color = &quot;steelblue&quot;) + # Show prediction point geom_point(x = 2.5, aes(y = mean(mpg)), size = 4, color = &quot;red&quot;) ## Warning in geom_point(x = 2.5, aes(y = mean(mpg)), size = 4, color = &quot;red&quot;): All aesthetics have length 1, but the data has 34 rows. ## ℹ Please consider using `annotate()` or provide this layer with ## data containing a single row. # Model 2: All data points new_mtcars |&gt; select(mpg, wt) |&gt; rbind(data.frame(mpg = 29, wt = 2.5)) |&gt; ggplot(aes(x = wt, y = mpg)) + geom_line(color = &quot;steelblue&quot;, size = 2) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Using all data points&quot;) + geom_vline(xintercept = 2.5, color = &quot;red&quot;) + # Show prediction point geom_point(x = 2.5, y = 29, size = 4, color = &quot;red&quot;) # Model 3: Linear Regression new_mtcars |&gt; select(mpg, wt) |&gt; rbind(data.frame(mpg = 29, wt = 2.5)) |&gt; ggplot(aes(x = wt, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;steelblue&quot;, size = 2) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Using linear regression: Prediction is based on a trend&quot;) + geom_vline(xintercept = 2.5, color = &quot;red&quot;) + # Show prediction point geom_point(x = 2.5, y = 24.1, size = 4, color = &quot;red&quot;) The predictions based on the constant model and linear regression haven’t changed much but the prediction based on all data points has changed a lot. Which of the three models above would you pick to predict mpg of the Alfa Romeo Alfetta GTV 2.0? The 3rd model: linear regression. Because the prediction did not change much by adding new data and it takes into account some specific characteristic of the car (its weight) compared to the constant model. Notes on the bias-variance trade-off: A model that is stable and not sensitive to changes in the data is said to have low variance. A model that is highly sensitive to changes in the data is said to have high variance (and is not very useful to make predictions for new data). A model that is flexible and captures complex patterns in the data is said to have low bias (however, there is a risk of overfitting the data). A model that is simplistic and does not take into account the complexities in the data is said to have high bias (there is a risk of underfitting the data). Ideally, a model should have low variance and low bias but can you see that there is a trade-off between the two conditions? For each model we introduce this semester, we will introduce a measure to check potential bias by comparing our predictions to the real data (looking at what we call “residuals”). Then we will also check potential variance by comparing the model based on new data (this is called cross-validation). Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Model basics "],["WS19_LinearRegression_Key.html", "Linear Regression Learning objectives 1. Predicting a numeric response with a numeric predictor 2. Using a categorical predictor 3. Using multiple predictors Recommended Resources", " Linear Regression Learning objectives Learn how to fit a linear regression model. Visualize regression models. Interpret the output of a linear regression model, including coefficients, residuals, and performance. Start your workflow by uploading the tidyverse package as always: # Load packages library(tidyverse) We will continue exploring the mtcars dataset: # Looks familiar? head(mtcars) This dataset contains information about different features of some cars that we will use to predict fuel efficiency (the mpg variable). 1. Predicting a numeric response with a numeric predictor First, let’s try to predict the mpg based on the weight of a car wt. a. Visualizing the model Using geom_smooth() we can visualize the linear regression model with method = lm: # Represent the relationship with a model mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + # Consider a linear regression model geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 2) + geom_point(size = 4) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Linear regression model to predict Miles per gallon based on Weight&quot;) How did R choose what line to fit this data? b. Fitting a model If we suspect there is a linear relationship between two variables, we can consider a linear regression model. To find the expression of the linear model represented above, we use the lm(response ~ predictor, data = ...) function: # Fit the model fit_lin &lt;- lm(mpg ~ wt, data = mtcars) # Take a look at the model summary summary(fit_lin) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 This output tells us a lot of things. In particular, it gives us the estimates of the model. We predict the value of mpg if we know the weight wt of a car as follows: \\(\\widehat{mpg} = 37.2851 - 5.3445 * wt\\) Note: We use the hat to specify that we get predicted values of mpg (as opposed to mpg, the observed values in the dataset). c. Predicting values Let’s use the expression of the model to calculate predicted values. Try it! Use the expression of the model above to create a new variable in mtcars called predicted that predicts values of mpg based on values of wt. Then calculate the mean of the predicted values. How does the mean of predicted values compare to the mean of the observed mpg? mtcars |&gt; mutate(predicted = 37.2851 - 5.3445*wt) |&gt; summarize(mean(predicted)) mean(mtcars$mpg) ## [1] 20.09062 The two means are very similar! That actually makes sense because of how the linear regression model is defined (but it takes some math to prove that). Much more convenient to calculate predicted values (especially when we will have more predictors with a longer expression for the model), let’s use the predict(model_name) function: mtcars |&gt; # Calculate predicted values mutate(predicted = predict(fit_lin)) |&gt; # Just show the variables of interest select(wt, mpg, predicted) We can also find predicted values for new data with the option newdata. For example, I have a Toyota RAV4 that weighs about 3,500 lbs: # Define new data rav4 &lt;- data.frame(wt = 3.5) # Find predicted values for new data predict(fit_lin, newdata = rav4) ## 1 ## 18.57948 The predicted fuel consumption is about 18.6 mpg… Well… my car is supposed to get a much better fuel consumption. This is an example of what we call extrapolation: we use a model that is not applicable to our new data. The cars contained in mtcars were listed in the 1974 Motor Trend US magazine and my car is from 2017 so fuel efficiency has changed a lot in between! Try it! Predict the value of mpg for a car that weighs 3,440 lbs. Are there any cars in mtcars that had such a weight? Does their observed value of mpg match the predicted value? Why or why not? # Find predicted values for new data predict(fit_lin, newdata = data.frame(wt = 3.44)) ## 1 ## 18.90014 # There are two cars with the same weight, different mpg mtcars |&gt; filter(wt == 3.44) |&gt; select(wt, mpg) The predicted value of 18.9 mpg was either above or below the values of mpg for the three cars in the mtcars dataset. Our predicted values don’t usually match the observed values because there is some variation in our data. d. Residuals Our predicted values don’t usually match exactly our observed values. The residuals represent the difference between observed values and predicted values: mtcars |&gt; # First add predicted values based on model mutate(predicted = predict(fit_lin)) |&gt; # Calculate residuals = observed - predicted mutate(residuals = mpg - predicted) |&gt; # Only display variables of interest select(wt, mpg, predicted, residuals) Or more conveniently using the resid(model_name) function: mtcars |&gt; # Calculate residuals mutate(residuals = resid(fit_lin)) |&gt; select(wt, mpg, residuals) Let’s visualize the residuals: mtcars |&gt; # Calculate predicted values mutate(predicted = predict(fit_lin)) |&gt; # Use a ggplot to represent the relationship ggplot(aes(x = wt, y = mpg)) + # Add the linear model geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 2) + # Add residuals = vertical segments from observations to predicted geom_segment(aes(xend = wt, yend = predicted), alpha = .5, color = &quot;red&quot;) + # Display the observed data geom_point(size = 4) + # Display the predicted (on top of the line) geom_point(aes(y = predicted), size = 4, color = &quot;orange&quot;) + labs(x = &quot;Weight (thousands of lbs)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Linear regression model with residuals&quot;) A linear regression model is actually built by minimizing the sum of squared residuals. Try it! Find the mean of the residuals. Why does it make sense to get this value? mtcars |&gt; # Calculate residuals mutate(residuals = resid(fit_lin)) |&gt; # Find the mean summarize(mean(residuals)) We should find 0 (not exactly 0 because of rounding). It makes sense because all of the data points balance each other. e. Performance To quantify performance for linear regression models, we can consider the average distance between the predicted values from the model and the observed values in the dataset. This is called the root mean square error (RMSE) of the model. # Calculate RMSE of regression model: square root of mean residuals squared sqrt(mean(resid(fit_lin)^2)) ## [1] 2.949163 The lower the RMSE, the better a model fits a dataset and the more reliable our predicted values can be. Note that the RMSE is reported in the same unit as the outcome variable. Here it means that the predicted values typically differ from the actual values of mpg by 2.95 mpg. We can also consider the adjusted coefficient of determination \\(R^2\\), which reports the percentage of variation in the response variable that can be explained by the predictor variables. # Report adjusted R-squared of regression model summary(fit_lin)$adj.r.squared ## [1] 0.7445939 The higher the \\(R^2\\), the better a model fits a dataset. Note that \\(R^2\\) represents a proportion between 0 and 1. Here it means that about 74.5% of the variation in mgp can be explained by the weight of a car. Try it! Predict mpg based on another numeric feature of the car (for example, disp, hp, …). Is the model with this new predictor performing better or worse than the model based on mpg? # Fit the model with disp fit_lin &lt;- lm(mpg ~ disp, data = mtcars) # Performance based on RMSE sqrt(mean(resid(fit_lin)^2)) ## [1] 3.148207 # Performance based on adjusted R-squared summary(fit_lin)$adj.r.squared ## [1] 0.7089548 The model with disp is not performing better (higher RMSE and lower adjusted R-squared). 2. Using a categorical predictor What if we chose to predict the fuel consumption based on the transmission of a car (the am variable, 0 = automatic vs 1 = manual)? Let’s take a look at the relationship with this new predictor: # Represent the relationship mtcars |&gt; ggplot(aes(x = am, y = mpg)) + # Consider a linear regression model geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 2) + geom_point(size = 4) + labs(x = &quot;Transmission (0 = automatic vs 1 = manual)&quot;, y = &quot;Miles per gallon&quot;, title = &quot;Linear regression model to predict Miles per gallon based on Transmission&quot;) It doesn’t really look like a linear relationship but we can still fit a linear regression model with the am predictor: # Fit the model fit_lin &lt;- lm(mpg ~ am, data = mtcars) # Take a look at the model summary summary(fit_lin) ## ## Call: ## lm(formula = mpg ~ am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3923 -3.0923 -0.2974 3.2439 9.5077 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.147 1.125 15.247 1.13e-15 *** ## am 7.245 1.764 4.106 0.000285 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.902 on 30 degrees of freedom ## Multiple R-squared: 0.3598, Adjusted R-squared: 0.3385 ## F-statistic: 16.86 on 1 and 30 DF, p-value: 0.000285 Try it! Write the expression of the new model. Predict values of mpg based on am using predict(). Why does it make sense to get what we get? And what are we getting exactly? # Look at the predicted values mtcars |&gt; mutate(predicted = predict(fit_lin)) |&gt; select(am, mpg, predicted) # Mean mpg per type of transmission mtcars |&gt; group_by(am) |&gt; summarize(mean_mpg = mean(mpg)) The expression of the model is \\(\\widehat{mpg}=17.147+7.245∗am\\) We get only two possible predicted values which correspond to the mean value of mpg for each type of transmission. 3. Using multiple predictors We can add many predictors to our linear regression model! What if we combine wt and am? # Fit the model fit_lin &lt;- lm(mpg ~ wt + am, data = mtcars) # Take a look at the model summary summary(fit_lin) ## ## Call: ## lm(formula = mpg ~ wt + am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5295 -2.3619 -0.1317 1.4025 6.8782 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.32155 3.05464 12.218 5.84e-13 *** ## wt -5.35281 0.78824 -6.791 1.87e-07 *** ## am -0.02362 1.54565 -0.015 0.988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.098 on 29 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7358 ## F-statistic: 44.17 on 2 and 29 DF, p-value: 1.579e-09 Try it! Predict the value of mpg for an automatic Alfa Romeo Alfetta GTV 2.0 which weighs about 2500 lbs. # Find predicted values for new data alfa &lt;- data.frame(wt = 2.5, am = 0) predict(fit_lin, newdata = alfa) ## 1 ## 23.93952 The predicted value of mpg is about 24 mpg for this car. And what if we add all possible predictors? # Fit the model using all predictors: refer to all variable with `.` (always double check it makes sense to add all predictors) fit_lin &lt;- lm(mpg ~ ., data = mtcars) # Take a look at the model summary summary(fit_lin) ## ## Call: ## lm(formula = mpg ~ ., data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4506 -1.6044 -0.1196 1.2193 4.6271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.30337 18.71788 0.657 0.5181 ## cyl -0.11144 1.04502 -0.107 0.9161 ## disp 0.01334 0.01786 0.747 0.4635 ## hp -0.02148 0.02177 -0.987 0.3350 ## drat 0.78711 1.63537 0.481 0.6353 ## wt -3.71530 1.89441 -1.961 0.0633 . ## qsec 0.82104 0.73084 1.123 0.2739 ## vs 0.31776 2.10451 0.151 0.8814 ## am 2.52023 2.05665 1.225 0.2340 ## gear 0.65541 1.49326 0.439 0.6652 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.65 on 21 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 ## F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 Now the expression of the model becomes very complex but check out the performance based on adjusted R-squared! Notes: Adding too many variables can create issues such as overfitting: the model is too specific to the cars in the dataset on which we “train” the model and it will be very difficult to generalize to other cars. We can quickly check which features might be more useful for making predicted by looking at the last column in the model output. Any . or * shows which features are “significant” while taking into account all other variables. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Linear regression More details on lm() "],["WS20_ModelSelection_Key.html", "Model Selection Learning objectives 1. Choosing predictors 2. Comparing models Recommended Resources", " Model Selection Learning objectives Understand model selection and recognize the impact of different predictors on model performance. Evaluate model fit using key metrics such as RMSE and adjusted \\(R^{2}\\). Detect overfitting by comparing model performance on training and test data. Start your workflow by uploading the tidyverse package as always: # Load packages library(tidyverse) We will work with the data from the following article: Hickey, W. (2007). The Ultimate Halloween Candy Power Ranking. FiveThirtyEight. https://fivethirtyeight.com/videos/the-ultimate-halloween-candy-power-ranking/ # Upload data from github candy &lt;- read_csv(&quot;https://raw.githubusercontent.com/laylaguyot/datasets/main//Halloween-candy.csv&quot;) # Take a quick look head(candy) This dataset is the result of an experiment: “Pit dozens of fun-sized candy varietals against one another, and let the wisdom of the crowd decide which one was best. While we don’t know who exactly voted, we do know this: 8,371 different IP addresses voted on about 269,000 randomly generated matchups.” Here are the top 19 winners: We are interested in determining what features of the candy might affect its win percentage. In that case, what is the outcome? What do you think could be a good predictor? The outcome should be the win percentage, and give your best guess for the best predictor! 1. Choosing predictors a. Predictors that do not make sense Not all variables should be considered as potential predictors. Try it! There is one variable in the dataset that would not be helpful as a predictor. Which one? Actually, try predicting the win percentage based on this variable (use lm). How does the model look like (use thesummary function)? # Fit the model and look at the model summary fit_lin &lt;- lm(winpercent ~ competitorname, data = candy) summary(fit_lin) ## ## Call: ## lm(formula = winpercent ~ competitorname, data = candy) ## ## Residuals: ## ALL 85 residuals are 0: no residual degrees of freedom! ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 66.9717 NaN ## competitorname3 Musketeers 0.6312 NaN ## competitornameAir Heads -14.6303 NaN ## competitornameAlmond Joy -16.6242 NaN ## competitornameBaby Ruth -10.0572 NaN ## competitornameBoston Baked Beans -43.5539 NaN ## competitornameCandy Corn -28.9608 NaN ## competitornameCaramel Apple Pops -32.4540 NaN ## competitornameCharleston Chew -27.9967 NaN ## competitornameChewey Lemonhead Fruit Mix -30.9541 NaN ## competitornameChiclets -42.4467 NaN ## competitornameDots -24.6996 NaN ## t value Pr(&gt;|t|) ## (Intercept) NaN NaN ## competitorname3 Musketeers NaN NaN ## competitornameAir Heads NaN NaN ## competitornameAlmond Joy NaN NaN ## competitornameBaby Ruth NaN NaN ## competitornameBoston Baked Beans NaN NaN ## competitornameCandy Corn NaN NaN ## competitornameCaramel Apple Pops NaN NaN ## competitornameCharleston Chew NaN NaN ## competitornameChewey Lemonhead Fruit Mix NaN NaN ## competitornameChiclets NaN NaN ## competitornameDots NaN NaN ## [ reached getOption(&quot;max.print&quot;) -- omitted 73 rows ] ## ## Residual standard error: NaN on 0 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: NaN ## F-statistic: NaN on 84 and 0 DF, p-value: NA The competitor’s name would not make sense to use as a predictor: it is a unique value for each candy. Most of the information for the model was marked as NA. Note that the R-squared value is 1 meaning that we can recover the exact values of win percentage if we know the competitor’s name: that makes sense because once again there is a unique value of win percentage for a candy name. There are two observations in this dataset that are not actually candies! # Check these competitor names candy |&gt; filter(str_detect(competitorname, &quot;One&quot;)) So let’s get rid of them: # Filter out non-candy observations candy &lt;- candy |&gt; filter(!str_detect(competitorname, &quot;One&quot;)) b. Exploring relationships We can visually inspect if there is a relationship between a potential predictor and the outcome. Try it! Pick the predictor that you think would best explain the win percentage of a candy. Use ggplot to represent the relationship between winpercent and the predictor with an appropriate graph. Does there appear to be a relationship to predict the win percentage? # Relationship with a numeric predictor = scatterplot candy |&gt; ggplot(aes(x = sugarpercent, y = winpercent)) + geom_point(size = 4) + labs(x = &quot;Sugar percentage&quot;, y = &quot;Win percentage&quot;) # Relationship with a categorical predictor = grouped boxplot or histogram candy |&gt; ggplot(aes(fill = as.factor(chocolate), y = winpercent)) + geom_boxplot() + labs(x = &quot;Candy contains chocolate: 0 = No, 1 = Yes&quot;, y = &quot;Win percentage&quot;) + scale_x_continuous(labels = NULL, breaks = NULL, limits = c(-1,1)) Sugar percentage does not seem to directly impact the win percentage. But the fact if a candy has chocolate or not does seem to have a higher win percentage. c. Model fit and predictions We can fit a model based on our data to make predictions for the outcome. Try it! Keep working with the predictor that you previously picked. Fit a model and look at the summary. Interpret the sign (+ or -) of the estimate. # Relationship with a numeric predictor # Fit the model and look at the model summary fit_lin &lt;- lm(winpercent ~ sugarpercent, data = candy) summary(fit_lin) ## ## Call: ## lm(formula = winpercent ~ sugarpercent, data = candy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.208 -10.789 -1.452 9.062 36.331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.159 3.257 13.87 &lt;2e-16 *** ## sugarpercent 11.076 5.798 1.91 0.0596 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.52 on 81 degrees of freedom ## Multiple R-squared: 0.04311, Adjusted R-squared: 0.0313 ## F-statistic: 3.649 on 1 and 81 DF, p-value: 0.05963 # Relationship with a categorical predictor # Fit the model and look at the model summary fit_lin &lt;- lm(winpercent ~ chocolate, data = candy) summary(fit_lin) ## ## Call: ## lm(formula = winpercent ~ chocolate, data = candy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.1995 -7.3358 -0.3664 8.8520 24.7670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.271 1.695 24.939 &lt; 2e-16 *** ## chocolate 18.651 2.539 7.347 1.43e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.5 on 81 degrees of freedom ## Multiple R-squared: 0.3999, Adjusted R-squared: 0.3925 ## F-statistic: 53.98 on 1 and 81 DF, p-value: 1.432e-10 Model with sugar percentage: higher sugar percentage means higher win percentage (positive estimate). Model with chocolate: containing chocolate means higher win percentage (positive estimate). d. Performance We should evaluate the performance of a linear regression model with the RMSE and adjusted \\(R^2\\). Try it! Keep working with the same predictor. Report the values of RMSE and adjusted \\(R^2\\) and compare them to a model with a different predictors. How to choose which model is better? # Relationship with a numeric predictor fit_lin &lt;- lm(winpercent ~ sugarpercent, data = candy) sqrt(mean(resid(fit_lin)^2)) # RMSE ## [1] 14.34027 summary(fit_lin)$adj.r.squared # adjusted R-squared ## [1] 0.03129886 # Relationship with a categorical predictor fit_lin &lt;- lm(winpercent ~ chocolate, data = candy) sqrt(mean(resid(fit_lin)^2)) # RMSE ## [1] 11.35639 summary(fit_lin)$adj.r.squared # adjusted R-squared ## [1] 0.3924875 Model with sugar percentage: the win percentage is predicted with about a 14 point difference from reality (RMSE), and only about 3% of the variation in win percentage is explained by sugar percentage. Model with chocolate: the win percentage is predicted with about a 11 point difference from reality (RMSE), and about 39% of the variation in win percentage is explained by sugar percentage. Comparing these two models, it looks like the chocolate variable predicts values of win percentage more accurately and explain more variation in the win percentage. 2. Comparing models a. Using multiple predictors Since we can also include more than one predictor, comparing models with different predictors can be tedious. One strategy is to fit all predictors and only focus on the ones that show more significance in the summary. This is not the best strategy, but it can help reduce the number of predictors. Let’s fit all predictors to explain win percentage: # Fit the model with all predictors but not the one that does not make sense fit_lin &lt;- lm(winpercent ~ ., data = candy |&gt; select(-competitorname)) summary(fit_lin) ## ## Call: ## lm(formula = winpercent ~ ., data = select(candy, -competitorname)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.3555 -6.1266 0.7911 6.6962 23.8793 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.0167 5.0502 6.340 1.86e-08 *** ## chocolate 21.2141 4.1193 5.150 2.24e-06 *** ## fruity 11.0080 4.0684 2.706 0.00853 ** ## caramel 2.7928 3.6880 0.757 0.45140 ## peanutyalmondy 10.6964 3.6481 2.932 0.00453 ** ## nougat 0.4134 5.7135 0.072 0.94252 ## crispedricewafer 8.8846 5.2600 1.689 0.09559 . ## hard -5.8636 3.4660 -1.692 0.09508 . ## bar 1.6927 5.1512 0.329 0.74342 ## pluribus 0.2011 3.1905 0.063 0.94991 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.69 on 71 degrees of freedom ## Multiple R-squared: 0.5455, Adjusted R-squared: 0.4751 ## F-statistic: 7.747 on 11 and 71 DF, p-value: 1.275e-08 Check for * in the last column. Which features are “significant” while taking into account all other variables? It looks like chocolate, fruity, peanutyalmondy, and sugarpercent are the most useful. Try it! Fit the model with only including the most significant predictors. How does adjusted \\(R^2\\) change? # Fit the model with significant predictors fit_lin &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + sugarpercent, data = candy) summary(fit_lin) ## ## Call: ## lm(formula = winpercent ~ chocolate + fruity + peanutyalmondy + ## sugarpercent, data = candy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.6166 -7.8557 -0.0544 6.7393 26.1737 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.001 4.101 7.559 6.70e-11 *** ## chocolate 22.488 3.799 5.919 8.21e-08 *** ## fruity 8.593 3.864 2.224 0.0290 * ## peanutyalmondy 8.929 3.501 2.551 0.0127 * ## sugarpercent 8.407 4.337 1.938 0.0562 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.82 on 78 degrees of freedom ## Multiple R-squared: 0.4884, Adjusted R-squared: 0.4622 ## F-statistic: 18.62 on 4 and 78 DF, p-value: 8.902e-11 Adjusted \\(R^2\\) has decreased a little bit but not by much! We simplified the model a lot though with only 4 predictors instead of 11! There are other strategies for selecting predictors but this is out of scope for our class. While having multiple predictors can improve our ability to make predictions, using too many predictors may lead to overfitting. The model may perform well on our data but will struggle making predictions for new data because it has learned specific patterns rather than generalizable trends. b. Testing for “new” data Since it is usually difficult to gather new data, we use the data that we have available and split it into what we call a train data (to train the model) and a test data (to test the model). For example, consider 80% of the candy data as the train data: # Sample 80% of the dataset into the train data train_data &lt;- sample_frac(candy, 0.8) Now we train the model we chose previously based on that train data: # Fit the model with significant predictors on train data fit_train &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + sugarpercent, data = train_data) summary(fit_train) ## ## Call: ## lm(formula = winpercent ~ chocolate + fruity + peanutyalmondy + ## sugarpercent, data = train_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.422 -7.415 1.061 6.939 24.716 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.313 4.492 6.971 2.60e-09 *** ## chocolate 21.937 4.145 5.293 1.73e-06 *** ## fruity 9.879 4.201 2.351 0.0219 * ## peanutyalmondy 10.184 3.870 2.631 0.0108 * ## sugarpercent 7.484 4.745 1.577 0.1199 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.97 on 61 degrees of freedom ## Multiple R-squared: 0.4849, Adjusted R-squared: 0.4511 ## F-statistic: 14.35 on 4 and 61 DF, p-value: 2.58e-08 Let’s calculate the RMSE for that model: # RMSE for the model fitted on the train data sqrt(mean(resid(fit_train)^2)) ## [1] 10.54971 Did we all get the same RMSE? Why or why not? We probably all got something different! This is because we are looking at a different train data since we randomly picked 80% of the observations. How is that trained model useful for predicting “new” data? Consider the rest of the data to test the model: # Get the rest of the dataset into the test data test_data &lt;- anti_join(candy, train_data, by = &quot;competitorname&quot;) Then evaluate the RMSE for the test data: # Evaluate performance with RMSE on test data sqrt(mean((test_data$winpercent - predict(fit_train, newdata = test_data))^2,)) ## [1] 10.44055 Comparing the value of the RMSE for the test data to the value of the RMSE for the train data can help us evaluate how our model would be able to generalize to new data. If the test RMSE is much higher than the train RMSE, this suggests overfitting: indicating that while the model may perform well on known data, it struggles to make accurate predictions with new data. Ideally, the RMSE for the test data should be similar to the RMSE for the train data. We will talk more about that with cross-validation. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Model building and refresh on Linear regression "],["WS21_IntroductionClassification_Key.html", "Introduction Classification Learning objectives 1. Basic classifications 2. Metrics 3. ROC/AUC Recommended Resources", " Introduction Classification Learning objectives Understand the concept of classification and its applications. Evaluate classification models using accuracy, true positive rate (TPR), and false positive rate (TPR). Interpret Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC). We will use the plotROC package. Install it if you are using it for the first time: # Install new packages (only needed once!) install.packages(&quot;plotROC&quot;) Then load the necessary packages for today: # Load packages library(tidyverse) library(plotROC) We will work with the biopsy dataset that contains information about tumor biopsy results. Nine features of the tumor were measured (on a 1-10 scale) as well as the outcome variable (malignant vs. benign). # Upload the data from GitHub biopsy &lt;- read_csv(&quot;https://raw.githubusercontent.com/laylaguyot/datasets/main//Biopsy.csv&quot;) # Take a quick look at the first 10 rows head(biopsy, 10) Try it! How many outcomes were malignant? How many were benign? # Find how many outcomes of each type table(biopsy$outcome) ## ## benign malignant ## 444 239 There are 444 benign outcomes and 239 malignant outcomes. 1. Basic classifications Let’s try different ways to classify a tumor as malignant or benign. a. Random classification Let’s first predict the outcome randomly. # Create a new object biopsy_pred &lt;- biopsy |&gt; # Only keep variables of interest select(clump_thickness, outcome) |&gt; # Create a predicted variable: using the `sample()` function sample mutate(predicted = sample(x = c(&quot;malignant&quot;,&quot;benign&quot;), # values to sample from size = length(outcome), # how many values to sample replace = TRUE)) # can use each value more than once # Take a look at the predicted variable head(biopsy_pred, 10) Were our predicted values correct? # How do the predicted values compare to the outcome values? table(outcome = biopsy_pred$outcome, predicted = biopsy_pred$predicted) ## predicted ## outcome benign malignant ## benign 201 243 ## malignant 114 125 Sometimes the predicted values were correct, sometimes were not correct. We can compute the accuracy of our predicted values: how many observations were correctly identified as malignant or benign? # Accuracy mean(biopsy_pred$outcome == biopsy_pred$predicted) ## [1] 0.477306 Why does it make sense to get about 50% accuracy? Do we all get the same accuracy? Why/Why not? Since we randomly predicted one out of 2 possible outcomes randomly, we are only correct about half the time. We don’t get the same accuracy because we all made different random predictions. b. Classification based on 1 variable We will focus on predicting the outcome based on clump_thickness. Try it! Use ggplot to represent the distribution of clump_thickness for malignant and benign tumors. Does there seem to be a relationship? # Best option: Using boxplots ggplot(biopsy, aes(x = clump_thickness, fill = outcome)) + geom_boxplot() + scale_y_discrete() + labs(x = &quot;Clump thickness (scale 1-10)&quot;) Tumors with a high clump thickness seem to be malignant (however, note that there is some overlap in clump thickness between the two outcomes). Let’s classify all tumors with a high value of clump_thickness (greater than 9) as malignant, and benign otherwise. # Update our new object biopsy_pred &lt;- biopsy |&gt; # Only keep variables of interest select(clump_thickness, outcome) |&gt; # Create another predicted variable based on clump_thickness &gt; 9 mutate(predicted = ifelse(clump_thickness &gt; 9, &quot;malignant&quot;, &quot;benign&quot;)) # Take a look at the new predicted variable head(biopsy_pred, 10) Were our predicted values correct? # How do the predicted values compare to the outcome values? table(outcome = biopsy_pred$outcome, predicted = biopsy_pred$predicted) ## predicted ## outcome benign malignant ## benign 444 0 ## malignant 170 69 None of the benign cases were predicted as malignant. But the majority of malignant cases were predicted as benign. Does that mean the accuracy has improved? # Accuracy mean(biopsy_pred$outcome == biopsy_pred$predicted) ## [1] 0.7510981 It’s better than before! c. Classification based on any cutoff value We chose a cutoff value of 9 for clump_thickness in the previous section. But what if we had chosen a different value? Let’s calculate the accuracy of our prediction based on different cutoff values. We’ll use a for-loop to repeat the process. # Initialize vector for accuracy values accuracy &lt;- vector() # Define possible cutoff values: from min to max clump thickness cutoff &lt;- min(biopsy$clump_thickness):max(biopsy$clump_thickness) # For each cutoff value: for(i in cutoff){ biopsy_pred &lt;- biopsy |&gt; # Create a predicted variable mutate(predicted = ifelse(clump_thickness &gt; i, # i takes values in cutoff &quot;malignant&quot;, &quot;benign&quot;)) # Find the resulting accuracy and save it into a vector accuracy[i] &lt;- mean(biopsy_pred$outcome == biopsy_pred$predicted) # add element i to accuracy object } accuracy ## [1] 0.5446559 0.6061493 0.7232796 0.8038067 0.8594436 0.8550512 ## [7] 0.8243045 0.7715959 0.7510981 0.6500732 Which value of clump thickness seems to result in the highest accuracy for the predicted values? We found a value of 5 for clump thickness optimized the accuracy: about 0.8594436%. 2. Metrics Let’s consider the classifier based on clump thickness greater than 5. # Make predictions when the cutoff value is 5 for clump_thickness biopsy_pred &lt;- biopsy |&gt; # Only keep variables of interest select(clump_thickness, outcome) |&gt; # Create another predicted variable based on clump_thickness &gt; 9 mutate(predicted = ifelse(clump_thickness &gt; 5, &quot;malignant&quot;, &quot;benign&quot;)) We will consider that a malignant case is a positive outcome and benign is a negative outcome (in medical contexts, a positive case usually means that a disease/condition was detected). # Confusion matrix: compare the true outcomes to predicted values table(outcome = biopsy_pred$outcome, predicted = biopsy_pred$predicted) |&gt; # Add total cases for rows and columns addmargins() ## predicted ## outcome benign malignant Sum ## benign 424 20 444 ## malignant 76 163 239 ## Sum 500 183 683 The true positive rate (TPR) is the number of truly predicted positive cases over the number of positive cases. The false positive rate (FPR) is the number of truly negative cases that were predicted to be positive over the number of negative cases. Try it! Based on the table and definitions above, what is the value of TPR? What is the value of FPR? What shall we do to increase the value of TPR? How would it affect the value of FPR? # TPR: Number of malignant outcome predicted malignant over number of total malignant outcomes 163/239 ## [1] 0.6820084 # FPR: Number of benign outcome predicted malignant over number of total benign outcomes 20/444 ## [1] 0.04504505 The value of TPR indicates that about 68.2% of the malignant outcomes were truly identified while the value of FPR indicates that about 4.5% of the benign outcomes were falsely predicted as malignant. We could lower the value of clump thickness to predict more malignant cases but that also means we would have more false positive as well. What if we wanted to do a better job at predicting malignant outcomes? 3. ROC/AUC The trade-off between TPR and FPR can be represented by the ROC curve. a. Receiver Operating Characteristics (ROC) curve A ROC curve usually represents the false positive rate (FPR) on the x-axis and the true positive rate (TPR) is represented on the y-axis: # Plot ROC depending on values of clump_thickness to predict the outcome ROC &lt;- ggplot(biopsy) + # New geom! geom_roc(aes(d = outcome, m = clump_thickness), n.cuts = 10) + labs(title = &quot;ROC curve based on clump thickness (scale 1 to 10&quot;) ROC ## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = ## 0 and malignant = 1! Note: R usually expects the outcome to be coded as 0 and 1, representing a negative and positive outcome, respectively. Since R assigns values in alphabetical order, here it made sense that the benign outcome is 0 and the malignant outcome is 1. When the cutoff value to predict a malignant outcome is based on a clump_thickness greater or equal to 1, we get a TPR value of 100% and a FPR value of 100% as well. Why? When the cutoff is clump_thickness ≥ 1, the model predicts every case as malignant, leading to a TPR of 100% (capturing all malignant cases) but also an FPR of 100% (misclassifying all benign cases as malignant). When the cutoff value to predict a malignant outcome is based on a clump_thickness greater or equal to 10, we get a TPR value of about 29% (see below or on the graph) and a FPR value of 0% as well. Why? When the cutoff is clump_thickness ≥ 10, the model predicts only the most extreme cases as malignant, leading to a TPR of about 29% (correctly identifying some malignant cases) and a FPR of 0% (no benign cases are misclassified as malignant). # Number of predicted malignant values with a clump thickness of 10 out of all true malignant outcomes sum((biopsy_pred |&gt; filter(clump_thickness == 10))$predicted == &quot;malignant&quot;) / sum(biopsy_pred$outcome == &quot;malignant&quot;) ## [1] 0.2887029 # Note: I asked ChatGPT for a better way to this calculation but it only provided incorrect alternatives... What about the TPR and FPR values when the cutoff value to predict a malignant outcome is based on a clump_thickness greater or equal to 5? When the cutoff is clump_thickness ≥ 5, the model leads to a TPR of about 87.5% and a FPR slighlty less than 25%. b. Area under the curve (AUC) The area under the curve (AUC) quantifies how well our classification is predicting the outcome. # Calculate the area under the curve with function calc_auc() calc_auc(ROC) ## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = ## 0 and malignant = 1! Let’s investigate what it means (this is a little bit difficult). If we randomly select 2 patients, one with a malignant tumor and one with a benign tumor, we will compare their clump thickness: if the clump thickness was higher for the patient with a malignant tumor, we assign a probability of 1 (that agrees with our model), if the clump thickness was the same for the two patients, we assign a probability of 0.5, if the clump thickness was lower for the patient with a malignant tumor, we assign a probability of 0 (that does not agree with our model). Then we replicate that process 1,000 times. # Replicate the process 1000 times probs &lt;- replicate(1000,{ # Sample 1 patient with a malignant outcome rand_positive &lt;- biopsy |&gt; filter(outcome == &quot;malignant&quot;) |&gt; select(clump_thickness) |&gt; sample_n(size = 1) |&gt; pull() # Sample 1 patient with benign outcome rand_negative &lt;- biopsy |&gt; filter(outcome == &quot;benign&quot;) |&gt; select(clump_thickness) |&gt; sample_n(size = 1) |&gt; pull() # Assign a probability value according to our model case_when(rand_positive &gt; rand_negative ~ 1, rand_positive == rand_negative ~ .5, rand_positive &lt; rand_negative ~ 0) }) # AUC mean(probs) ## [1] 0.902 You can interpret the AUC as the fact that a randomly selected patient with a malignant tumor has a higher predicted probability to have a malignant tumor than a randomly selected person with a benign tumor. On average, about 91% of the time, malignant tumors will have higher probabilities of being malignant compared to benign outcomes. In a nutshell: the higher the AUC, the better the classifier is! Try it! Pick another predictor in the biopsy dataset. Visualize the relationship with the outcome. Could this new predictor help us classify a tumor as malignant or benign? Build the ROC curve and find the corresponding AUC value. Is this new predictor resulting in a better model than the one with clump thickness? Paste the visualization of the relationship, your ROC plot, and the value of AUC on the slide corresponding to the new predictor: https://docs.google.com/presentation/d/1tPd0MzCnx5TOIDOoMMEXbruT2I7ijQvBHq031op_uAk/edit?usp=sharing # Let&#39;s pick another predictor: uniform_cell_size ggplot(biopsy, aes(x = uniform_cell_size, fill = outcome)) + geom_boxplot() + scale_y_discrete() + labs(x = &quot;Uniform cell size (scale 1-10)&quot;) # Plot ROC depending on values of uniform_cell_size to predict the outcome ROC &lt;- ggplot(biopsy) + # New geom! geom_roc(aes(d = outcome, m = uniform_cell_size), n.cuts = 10) ROC ## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = ## 0 and malignant = 1! # Calculate the area under the curve calc_auc(ROC)$AUC ## Warning in verify_d(data$d): D not labeled 0/1, assuming benign = ## 0 and malignant = 1! ## [1] 0.9758236 A high value of uniform cell size seems to be associated with a malignant outcome. The AUC value is about 98%, that’s higher than for the model with clump thickness: we have a better classification model with uniform cell size! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Introduction to machine learning R documentation for plotROC "],["WS22_LogisticRegression_Key.html", "Logistic Regression Learning objectives 1. Predicting a binary response with a numeric predictor 2. Using multiple predictors Recommended Resources", " Logistic Regression Learning objectives Use logistic regression to model a binary outcome variable based on one or more predictors. Make predictions using the logistic model and apply a classification threshold to obtain binary classifications. Evaluate model performance using an ROC curve and calculate the Area Under the Curve (AUC). We will use the packages tidyverse and plotROC: # Load packages library(tidyverse) library(plotROC) We will continue working with the biopsy dataset that contains information about tumor biopsy results. # Upload the data from GitHub biopsy &lt;- read_csv(&quot;https://raw.githubusercontent.com/laylaguyot/datasets/main//Biopsy.csv&quot;) # Take a quick look at 10 random rows head(biopsy, 10) This dataset contains information about 9 different features of tumors that we will use to predict the outcome variable (a malignant vs. benign tumor). When using classification models in R, the functions usually expect the outcome to be coded as 0 or 1 (0 represents a “negative” case and 1 a “positive” case). Try it! Overwrite the outcome variable in biopsy so that it has value 0 or 1. What proportion of tumors are malignant? # Overwrite biopsy biopsy &lt;- biopsy |&gt; mutate(outcome = ifelse(outcome == &quot;malignant&quot;, 1, 0)) mean(biopsy$outcome) ## [1] 0.3499268 Almost 35% of the tumors were malignant in the dataset. 1. Predicting a binary response with a numeric predictor First, let’s predict the outcome based on clump_thickness. a. Visualizing the model In the previous worksheet, we already looked at the relationship between these two variables but let’s take a slightly different look: # Represent the relationship with a model biopsy |&gt; ggplot(aes(x = clump_thickness, y = outcome)) + # Consider a logistic regression model geom_smooth(method = &quot;glm&quot;, se = FALSE, method.args = list(family = &quot;binomial&quot;), size = 2) + # Show original data geom_point(size = 4, alpha = 0.5) + labs(x = &quot;Clump thickness (scale 1-10)&quot;, y = &quot;Outcome (1 = malignant, 0 = benign)&quot;, title = &quot;Logistic regression model to predict malignancy based on clump thickness&quot;) How did R choose a logistic curve to fit this data? b. Fitting a model We can find the expression of the logistic regression model with the glm(outcome ~ predictor, data = ..., family = \"binomial\") function (glm stands for generalized linear models): # Fit the model fit_log &lt;- glm(outcome ~ clump_thickness, data = biopsy, family = &quot;binomial&quot;) # Take a look at the model summary summary(fit_log) ## ## Call: ## glm(formula = outcome ~ clump_thickness, family = &quot;binomial&quot;, ## data = biopsy) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.11012 0.37894 -13.48 &lt;2e-16 *** ## clump_thickness 0.93042 0.07418 12.54 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 884.35 on 682 degrees of freedom ## Residual deviance: 458.48 on 681 degrees of freedom ## AIC: 462.48 ## ## Number of Fisher Scoring iterations: 6 The output gives the logit-form of the model which is: \\(\\ln{\\frac{\\hat{p}}{1-\\hat{p}}} = -5.11012 + 0.93042 * clump\\_thickness\\) where \\(\\hat{p}\\) is the probability of the tumor being malignant (the value of 1). Then we can predict the probability of the tumor being malignant by using the probability form: \\(\\hat{p} = \\frac{e^{-5.11012 + 0.93042 * clump\\_thickness}}{1 + e^{-5.11012 + 0.93042 * clump\\_thickness}}\\) c. Making predictions Let’s use the expression of the model to calculate predicted values. Try it! Use the expression of the probability form (note that exp() refers to the exponential function) to create a new variable called probability that predicts the probability of the tumor being malignant based on values of clump_thickness. Based on these probabilities, how do we decide if the tumor should be considered as malignant or benign? biopsy |&gt; # Use the expression of the model mutate(probability = exp(-5.11012 + 0.93042 * clump_thickness)/(1 + exp((-5.11012 + 0.93042 * clump_thickness)))) |&gt; select(clump_thickness, outcome, probability) We would need to define some cutoff value: for example, if the probability is greater than 0.5 then the tumor should be predicted as malignant, and benign otherwise. Much more convenient to calculate probability values (especially when we will have more predictors with a longer expression for the model) is the predict(model_name, type = \"response\") function: biopsy |&gt; # Calculate probability values mutate(probability = predict(fit_log, type = &quot;response&quot;)) |&gt; select(clump_thickness, outcome, probability) We can use the probability values to predict the outcome as malignant or benign (1 or 0, respectively). We would have to decide on a cutoff value for the probability of the tumor being malignant. For example, let’s try the cutoff value 0.5: biopsy_pred &lt;- biopsy |&gt; # Create new variables for probability and predicted values mutate(probability = predict(fit_log, type = &quot;response&quot;), predicted = ifelse(probability &gt; 0.5, 1, 0)) |&gt; select(clump_thickness, outcome, probability, predicted) # Take a look head(biopsy_pred, 10) We can also make predictions for new data. For example, let’s consider a tumor with a clump thickness of 5: # Make predictions for new data tumor &lt;- data.frame(clump_thickness = 5) predict(fit_log, newdata = tumor, type = &quot;response&quot;) ## 1 ## 0.3874509 The model predicts a probability of approximately 39% for the tumor to be malignant if it has a clump thickness of 5. d. Error in predicted values Sometimes our predicted values are correct, sometimes they’re not! Recall the concepts of True Positive/True Negative (correct predicted values) and False Positive/False Negative (incorrect predicted values). Try it! Using biopsy_pred, visualize the logistic regression model as above and color the original data points by the predicted value for the outcome. Which points on the graph show True Positive cases? False Positive cases? # Represent the relationship with a model biopsy_pred |&gt; ggplot(aes(x = clump_thickness, y = outcome)) + # Consider a logistic regression model geom_smooth(method = &quot;glm&quot;, se = FALSE, method.args = list(family = &quot;binomial&quot;), size = 2) + # Show original data, colored by predicted values geom_point(size = 4, alpha = 0.5, aes(color = as.factor(predicted))) + # add as.factor() to only see 2 categories labs(x = &quot;Clump thickness (scale 1-10)&quot;, y = &quot;Outcome (1 = malignant, 0 = benign)&quot;, title = &quot;Logistic regression model to predict malignancy based on clump thickness&quot;) The True Positive cases are the malignant cases that were truly detected as malignant: blue dots on the top. The FALSE Positive cases are the benign cases that were falsely detected as malignant: blue dots on the bottom. e. Performance To assess the performance for a logistic regression models, we can consider the ROC curve and the corresponding area under the curve (AUC): # ROC curve ROC &lt;- biopsy_pred |&gt; ggplot() + # the predictions are based on the probability values geom_roc(aes(d = outcome, m = probability), n.cuts = 10) ROC The cutoff values shown on the ROC curve represent possible thresholds for classifying a tumor as malignant or benign based on the predicted probability (it does not have to be 0.5 as we tried earlier). # Calculate the area under the curve calc_auc(ROC)$AUC ## [1] 0.908878 Our classifier seems to perform well with a AUC of approximately 91%. 2. Using multiple predictors What if we add more predictors to our model? That way we can try to predict the outcome with more information: # Fit the model using two predictors fit_log &lt;- glm(outcome ~ clump_thickness + uniform_cell_size, data = biopsy, family = &quot;binomial&quot;) # Take a look at the model summary summary(fit_log) ## ## Call: ## glm(formula = outcome ~ clump_thickness + uniform_cell_size, ## family = &quot;binomial&quot;, data = biopsy) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.38224 0.64375 -11.468 &lt; 2e-16 *** ## clump_thickness 0.61964 0.09649 6.422 1.35e-10 *** ## uniform_cell_size 1.29019 0.13740 9.390 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 884.35 on 682 degrees of freedom ## Residual deviance: 196.58 on 680 degrees of freedom ## AIC: 202.58 ## ## Number of Fisher Scoring iterations: 7 The model becomes more complex. How has the performance of our model improved? # ROC curve ROC &lt;- biopsy |&gt; # Recalculate predictions mutate(probability = predict(fit_log, type = &quot;response&quot;)) |&gt; ggplot() + geom_roc(aes(d = outcome, m = probability), n.cuts = 10) ROC The curve looks like the performance has improved (curving close to the top left corner, meaning that the True Positive rate is high for a low False Positive rate). # Calculate the area under the curve calc_auc(ROC)$AUC ## [1] 0.9879095 The value of the AUC is closer to 1 so our model with two predictors can better predict true malignant cases without many false malignant cases! Notes: Adding too many variables can create issues such as overfitting: the model can become too specific to the tumors in the dataset on which we “train” the model and it will be very difficult to generalize to other tumors (which is the goal of creating a model). We can quickly check which features might be more useful for predicting the outcome by looking at the last column in the model output. Any . or * indicates which features are more useful in predicting the outcome. Try it! Fit a model with all predictors that make sense. Which predictors seem to be most useful to predict the malignancy of a tumor? What is the corresponding value of AUC? # Fit the model using two predictors fit_log &lt;- glm(outcome ~ ., data = biopsy, family = &quot;binomial&quot;) # Take a look at the model summary summary(fit_log) ## ## Call: ## glm(formula = outcome ~ ., family = &quot;binomial&quot;, data = biopsy) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.10394 1.17488 -8.600 &lt; 2e-16 *** ## clump_thickness 0.53501 0.14202 3.767 0.000165 *** ## uniform_cell_size -0.00628 0.20908 -0.030 0.976039 ## uniform_cell_shape 0.32271 0.23060 1.399 0.161688 ## marg_adhesion 0.33064 0.12345 2.678 0.007400 ** ## epithelial_cell_size 0.09663 0.15659 0.617 0.537159 ## bare_nuclei 0.38303 0.09384 4.082 4.47e-05 *** ## bland_chromatin 0.44719 0.17138 2.609 0.009073 ** ## normal_nucleoli 0.21303 0.11287 1.887 0.059115 . ## mitoses 0.53484 0.32877 1.627 0.103788 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 884.35 on 682 degrees of freedom ## Residual deviance: 102.89 on 673 degrees of freedom ## AIC: 122.89 ## ## Number of Fisher Scoring iterations: 8 # ROC curve ROC &lt;- biopsy |&gt; # Recalculate predictions mutate(probability = predict(fit_log, type = &quot;response&quot;)) |&gt; ggplot() + geom_roc(aes(d = outcome, m = probability), n.cuts = 10) ROC # Calculate the area under the curve calc_auc(ROC)$AUC ## [1] 0.9963248 While taking into account all predictors, the most useful predictors seem to be clump thickness and bare nuclei. The AUC value has increased again, indicating that this model is even better at detecting true malignant cases in this dataset. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings: Logistic regression R documentation for glm "],["WS23_OtherModels_Key.html", "Other Models Learning objectives 1. Decision trees for a numeric outcome 2. Decision trees for a categorical outcome Recommended Resources", " Other Models Learning objectives Understand the structure and intuition of decision trees. Build decision tree models for a numeric outcome or a categorical outcome. Evaluate model performance using appropriate metrics. We will use many packages today! First, install rpart and rpart.plot: # Install new packages (only needed once!) install.packages(&quot;rpart&quot;) install.packages(&quot;rpart.plot&quot;) Then load the packages: # Load packages library(tidyverse) library(rpart) library(rpart.plot) library(plotROC) We will explore a new dataset, titanic_dataset, which contains information about passengers of the Titanic, that sank on 15 April 1912 after colliding with an iceberg. # Upload the data from GitHub titanic_dataset &lt;- read_csv(&quot;https://raw.githubusercontent.com/laylaguyot/datasets/main//titanic_dataset.csv&quot;) # Take a quick look head(titanic_dataset) Here are some details about how some of the variables were coded: if a passenger Survived the sinking (Yes = 1, No = 0) the passenger class, Pclass (First Class = 1, Second Class = 2, Third Class = 3) the number of siblings or spouses, SibSp, that a passenger had on board and the number of parents or children, Parch, the passenger had on board the port of embarkation for the passenger, Embarked (Cherbourg = C, Queenstown = Q, Southampton = S). If we wanted to predict the value of the fare that a passenger paid for the trip based on some predictors, which variable would be the outcome? Which modeling approach would be more appropriate: regression or classification? The outcome would be Fare which is a numeric variable so regression is more appropriate. If we wanted to predict if a passenger survived or not, which variable would be the outcome? Which modeling approach would be more appropriate: regression or classification? The outcome would be Survived which is a categorical variable so classification is more appropriate.* Which variables in the titanic_dataset would not be appropriate to include in the models described above? Any variable that is too specific to a passenger: PassengerId, Name, Ticket, or Cabin. Let’s consider the algorithm of the decision tree. This model comes up with some rules to split our data into subsets. 1. Decision trees for a numeric outcome For a numeric outcome, the prediction is done according to the average outcome among the subset. a. Fitting a model We can use the function rpart(outcome ~ predictor, data = ...) from the `rpart`` package to fit this model: # Consider the decision tree model fit_tree_reg &lt;- rpart(Fare ~ Pclass, data = titanic_dataset) b. Visualizing the model We can visualize our tree with its corresponding rules with the function rpart.plot: # Visualize the decision tree rpart.plot(fit_tree_reg) Try it! To make sense of the decision tree above, 1) calculate the mean Fare across all classes, 2) calculate the mean Fare in each class, 3) find the percentage of passengers in each class. # The mean `Fare` is the number shown before the rule is applied titanic_dataset |&gt; summarize(mean(Fare)) # The mean `Fare` per subset is the number shown after the rule is applied titanic_dataset |&gt; group_by(Pclass) |&gt; summarize(mean = mean(Fare), n = n()) |&gt; mutate(percent = n/sum(n)) The tree made a rule: compare the fare paid in 1st class versus 2nd/3rd class. The predicted value for the Fare is $16 if a passenger was in 2nd/3rd class while it is $84 if the passenger was in 1st class. c. Making predictions Let’s use the model to find the average fare for a passenger based on their class with the predict(model, data) function: # Find the average fare for each subgroup titanic_dataset |&gt; mutate(predicted = predict(fit_tree_reg)) Note that we only have two possible predicted values since we only have two subsets (1st class vs 2nd/3rd class). d. Evaluating performance Let’s evaluate the performance of this model with RMSE: # Evaluate performance with RMSE sqrt(mean((titanic_dataset$Fare - predict(fit_tree_reg, titanic_dataset))^2)) ## [1] 40.03789 The decision tree model has a typical prediction error of approximately $40. Try it! Add another predictor to predict the Fare using a decision tree model. Does the performance of the model improve? # Fit another decision tree model fit_tree_reg &lt;- rpart(Fare ~ Pclass + Embarked, data = titanic_dataset) # Visualize the decision tree rpart.plot(fit_tree_reg) # Evaluate performance with RMSE sqrt(mean((titanic_dataset$Fare - predict(fit_tree_reg, titanic_dataset))^2)) ## [1] 39.19377 The performance has slightly improved with a lower RMSE. 2. Decision trees for a categorical outcome For a categorical outcome, the prediction is done according to the majority outcome among the subsets. a. Fitting a model We can still use the function rpart(outcome ~ predictor, data = ...) by adding method = \"class\": # Consider the decision tree model fit_tree_class &lt;- rpart(Survived ~ Pclass, data = titanic_dataset, method = &quot;class&quot;) # classification b. Visualizing the model We can visualize our tree with its corresponding rules with the function rpart.plot: # Visualize the decision tree rpart.plot(fit_tree_class) Try it! To make sense of the decision tree above, 1) calculate the proportion of passengers who survived across all classes, 2) the proportion of passengers who survived in each class. # The proportion of `Survived` is the number shown before the rules are applied titanic_dataset |&gt; summarize(mean(Survived)) # The propotion of `Survived` is the number shown after the rules are applied titanic_dataset |&gt; group_by(Pclass) |&gt; summarize(percent_survived = mean(Survived), nb_passengers = n()) |&gt; mutate(percent_class = nb_passengers/sum(nb_passengers)) The tree made two rules: compare the survival of passengers in 3rd class versus 1st/2nd class then compare the survival of passengers in 2nd class versus 1st class. The proportion of survival was predicted to be 63% on 1st class, 47% in 2nd class, and 24% in 3rd class. The percentage under represents the amount of data: 24% of the passengers were in 1st class, 21% in 2nd class, 55% in 3rd class. c. Making predictions We can use the model to find the probability of a passenger to have survived based on their class with the predict(model, data) function: # Find the probability for each passenger to have survived predict(fit_tree_class, titanic_dataset) ## 0 1 ## 1 0.7576375 0.2423625 ## 2 0.3703704 0.6296296 ## 3 0.7576375 0.2423625 ## 4 0.3703704 0.6296296 ## 5 0.7576375 0.2423625 ## 6 0.7576375 0.2423625 ## 7 0.3703704 0.6296296 ## 8 0.7576375 0.2423625 ## 9 0.7576375 0.2423625 ## 10 0.5271739 0.4728261 ## 11 0.7576375 0.2423625 ## 12 0.3703704 0.6296296 ## 13 0.7576375 0.2423625 ## 14 0.7576375 0.2423625 ## 15 0.7576375 0.2423625 ## 16 0.5271739 0.4728261 ## 17 0.7576375 0.2423625 ## 18 0.5271739 0.4728261 ## 19 0.7576375 0.2423625 ## 20 0.7576375 0.2423625 ## 21 0.5271739 0.4728261 ## 22 0.5271739 0.4728261 ## 23 0.7576375 0.2423625 ## 24 0.3703704 0.6296296 ## 25 0.7576375 0.2423625 ## [ reached getOption(&quot;max.print&quot;) -- omitted 866 rows ] # or titanic_dataset |&gt; mutate(predicted = predict(fit_tree_class)[,2]) The output shows two columns. Indeed, the predict() function provides the proportions of each “outcome” in each subset. In our context, we are interested in the second column which indicates the probability of surviving: the predictions can be calculated with predict(model, data)[ ,2]. d. Evaluating performance We can evaluate the performance of this model by visualizing the ROC curve: # ROC curve ROC &lt;- ggplot(titanic_dataset) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the decision tree model m = predict(fit_tree_class, titanic_dataset)[ ,2])) ROC And report the AUC: # Calculate the area under the curve calc_auc(ROC)$AUC ## [1] 0.681417 This model is not great… Try it! Add another predictor to predict if a passenger Survived using a decision tree model. Does the performance of the model improve? # Fit another decision tree model fit_tree_class &lt;- rpart(Survived ~ Pclass + Sex, data = titanic_dataset, method = &quot;class&quot;) # Visualize the decision tree rpart.plot(fit_tree_class) # ROC curve ROC &lt;- ggplot(titanic_dataset) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the decision tree model m = predict(fit_tree_class, titanic_dataset)[ ,2])) ROC # Calculate the area under the curve calc_auc(ROC)$AUC ## [1] 0.7668728 The performance has improved with a higher AUC value! e. Predicting more than 2 outcomes Decision trees can predict more than two outcomes. For example, instead of predicting binary outcomes (such as Survived or not), we might predict a categorical variable with more than two classes. Let’s consider predicting the passenger class (Pclass), which has three possible outcomes (1st class, 2nd class, and 3rd class) based on the Fare paid by a passenger. # Fit another decision tree model fit_tree_class &lt;- rpart(Pclass ~ Fare, data = titanic_dataset, method = &quot;class&quot;) # Visualize the decision tree rpart.plot(fit_tree_class) After fitting the model, we can check the predicted probabilities for each class. By default, predict() gives the probabilities for each class: # Probability of each class predict(fit_tree_class, titanic_dataset) ## 1 2 3 ## 1 0.01769912 0.01769912 0.9646018 ## 2 0.70588235 0.11418685 0.1799308 ## 3 0.01769912 0.01769912 0.9646018 ## 4 0.70588235 0.11418685 0.1799308 ## 5 0.01769912 0.01769912 0.9646018 ## 6 0.01769912 0.01769912 0.9646018 ## 7 0.70588235 0.11418685 0.1799308 ## 8 0.00000000 0.19354839 0.8064516 ## 9 0.00000000 0.87755102 0.1224490 ## 10 0.70588235 0.11418685 0.1799308 ## 11 0.00000000 0.19354839 0.8064516 ## 12 0.70588235 0.11418685 0.1799308 ## 13 0.01769912 0.01769912 0.9646018 ## 14 0.70588235 0.11418685 0.1799308 ## 15 0.01769912 0.01769912 0.9646018 ## 16 0.00000000 0.19354839 0.8064516 ## [ reached getOption(&quot;max.print&quot;) -- omitted 875 rows ] We can also find the most likely class for each passenger based on the probabilities by using the type = \"class\" option in the predict() function: # Determine which class is more likely for each passenger predict(fit_tree_class, titanic_dataset, type = &quot;class&quot;) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## 3 1 3 1 3 3 1 3 2 1 3 1 3 1 3 3 1 2 3 3 2 2 ## 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## 3 1 3 1 3 1 3 3 1 1 3 2 1 1 3 3 3 2 3 3 3 1 ## 45 46 47 48 49 50 ## 3 3 3 3 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 841 entries ] ## Levels: 1 2 3 Since we are not just dealing with positive/negative cases, the ROC curve and AUC values cannot be used here. But we can report the accuracy: the proportion of cases that were truly predicted in each class: # Adding predicted class titanic_pred &lt;- titanic_dataset |&gt; mutate(predicted = predict(fit_tree_class, titanic_dataset, type = &quot;class&quot;)) # Confusion matrix table(Pclass = titanic_pred$Pclass, predicted = titanic_pred$predicted) |&gt; # Add total cases for rows and columns addmargins() ## predicted ## Pclass 1 2 3 Sum ## 1 204 6 6 216 ## 2 33 121 30 184 ## 3 52 12 427 491 ## Sum 289 139 463 891 # Reporting accuracy mean(titanic_pred$Pclass == titanic_pred$predicted) ## [1] 0.8439955 The model is producing a relatively high accuracy! Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings for other models in Examples of algorithms R documentation for rpart and rpart.plot "],["WS24_CrossValidation_Key.html", "Cross Validation Learning objectives 1. Fit a model on the entire dataset 2. Train and Test a model 3. k-fold cross-validation Recommended Resources", " Cross Validation Learning objectives Understand the purpose of cross-validation and how it helps assess model generalizability. Split data into training and test sets, and evaluate performance on both. Implement and interpret k-fold cross-validation. Start the workflow by uploading the necessary packages: # Load packages library(tidyverse) library(rpart) library(plotROC) Recall the titanic_dataset, which contains information about passengers of the Titanic. # Upload the data from GitHub titanic_dataset &lt;- read_csv(&quot;https://raw.githubusercontent.com/laylaguyot/datasets/main//titanic_dataset.csv&quot;) # Take a quick look head(titanic_dataset) We will continue looking at two potential outcomes in this dataset: predicting the Fare paid or predicting if a passenger Survived. Some variables cannot be used in the model because they are too specific to each passenger: PassengerId, Name, Ticket, and Cabin. Let’s keep PassengerId to identify unique rows but remove other variables from the dataset. Also, let’s ignore missing values for now since they would be excluded from modeling: # Prepare the dataset for modeling titanic_modeling &lt;- titanic_dataset |&gt; select(-Name, -Ticket, -Cabin) |&gt; na.omit() # New dimensions dim(titanic_modeling) ## [1] 712 9 Less rows! 1. Fit a model on the entire dataset In the last worksheet, we considered a model to predict if a passenger survived based solely on the passenger’s class. Let’s take a look at using a logistic regression model: # Fit a logistic regression model fit_log &lt;- glm(Survived ~ Pclass, data = titanic_modeling, family = &quot;binomial&quot;) # Calculate performance with AUC ROC &lt;- ggplot(titanic_modeling) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(fit_log, type = &quot;response&quot;))) calc_auc(ROC)$AUC ## [1] 0.693298 The model is pretty bad at predicting if a passenger survived or not… What if we considered more predictors? Try it! Include all predictors that make sense in this context. Has the performance of the logistic regression model improved? # Fit a logistic regression model fit_log &lt;- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = titanic_modeling, family = &quot;binomial&quot;) # Calculate performance with AUC ROC &lt;- ggplot(titanic_modeling) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(fit_log, type = &quot;response&quot;))) calc_auc(ROC)$AUC ## [1] 0.8601939 Yes, the performance has improved! However, we discussed that adding many predictors could overspecify the model: if the model is too specific to the particular data, it would not be able to make appropriate predictions on new data. That’s why we want to test the performance of our model with cross-validation. The principle of cross-validation is to train a model on some data and test the model’s performance on “new” data. Since we can’t reproduce the sinking of the Titanic, we will use the data available and split it as a train set and a test set. 2. Train and Test a model Let’s separate our entire dataset into a train dataset and a test dataset (representing about 70% and 30% of the entire dataset, respectively): # Sample 70% of the dataset into the train set train_data &lt;- sample_frac(titanic_modeling, 0.7) # Get the rest of the dataset into the test set test_data &lt;- anti_join(titanic_modeling, train_data, by = &quot;PassengerId&quot;) a. Train the model on the train set Let’s consider the logistic regression model to predict the fare paid by a passenger based on multiple predictors: # Fit a logistic regression model on train data train_model &lt;- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_data, family = &quot;binomial&quot;) Since the train model is fitted on the train data, train_model is the best model to fit the train data. # Calculate performance on train data with AUC ROC &lt;- ggplot(train_data) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(train_model, type = &quot;response&quot;))) calc_auc(ROC)$AUC ## [1] 0.8637815 Do well all get the same AUC value? Why or why not? The train data and test data were split randomly: we don’t actually have the same train data so that means we also don’t have the same model! But how does this model work on “new” data? We will compare the performance of train_model on the train data with the performance on the test data. b. Test the model on the test set Let’s compare the performance of the model on the train data vs the performance on the test data for predicting if a passenger survived or not: # Calculate performance on test data with AUC ROC &lt;- ggplot(test_data) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(train_model, newdata = test_data, type = &quot;response&quot;))) calc_auc(ROC)$AUC ## [1] 0.8493587 How does the model’s performance differ between the training and test data? Different answers are possible depending on AUC values (recall that the higher the AUC value, the better the performance is): if the AUC value is higher for the test dataset: the performance of the model is better for new data. if the AUC value is lower for the test dataset: the performance of the model is worse for new data. if the AUC value is about the same on both datasets: the performance of the model is stable for new data. Try it! What if you tried the decision tree model using the same predictors? How does it perform on the entire dataset? How does it perform on new data? # Fit the decision tree model fit_tree &lt;- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = titanic_modeling, method = &quot;class&quot;) # Calculate performance on train data with AUC ROC &lt;- ggplot(titanic_modeling) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(fit_tree, newdata = titanic_modeling)[ ,2])) calc_auc(ROC)$AUC ## [1] 0.8367892 # Fit the decision tree model train_model &lt;- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Embarked, data = train_data, method = &quot;class&quot;) # Calculate performance on test data with AUC ROC &lt;- ggplot(test_data) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(train_model, newdata = test_data)[ ,2])) calc_auc(ROC)$AUC ## [1] 0.768023 Compare if the AUC value is lower/higher for the performance on the test dataset. 3. k-fold cross-validation Previously, our results for comparing the performance might differ based on which train data and test data we considered. Let’s try the k-fold cross-validation to repeat this process. a. Define algorithm Algorithm for k-fold cross-validation: Divide datasets into k equal parts (usually 5 or 10) Use k−1 parts as the train data Test the model on the remaining part, the test data Repeat k times, so each part has been used once as a test data Average performance over k performances b. Create folds First, we will create the different folds: # Choose number of folds k = 5 # To have the same random sample, use set.seed set.seed(322) # Randomly order rows in the dataset shuffled_data &lt;- titanic_modeling[sample(nrow(titanic_modeling)), ] # Create k folds in the dataset shuffled_data &lt;- titanic_modeling |&gt; mutate(folds = cut(seq(1:nrow(shuffled_data)), breaks = k, labels = FALSE)) Try it! Check how many observations are in each fold. # Find frequencies of folds table(shuffled_data$folds) ## ## 1 2 3 4 5 ## 143 142 142 142 143 About the same number of observations are in each fold. c. Repeat the process We can focus on each fold at a time, repeating the same process by using a for-loop. For example: # Use a for-loop to keep track of the size of the train data for(i in 1:k){ train_i &lt;- shuffled_data |&gt; filter(folds != i) # train data = all observations except in fold i # Find number of rows print(nrow(train_i)) } ## [1] 569 ## [1] 570 ## [1] 570 ## [1] 570 ## [1] 569 We can keep track of the size of each fold to train the data by saving each value in a vector: # Initialize a vector to keep track of the size of the train data n_train &lt;- NULL # Use a for-loop to get performance for each k-fold for(i in 1:k){ train_i &lt;- shuffled_data |&gt; filter(folds != i) # train data = all observations except in fold i # Save number of rows n_train[i] &lt;- nrow(train_i) } # Check that object n_train ## [1] 569 570 570 570 569 d. Evaluate average performance Now putting it together: we fit a model on each train data (all but the i-th fold) and evaluate the performance on the corresponding test data (the i-th fold) and repeat the process for each k-fold (using a for-loop): # Initialize a vector to keep track of the performance for each k-fold perf_k &lt;- NULL # Use a for-loop to get performance for each k-fold for(i in 1:k){ # Split data into train and test data train_i &lt;- shuffled_data |&gt; filter(folds != i) # train data = all observations except in fold i test_i &lt;- shuffled_data |&gt; filter(folds == i) # test data = observations in fold i # Train model on train data (all but fold i) # CHANGE: what model/predictors should be included train_model &lt;- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_i, family = &quot;binomial&quot;) # Performance listed for each test data = fold i # CHANGE: how the performance is calculated ROC &lt;- ggplot(test_i) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(train_model, newdata = test_i, type = &quot;response&quot;))) # Save the AUC values perf_k[i] &lt;- calc_auc(ROC)$AUC } # Performance for each fold perf_k ## [1] 0.8193297 0.8451548 0.8482017 0.8452236 0.8797209 # Average performance over all k folds mean(perf_k) ## [1] 0.8475261 sd(perf_k) ## [1] 0.02146464 What does the comparison of performances across k folds tell us? if the performance is consistently good (high mean, low sd), the model is likely to generalize well to “new” data. if the performance is consistently bad (low mean, low sd), the model might be underfitting. if the performance varies a lot across the cross-validation folds (high sd), the model might be overfitting. Try it! Evaluate the performance of the decision tree model with cross-validation, adding all predictors that make sense to predict if a passenger survived or not. How is the average performance? does it vary much from fold to fold? # Initialize a vector to keep track of the performance for each k-fold perf_k &lt;- NULL # Use a for-loop to get performance for each k-fold for(i in 1:k){ # Split data into train and test data train_i &lt;- shuffled_data |&gt; filter(folds != i) # train data = all observations except in fold i test_i &lt;- shuffled_data |&gt; filter(folds == i) # test data = observations in fold i # Train model on train data (all but fold i) # CHANGE: what model/predictors should be included train_model &lt;- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Embarked, data = train_i, method = &quot;class&quot;) # Performance listed for each test data = fold i # CHANGE: how the performance is calculated ROC &lt;- ggplot(test_i) + geom_roc(aes( # Outcome is Survived d = Survived, # Probability of surviving based on the logistic model m = predict(train_model, newdata = test_i)[ ,2])) # Save the AUC values perf_k[i] &lt;- calc_auc(ROC)$AUC } # Performance for each fold perf_k ## [1] 0.8179522 0.8375624 0.7986739 0.7542683 0.8874179 # Average performance over all k folds mean(perf_k) ## [1] 0.819175 sd(perf_k) ## [1] 0.04906593 The average performance on new data is consistently worse than the performance on the entire dataset (see above). What would we need to change in this worksheet if we were predicting the Fare paid by a passenger? We would need to change the model approach (linear regression or decision tree to predict a numeric outcome). We would also need to change how we evaluate the performance of the model (using RMSE). Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings Cross validation "],["WS25_UnsupervisedLearning_Key.html", "Unsupervised Learning Learning objectives 1. Distances 2. Correlation Recommended Resources", " Unsupervised Learning Learning objectives Understand the concept of distance between observations and calculate distances between observations. Scale variables and compute z-scores. Create and interpret a correlation matrix. We will use two new pacakges: ade4 to access a built-in dataset and ggcorrplot to visualize a correlation matrix. # Install new packages (only needed once!) install.packages(&quot;ade4&quot;) install.packages(&quot;ggcorrplot&quot;) Then load the packages for today: # Load packages library(tidyverse) library(ade4) library(ggcorrplot) Let’s consider the built-in database olympic which gives the performances of 33 men in the decathlon (10 disciplines) at the Olympic Games in 1988 (Seoul). We will focus on the dataset tab. # Save the database into your environment, then the dataset data(&quot;olympic&quot;) athletes &lt;- olympic$tab # Take a quick look at the dataset head(athletes) The names of the variables might not be very intuitive so let’s rename them: # Quick cleanup athletes &lt;- athletes |&gt; # Translate the variable names (from French!) and reorder select(time_100 = `100`, time_110 = `110`, time_400 = `400`, time_1500 = `1500`, dist_disc = disq, dist_weight = poid, dist_high_jump = haut, dist_long_jump = long, dist_javelin = jave, dist_perch = perc) head(athletes) We will compare the athletes based on their performance in the 10 disciplines/variables. 1. Distances A key concept in clustering is measuring distances between observations—that is, how ‘far apart’ they are in terms of their features. a. Euclidean distance We can consider the Euclidean distance (the most typical distance): \\[ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} \\] Let’s compare the athlete that ranked first (first row) and the one that ranked last (last row) in terms of their performance in running 100 meters: # Calculate distance by hand athletes |&gt; # Only keep the first and last athlete filter(row_number() == 1 | row_number() == 33) |&gt; # Find the Euclidean distance = sqrt of the squared differences summarize(euclidean = sqrt(diff(time_100)^2)) The two athletes are at a distance of 0.32 seconds from each other. What if we wanted to also consider their performance in throwing a disc? Try it! Calculate the Euclidean distance between the first and last athletes also taking into account their performance in throwing a disc. Why is the distance much bigger now? Compare this distance between the first and second athletes. athletes |&gt; # Only keep the first and last athlete filter(row_number() == 1 | row_number() == 33) |&gt; # Find the Euclidean distance = sqrt of the squared differences summarize(euclidean = sqrt(diff(time_100)^2 + diff(dist_disc)^2)) athletes |&gt; # Only keep the first and last athlete filter(row_number() == 1 | row_number() == 2) |&gt; # Find the Euclidean distance = sqrt of the squared differences summarize(euclidean = sqrt(diff(time_100)^2 + diff(dist_disc)^2)) We add all the differences for each discipline: these differences are not expressed in the same unit (we should scale the variables so they become “unitless” and comparable). The distance is much bigger between the first and second athletes compared to the distance between the first and last athletes. b. Scaling To ensure all variables contribute equally, we usually scale them: we compare an individual value to all the values of the variable (a measure of position). Let’s scale the performance of the athletes for each discipline by subtracting the mean and divide by the standard deviation (this is called a z-score): # Scale all variables athletes_scaled &lt;- athletes |&gt; scale() |&gt; as.data.frame() head(athletes_scaled) Try it! Calculate the mean and standard deviation of the scaled values for the time to run 100 meters. Why does it make sense to get what we get? What does a positive scaled value indicate? a negative value? athletes_scaled |&gt; # Calculate the mean and standard deviation summarize(mean = mean(time_100), sd = sd(time_100)) The scaling process positions the data such that the mean is close to 0 and the standard deviation is 1. A positive value indicates that an athlete’s time is above the average time, while a negative value indicates that an athlete’s time is below average (meaning he’s running faster!). 2. Correlation a. Correlation Coefficient Another important concept in unsupervised learning is correlation which describes the (linear) relationship between two variables. For example, let’s look at the relationship between time to run 100 meters and length of a long jump: # Visualize the relationship between time_100 and long_jump ggplot(athletes, aes(x = time_100, y = dist_long_jump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Time to run 100 meters (in seconds)&quot;, y = &quot;Distance for long jump (in meters)&quot;) We can use the correlation coefficient to describe the strength and direction of the relationship between those two variables: # Find the correlation between two variables cor(athletes$time_100, athletes$dist_long_jump, use = &quot;pairwise.complete.obs&quot;) # ignore missing values ## [1] -0.5395676 What if we would like to find the correlation coefficients between all pairs of numeric variables? That’s a lot of calculations of the correlation coefficients… b. Correlation Matrix We can actually find the correlation between all pairs of variables by not specifying the variables: # Find pairwise correlations cor(athletes, use = &quot;pairwise.complete.obs&quot;) ## time_100 time_110 time_400 time_1500 ## time_100 1.00000000 0.63836152 0.60589539 0.26103415 ## time_110 0.63836152 1.00000000 0.54602764 0.14329760 ## time_400 0.60589539 0.54602764 1.00000000 0.58728182 ## time_1500 0.26103415 0.14329760 0.58728182 1.00000000 ## dist_disc -0.04722423 -0.11049751 0.14218823 0.40231998 ## dist_disc dist_weight dist_high_jump ## time_100 -0.04722423 -0.20797283 -0.14590755 ## time_110 -0.11049751 -0.29571686 -0.30673504 ## time_400 0.14218823 0.09458277 -0.08750434 ## time_1500 0.40231998 0.26882944 -0.11408736 ## dist_disc 1.00000000 0.80635220 0.14742183 ## dist_long_jump dist_javelin dist_perch ## time_100 -0.53956763 -0.06471440 -0.38913812 ## time_110 -0.47799594 -0.06282187 -0.52154860 ## time_400 -0.51532883 0.12035232 -0.31865911 ## time_1500 -0.39558645 0.09637566 -0.03149506 ## dist_disc 0.04192321 0.44290752 0.34396570 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] # Find pairwise correlations with scaled data cor(athletes_scaled, use = &quot;pairwise.complete.obs&quot;) ## time_100 time_110 time_400 time_1500 ## time_100 1.00000000 0.63836152 0.60589539 0.26103415 ## time_110 0.63836152 1.00000000 0.54602764 0.14329760 ## time_400 0.60589539 0.54602764 1.00000000 0.58728182 ## time_1500 0.26103415 0.14329760 0.58728182 1.00000000 ## dist_disc -0.04722423 -0.11049751 0.14218823 0.40231998 ## dist_disc dist_weight dist_high_jump ## time_100 -0.04722423 -0.20797283 -0.14590755 ## time_110 -0.11049751 -0.29571686 -0.30673504 ## time_400 0.14218823 0.09458277 -0.08750434 ## time_1500 0.40231998 0.26882944 -0.11408736 ## dist_disc 1.00000000 0.80635220 0.14742183 ## dist_long_jump dist_javelin dist_perch ## time_100 -0.53956763 -0.06471440 -0.38913812 ## time_110 -0.47799594 -0.06282187 -0.52154860 ## time_400 -0.51532883 0.12035232 -0.31865911 ## time_1500 -0.39558645 0.09637566 -0.03149506 ## dist_disc 0.04192321 0.44290752 0.34396570 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] How does the correlation coefficients compare for the original vs scaled data? They’re the same! Scaling the data does not change the correlation between the variables. The output is a matrix representing correlations so it is called a correlation matrix! It is pretty ugly though… let’s make it pretty with ggcorrplot(correlation_matrix)! # Use the ggcorrplot to visualize the correlation matrix ggcorrplot(cor(athletes_scaled)) We can add some options to make the correlation matrix even prettier: # We can add some options ggcorrplot(cor(athletes), type = &quot;upper&quot;, # upper diagonal only lab = TRUE, # print values method = &quot;circle&quot;) # use circles with different sizes This visualization makes it easier to identify which variables are most strongly correlated. Try it! Create a graph to display the relationship between the pair of variables that has the strongest correlation coefficient. Describe the relationship. # The max correlation is for weight and disc ggplot(athletes, aes(x = dist_disc, y = dist_weight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + labs(x = &quot;Disc (in meters)&quot;, y = &quot;Weight (in meters)&quot;) The two variables, weight and disc, are highly correlated: they actually represent similar disciplines. c. Combining variables Next, we will talk about combining variables that are highly correlated together to reduce the number of variables in our data. Try it! In the scaled data, add all of the variables representing times together and all the variables representing distances together. Then create a graph to display the relationship between the two sums and find the correlation coefficient between these two variables. Where should the winners be? athletes_scaled |&gt; mutate(times = time_100 + time_400 + time_110 + time_1500, distances = dist_long_jump + dist_weight + dist_high_jump + dist_disc + dist_perch + dist_javelin, # keep track of rank rank = row_number()) |&gt; ggplot(aes(x = times, y = distances)) + geom_point() + # Add labels geom_text(aes(label = rank), nudge_y = 0.5) labs(x = &quot;Sum of times&quot;, y = &quot;Sum of distances&quot;) ## $x ## [1] &quot;Sum of times&quot; ## ## $y ## [1] &quot;Sum of distances&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; athletes_scaled |&gt; mutate(times = time_100 + time_400 + time_110 + time_1500, distances = dist_long_jump + dist_weight + dist_high_jump + dist_disc + dist_perch + dist_javelin, # keep track of rank rank = row_number()) |&gt; summarize(correlation = cor(times, distances)) The winners of the decathlon are in the left/top corner (less time, more distance). But other factors also seem to play a role in distinguishing the athletes. See more details for how to determine the winner of a decathlon here Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings Large datasets Learn more about decathlon "],["WS26_DimensionReduction_Key.html", "Dimension Reduction Learning objectives 1. Principal Component Analysis 2. Some considerations for using PCA Recommended Resources", " Dimension Reduction Learning objectives Understand the concept of Principal Component Analysis (PCA). Perform PCA, interpret PCA outputs, visualize principal components. Compare observations using the reduced dimensions. We will use a new package, factoextra, to provide information about the algorithm for dimension reduction using ggplot functions. # Install new packages (only needed once!) install.packages(&quot;factoextra&quot;) Then load all packages for today: # Load packages library(tidyverse) library(ade4) library(ggcorrplot) library(factoextra) We will consider the built-in database olympic again which gives the performances of 33 men in the decathlon (10 disciplines) at the Olympic Games in 1988: # Save the database into your environment, then the dataset data(&quot;olympic&quot;) athletes &lt;- olympic$tab # Quick cleanup athletes &lt;- athletes |&gt; # Translate the variable names (from French!) and reorder select(time_100 = `100`, time_110 = `110`, time_400 = `400`, time_1500 = `1500`, dist_disc = disq, dist_weight = poid, dist_high_jump = haut, dist_long_jump = long, dist_javelin = jave, dist_perch = perc) head(athletes) Previously, we talked about the correlation matrix, representing the correlation between each pair of variables: # Correlation matrix ggcorrplot(cor(athletes), type = &quot;upper&quot;, # upper diagonal only lab = TRUE) # print values Now, we would like to combine some of these variables together to visualize our data in 2 dimensions. 1. Principal Component Analysis The 4 steps in PCA are: Prepare the data: Scale the data (subtract mean, divide by standard deviation). Perform PCA: Using prcomp() on your prepared variables. Choose the number of principal components: Make a scree plot (or choose based on interpretability). Consider PC scores (the new coordinates for each observation on PCs of interest) and visualize and interpret (if possible) retained PCs and scores. a. Prepare the dataset and explore correlations We aim to group variables that convey similar information. It is a good practice to scale our variables so they are all in the same unit (how many standard deviations away a value is from the mean) with scale() # Prepare the dataset athletes_scaled &lt;- athletes |&gt; # Scale the variables scale() |&gt; # Save as a data frame as.data.frame() # Take a look at the scaled data head(athletes_scaled) Recall: What does a negative value indicate in the scaled data? A negative value indicates that a given athlete is below average compared to the other athletes. b. Perform PCA Let’s perform PCA on our 10 variables using prcomp(). # PCA performed with the function prcomp() pca &lt;- athletes_scaled |&gt; prcomp() # The output creates 5 different objects names(pca) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; Without going into too much detail, let’s describe the element x. Instead of having the performances of the 33 athletes for each 10 disciplines, we have new values according to the new variables PC1, PC2, …, PC10. The first few principal components (PC), also called dimensions, try to maximize the variation explained by all variables. # New perspective on our data new_athletes &lt;- pca$x |&gt; as.data.frame() new_athletes Try it! The procedure for PCA also assumes that the components/dimensions are not correlated. Make the correlation matrix to check the correlation between the components/dimensions. What do you notice? # Correlation matrix ggcorrplot(cor(new_athletes), type = &quot;upper&quot;, # upper diagonal only lab = TRUE) # print values None of the components/dimensions are correlated! Let’s use the new dimensions (PC1 and PC2, also called Dim1 and Dim2 respectively) to represent the athletes. Try it! Using the new_athletes represent the athletes on the first two principal components. # Visualize the individuals according to PC1 and PC2 new_athletes |&gt; ggplot() + geom_point(aes(x = PC1, y = PC2)) Or, more convenient, using the functions from the factoextra package: # Visualize the individuals according to PC1 and PC2 fviz_pca_ind(pca, repel = TRUE) # Avoid text overlapping for the row number Note that the numbers shown on the scatterplot represent the rank of each athlete. The scatterplot above is a new perspective on our data: it shows how the 33 athletes compare to each other, taking into account the 10 disciplines which are summarized with Dim1 and Dim2, the first two principal components. Since we reduced the amount of variables, we lost some information about how the 33 athletes vary from each other: Dim1 takes into account 34.2% of the total variation and Dim2 takes into account another 26.1% of the total variation. c. Choose the number of principal components The idea is to reduce the number of variables so we would like to keep only a few of the principal components (also called dimensions). A scree plot displays the amount of variance explained by each principal component. The more we explain the total variation, the better! # Visualize percentage of variance explained for each PC in a scree plot fviz_eig(pca, addlabels = TRUE) We are usually looking to keep about 80% of the variance in the data with the few first principal components/dimensions. Try it! Reading the plot above, how many dimensions should we consider to keep about 80% of the variance? # From reading the plot 34.2 + 26.1 + 9.4 + 8.8 ## [1] 78.5 Keeping the first 4 dimensions will add up to about 78.5% so we would need 5 dimensions to keep at least 80% of the variation. Note: it’s difficult to represent data with 5 “variables”! d. Interpret components/dimensions Each component/dimension is actually a linear combination of the old variables (each of the 10 disciplines). We can take a look at the contribution of each variable to each component/dimension: # Visualize the contributions of the variables to the PCs in a table get_pca_var(pca)$coord |&gt; as.data.frame() For example, the first principal component (Dim.1) is: \\[ Dim.1 = -0.7689031 * time\\_100 + ... -0.3145678*time\\_1500 + ... + 0.7101094*dist\\_perch \\] Try it! Use dplyr functions to find the variable that contributes the most positively to the first principal component and the variable that contributes the most negatively as well. # From the new coordinates get_pca_var(pca)$coord |&gt; as.data.frame() |&gt; filter(Dim.1 == max(Dim.1) | Dim.1 == min(Dim.1)) |&gt; select(Dim.1) The variable dist_long_jump contributes the most positively to the first principal component while the variable time_110 contributes the most negatively to the first principal component. We can visualize the contributions of the variables with what we call a correlation circle: # Correlation circle fviz_pca_var(pca, col.var = &quot;black&quot;, repel = TRUE) # Avoid text overlapping of the variable names Based on this visualization, we can see that some disciplines contribute positively to the first component and some contribute negatively to that same dimension. What do you notice when comparing the nature of those disciplines opposing each other on the first dimension? The variables of running are opposite of the other variables. Finally, we can visualize both the individuals and the variables’ contributions in a single plot called a biplot: # Visualize both variables and individuals in the same graph fviz_pca_biplot(pca, repel = TRUE) # Avoid text overlapping of the names The labels for the athletes show their overall decathlon rank. What do you notice about where the best ranked athletes are located? the worst ranked athletes? Most of the best athletes are on the right of the graph while the others are on the left. What does it mean for an athlete to have a high value for the first dimension? A high value on the first dimension means an athlete performs well in events where higher scores are better (e.g., long jump, pole vault, etc., above average) and also performs well in events where lower scores are better (i.e., faster times, below average). 2. Some considerations for using PCA While PCA is a powerful tool for reducing the number of variables and uncovering patterns in the data, it has a few important limitations: PCA only works with numeric variables: Since PCA is based on the correlation matrix, it cannot be applied directly to categorical variables without some form of encoding. PCA ignores missing values: Any rows with missing values must be removed beforehand, which could influence the analysis. Interpretation can be difficult: The principal components are combinations of the original variables, which can sometimes make interpretation not so intuitive. Linear assumptions: PCA captures linear relationships between variables, so it might not be ideal if the data has strong nonlinear patterns. Remember the dataset about a community of penguins in the Antarctic? # Save the object as a dataframe penguins &lt;- as.data.frame(palmerpenguins::penguins) # Take a quick look head(penguins) Try it! Apply PCA to this dataset by 1) only keeping the variables about size measurements, 2) ignoring missing values, 3) scaling the data. Then represent the penguins on the first two principal components. What do you notice? # Prepare the dataset penguins_scaled &lt;- penguins |&gt; # Only keep the measurements select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt; # Ignore missing values drop_na() |&gt; # Scale the variables scale() |&gt; # Save as a data frame as.data.frame() # Perform PCA pca &lt;- penguins_scaled |&gt; prcomp() # Represent the penguins comparing all 4 measurements pca$x |&gt; as.data.frame() |&gt; ggplot() + geom_point(aes(x = PC1, y = PC2)) Looks like there are two main groups of penguins… why? Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings Dimension reduction Learn more about factoextra "],["WS27_Clustering_Key.html", "Clustering Learning objectives 1. Clustering 2. Include more variables in clustering Your turn! Recommended Resources", " Clustering Learning objectives Understand the purpose of clustering, especially k-means clustering. Interpret clustering results and visualize clusters. Evaluate the number of clusters using silhouette width. Recognize the role of PCA when visualizing clustering in high dimensions. We will use a new package today containing the functions related to clustering: # Install new packages (only needed once!) install.packages(&quot;cluster&quot;) Then load all packages for today: # Load packages library(tidyverse) library(ade4) library(factoextra) library(cluster) Remember the atheletes dataset? It contains information about the performance of 33 athletes in the 10 disciplines of the decathlon at the Olympics in 1988: # Save the database into your environment, then the dataset data(&quot;olympic&quot;) athletes &lt;- olympic$tab # Quick cleanup athletes &lt;- athletes |&gt; # Translate the variable names (from French!) and reorder select(time_100 = `100`, time_110 = `110`, time_400 = `400`, time_1500 = `1500`, dist_disc = disq, dist_weight = poid, dist_high_jump = haut, dist_long_jump = long, dist_javelin = jave, dist_perch = perc) head(athletes) We would like to see if these athletes cluster together in some groups. 1. Clustering The goal of clustering is to identify observations that are alike/close to each other. We will consider the algorithm for k-means clustering: Pick k points of the n observations at random to serve as initial cluster centers. Assign each n-k observation to the cluster whose center is closest. For each group, calculate means and use them as new centers. Repeat steps 2-3 until groups stabilize. Before we apply the algorithm, we need to prepare the data so that all variables are on the same scale. a. Prepare the data We should scale our variables before clustering so that variables can be comparable regardless of their scale. # Prepare the dataset athletes_scaled &lt;- athletes |&gt; # Scale the variables scale() |&gt; # Save as a data frame as.data.frame() # Take a look at the scaled data head(athletes_scaled) All variables are on the same “unitless” scale! Let’s first focus on 2 variables in the athletes dataset, time_100 and dist_disc. Try it! Represents the relationship between the performances of the athletes in these two disciplines with a ggplot. Do you notice any groups of athletes? # Make a plot athletes |&gt; ggplot(aes(x = time_100, y = dist_disc)) + geom_point() Maybe two main groups? Athletes who have somewhat of a linear relationship between the two performances and the ones who excel in throwing the disc but did not run very fast. b. Apply the algorithm Let’s use the k-means algorithm with the kmeans(data, k = nb_clusters) function with k = 2 to find 2 clusters: # For reproducible results: why? set.seed(322) # Use the function kmeans() to find clusters kmeans_results &lt;- athletes_scaled |&gt; select(time_100, dist_disc) |&gt; kmeans(centers = 2) # centers sets the number of clusters to find # The output provides some information about the clusters and creates 9 different objects names(kmeans_results) ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; We will focus on the cluster object: # A vector attributing a cluster number to each observation kmeans_results$cluster ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 ## 23 24 25 26 27 28 29 30 31 32 33 ## 2 1 1 1 1 2 1 1 2 1 1 The cluster object indicates which observations (i.e., which athlete) is in which cluster. c. Visualize and interpret the clusters We can save the identification of the cluster for each observation in the original dataset to manipulate the observations for each cluster: # Consider the original dataset athletes |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) |&gt; # Only keep the variables of interest select(time_100, dist_disc, cluster) |&gt; head() What characteristics do the athletes share in each cluster? We can visualize the clusters and create summary statistics of each variable to understand some characteristics about the clusters. Try it! Using the original dataset, visualize the relationship between the performances in the two disciplines for each cluster. Also find the mean and standard deviation for the performance in each discipline for each cluster. Are there any any differences between the clusters? # Make a plot athletes |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) |&gt; ggplot(aes(x = time_100, y = dist_disc, color = cluster)) + geom_point() # Find mean and SD athletes |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) |&gt; # For each cluster group_by(cluster) |&gt; # Find the mean and sd of the variables summarize(mean_100 = mean(time_100), sd_100 = sd(time_100), mean_disc = mean(dist_disc), sd_disc = sd(dist_disc)) The athletes is cluster 1 have much lower values for throwing the disc and slightly higher values for running 100 meters. We compared 2 clusters but how did we decide that our athletes should be separated into 2 groups? d. Choose the number of clusters Determining the number of clusters to use can be tricky. We can either consider the context or using measures such as the average silhouette width (which measures how cohesive and separated clusters are, simultaneously) for multiple values of k. A high average silhouette width indicates a good clustering structure: the observations within the groups are close to each other and the groups are very distinct from each other. We can use the function fviz_nbclust(scaled_data, clustering_algorithm, method) to compare different values of k: # Maximize the silhouette while keeping a small number of clusters fviz_nbclust(athletes_scaled, kmeans, method = &quot;silhouette&quot;) The average silhouette width seems to indicate that 3 clusters maximize the average width silhouette for the athletes_scaled. Try it! Split the athletes in 3 clusters with kmeans. How do the athletes compare now between the 3 clusters? # For reproducible results set.seed(322) # Use the function kmeans() to find clusters kmeans_results &lt;- athletes_scaled |&gt; select(time_100, dist_disc) |&gt; kmeans(centers = 3) # centers sets the number of clusters to find # Make a plot athletes |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) |&gt; ggplot(aes(x = time_100, y = dist_disc, color = cluster)) + geom_point() # Find mean and SD athletes |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) |&gt; # For each cluster group_by(cluster) |&gt; # Find the mean and sd of the variables summarize(mean_100 = mean(time_100), sd_100 = sd(time_100), mean_disc = mean(dist_disc), sd_disc = sd(dist_disc)) The athletes is cluster 1 have the lowest values for running 100 meters. The athletes in cluster 2 have the greatest values for throwing the disc but also for running 100 meters. The athletes in cluster 3 have low values for throwing the disc and high values for running 100 meters (they don’t perform as well as the other athletes). 2. Include more variables in clustering What if we would like to consider more variables to compare the athletes? We can technically use all the variables in the athletes dataset! From above, the average silhouette width indicates that we should consider 3 clusters: # For reproducible results set.seed(322) # Use the function kmeans() to find clusters kmeans_results &lt;- athletes_scaled |&gt; kmeans(centers = 3) Visualize the clusters with fviz_cluster(). # Let&#39;s visualize our data with cluster assignment fviz_cluster(kmeans_results, data = athletes_scaled) What do the labels of the x-axis and y-axis indicate? Why? It is impossible to visualize the clusters across the 10 variables so R performed PCA! What characteristics do the athletes share within each cluster? Let’s describe each cluster with the mean: # Create basic summary statistics for each cluster in original units athletes |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) |&gt; # For each cluster group_by(cluster) |&gt; # Find the mean of all variables summarize_all(mean) Looking at these means, is there a cluster of athletes that performs consistently better for timed disciplines? worse? Athletes in Cluster 3 have the lowest mean for all timed disciplines (better performers) while Cluster 2 has the highest mean for the timed events (not as good performers). What about for other events? Cluster 1 has the lowest mean for all distance disciplines (except for high jump). Cluster 2 has the highest mean for throwing the disc and weight, while Cluster 3 has the highest means for high jump, long jump, javelin and perch. Note: The k-means algorithm relies on computing distances (typically Euclidean distance) between data points to assign them to clusters. This kind of distance calculation only makes sense when the variables are numeric and continuous. Your turn! Let’s practice some clustering on the penguins dataset. # Save the object as a dataframe penguins &lt;- as.data.frame(palmerpenguins::penguins) # Take a quick look head(penguins) Only consider the variables about size measurements and ignore missing values. Prepare the data for clustering by scaling these variables. Name the resulting dataset as penguins_scaled. # Prepare the dataset penguins_scaled &lt;- penguins |&gt; # Only keep the measurements select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt; # Ignore missing values drop_na() |&gt; # Scale the variables scale() |&gt; # Save as a data frame as.data.frame() How many clusters should we find? # Maximize the silhouette while keeping a small number of clusters fviz_nbclust(penguins_scaled, kmeans, method = &quot;silhouette&quot;) Looks like we should look for 2 clusters of penguins. Apply the algorithm with the corresponding number of clusters. Visualize the clusters with fviz_cluster. # For reproducible results set.seed(322) # Use the function kmeans() to find clusters kmeans_results &lt;- penguins_scaled |&gt; kmeans(centers = 2) # centers sets the number of clusters to find # Visualize our data with cluster assignment fviz_cluster(kmeans_results, data = penguins_scaled) Add a variable to the original penguins dataset (but ignoring missing values for the size measurements) that describes which cluster each penguin belongs to. Compare the number of penguins of each species per cluster. Do you notice anything? Also compare the number of penguins of each island per cluster and of each sex per cluster. # Create basic summary statistics for each cluster in original units penguins_cluster &lt;- penguins |&gt; filter(!is.na(bill_length_mm), !is.na(bill_depth_mm), !is.na(flipper_length_mm), !is.na(body_mass_g)) |&gt; # Save cluster assignment as a new variable mutate(cluster = as.factor(kmeans_results$cluster)) # Species per cluster table(penguins_cluster$cluster, penguins_cluster$species) ## ## Adelie Chinstrap Gentoo ## 1 151 68 0 ## 2 0 0 123 # Island per cluster table(penguins_cluster$cluster, penguins_cluster$island) ## ## Biscoe Dream Torgersen ## 1 44 124 51 ## 2 123 0 0 # Sex per cluster table(penguins_cluster$cluster, penguins_cluster$sex) ## ## female male ## 1 107 107 ## 2 58 61 Penguins in the second cluster are all from the same species, the Gentoo! This is an example of how clustering can be used to distinguish between different species! They are also all from the Biscoe island. Each cluster has about the same ratio of female/male penguins. Recommended Resources Worksheet keys are posted at the end of the week on Canvas under Programming Tools. Recommended readings Clustering Learn more about kmeans "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
